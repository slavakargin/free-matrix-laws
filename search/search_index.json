{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"free-matrix-laws","text":"<p>Utilities for distributions of matrix-valued non-commutative variables: Cauchy/resolvent transforms, operator-valued maps (e.g., \\(\\eta(B)=\\sum_i A_i B A_i\\)), and small fixed-point solvers.</p> <p>Work-in-progress. Feedback welcome.</p>"},{"location":"#install","title":"Install","text":"<p>From GitHub (read-only users) <pre><code>pip install -U \"git+https://github.com/slavakargin/free-matrix-laws.git@main\"\n</code></pre></p>"},{"location":"#iterating-the-operator-valued-cauchy-transform","title":"Iterating the operator-valued Cauchy transform","text":"<p>We solve the operator-valued semicircle equation $$ z\\,G \\;=\\; I \\;+\\; \\eta(G)\\,G, \\qquad \\Im z&gt;0, $$ where \\(\\eta(B)=\\sum_{i=1}^s A_i\\,B\\,A_i^\\ast\\) is a completely positive (Kraus) map.</p>"},{"location":"#fixed-point-maps","title":"Fixed-point maps","text":"<p>The simplest iteration is $$ G \\;\\mapsto\\; (\\,zI - \\eta(G)\\,)^{-1}. $$</p> <p>Following Helton\u2013Rashidi Far\u2013Speicher (IMRN 2007), a numerically friendlier choice is the half-averaged step $$ G \\;\\mapsto\\; \\tfrac12\\Big[\\,G \\;+\\; (\\,zI - \\eta(G)\\,)^{-1}\\Big], $$ which damps oscillations while preserving the correct fixed point.</p> <p>Reference. J. W. Helton, R. Rashidi Far, R. Speicher, Operator-valued Semicircular Elements: Solving a Quadratic Matrix Equation with Positivity Constraints, IMRN (2007).</p> <p>In code: use <code>free_matrix_laws.solve_cauchy_semicircle(z, A, ...)</code>, which implements the half-averaged step.</p>"},{"location":"api/ensembles/","title":"API \u2014 random ensembles","text":""},{"location":"api/ensembles/#free_matrix_laws.ensembles.random_semicircle","title":"<code>random_semicircle(n, field='real', variance=1.0, seed=None)</code>","text":"<p>Generate an \\(n\\times n\\) Hermitian (self-adjoint) Wigner matrix whose eigenvalues follow (as \\(n\\to\\infty\\)) a semicircle law with variance parameter \\(c=\\texttt{variance}\\), i.e. support \\([-2\\sqrt{c},\\,2\\sqrt{c}]\\).</p> <p>Conventions (entry variances): - \\(\\texttt{field}=\\text{\"real\"}\\) (GOE normalization):   off-diagonal \\(\\operatorname{Var}(H_{ij}) = c/n\\), diagonal \\(\\operatorname{Var}(H_{ii}) = 2c/n\\). - \\(\\texttt{field}=\\text{\"complex\"}\\) (GUE normalization):   off-diagonal \\(\\mathbb E|H_{ij}|^2 = c/n\\), diagonal \\(\\operatorname{Var}(H_{ii}) = c/n\\).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Matrix size.</p> required <code>field</code> <code>(real, complex)</code> <p>Real symmetric (GOE-type) or complex Hermitian (GUE-type).</p> <code>\"real\",\"complex\"</code> <code>variance</code> <code>float</code> <p>Semicircle variance parameter \\(c&gt;0\\). The spectrum concentrates on \\([-2\\sqrt{c},\\,2\\sqrt{c}]\\) in the large-\\(n\\) limit.</p> <code>1.0</code> <code>seed</code> <code>int or Generator</code> <p>Random seed or generator.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>H</code> <code>ndarray</code> <p>Hermitian matrix (dtype float64 for real, complex128 for complex).</p> Notes <p>Implementation uses the convenient symmetrize-and-scale recipe:</p> <ul> <li>Real: \\(H = \\dfrac{X + X^\\top}{\\sqrt{2n}}\\), \\(X_{ij}\\sim\\mathcal N(0,1)\\).   This yields \\(\\operatorname{Var}(H_{ij})=1/n\\) (off-diagonal), \\(\\operatorname{Var}(H_{ii})=2/n\\).</li> <li>Complex: draw \\(Z_{ij}\\sim \\mathcal{CN}(0,1)\\) (i.e. \\((X+iY)/\\sqrt{2}\\) with \\(X,Y\\sim\\mathcal N(0,1)\\)),   then \\(H = \\dfrac{Z + Z^\\ast}{\\sqrt{2n}}\\), giving \\(\\mathbb E|H_{ij}|^2=1/n\\) and   \\(\\operatorname{Var}(H_{ii})=1/n\\). Finally multiply by \\(\\sqrt{c}\\).</li> </ul> Source code in <code>src/free_matrix_laws/ensembles.py</code> <pre><code>def random_semicircle(n: int,\n                      field: str = \"real\",\n                      variance: float = 1.0,\n                      seed=None):\n    r'''\n    Generate an $n\\times n$ Hermitian (self-adjoint) Wigner matrix whose eigenvalues\n    follow (as $n\\to\\infty$) a semicircle law with variance parameter $c=\\texttt{variance}$,\n    i.e. support $[-2\\sqrt{c},\\,2\\sqrt{c}]$.\n\n    Conventions (entry variances):\n    - $\\texttt{field}=\\text{\"real\"}$ (GOE normalization):\n      off-diagonal $\\operatorname{Var}(H_{ij}) = c/n$, diagonal $\\operatorname{Var}(H_{ii}) = 2c/n$.\n    - $\\texttt{field}=\\text{\"complex\"}$ (GUE normalization):\n      off-diagonal $\\mathbb E|H_{ij}|^2 = c/n$, diagonal $\\operatorname{Var}(H_{ii}) = c/n$.\n\n    Parameters\n    ----------\n    n : int\n        Matrix size.\n    field : {\"real\",\"complex\"}, default \"real\"\n        Real symmetric (GOE-type) or complex Hermitian (GUE-type).\n    variance : float, default 1.0\n        Semicircle variance parameter $c&gt;0$. The spectrum concentrates on\n        $[-2\\sqrt{c},\\,2\\sqrt{c}]$ in the large-$n$ limit.\n    seed : int or numpy.random.Generator, optional\n        Random seed or generator.\n\n    Returns\n    -------\n    H : ndarray\n        Hermitian matrix (dtype float64 for real, complex128 for complex).\n\n    Notes\n    -----\n    Implementation uses the convenient symmetrize-and-scale recipe:\n\n    - Real: $H = \\dfrac{X + X^\\top}{\\sqrt{2n}}$, $X_{ij}\\sim\\mathcal N(0,1)$.\n      This yields $\\operatorname{Var}(H_{ij})=1/n$ (off-diagonal), $\\operatorname{Var}(H_{ii})=2/n$.\n    - Complex: draw $Z_{ij}\\sim \\mathcal{CN}(0,1)$ (i.e. $(X+iY)/\\sqrt{2}$ with $X,Y\\sim\\mathcal N(0,1)$),\n      then $H = \\dfrac{Z + Z^\\ast}{\\sqrt{2n}}$, giving $\\mathbb E|H_{ij}|^2=1/n$ and\n      $\\operatorname{Var}(H_{ii})=1/n$. Finally multiply by $\\sqrt{c}$.\n    '''\n    if variance &lt;= 0:\n        raise ValueError(\"variance must be &gt; 0\")\n    rng = np.random.default_rng(seed)\n\n    if field == \"real\":\n        X = rng.standard_normal((n, n))\n        H = (X + X.T) / np.sqrt(2.0 * n)          # GOE normalization\n        if variance != 1.0:\n            H = np.sqrt(variance) * H\n        return H\n\n    if field == \"complex\":\n        X = rng.standard_normal((n, n))\n        Y = rng.standard_normal((n, n))\n        Z = (X + 1j * Y) / np.sqrt(2.0)           # CN(0,1) entries\n        H = (Z + Z.conj().T) / np.sqrt(2.0 * n)   # GUE normalization\n        if variance != 1.0:\n            H = np.sqrt(variance) * H\n        return H\n\n    raise ValueError('field must be \"real\" or \"complex\"')\n</code></pre>"},{"location":"api/opvalued/","title":"API \u2014 operator-valued maps","text":""},{"location":"api/opvalued/#free_matrix_laws.opvalued.covariance_map","title":"<code>covariance_map(B, A)</code>","text":"<p>Apply the completely positive (Kraus) map \\(\\eta(B) = \\sum_{i=1}^s A_i\\, B\\, A_i^{\\ast}\\).</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>(n, n) ndarray</code> <p>Square matrix the map acts on.</p> required <code>A</code> <code>sequence of (n, n) ndarrays or (s, n, n) ndarray</code> <p>Kraus operators \\(A_i\\) (no self-adjointness assumed).</p> required <p>Returns:</p> Type Description <code>(n, n) ndarray</code> <p>The value of \\(\\eta(B)\\).</p> Notes <p>\u2022 Uses \\(A_i^{\\ast}\\) (conjugate transpose), not \\(A_i^{\\mathsf T}\\). \u2022 Accepts a list/tuple of \\((n,n)\\) or a stacked array \\((s,n,n)\\).</p> Source code in <code>src/free_matrix_laws/opvalued.py</code> <pre><code>def covariance_map(B: np.ndarray, A: Sequence[np.ndarray] | np.ndarray) -&gt; np.ndarray:\n    r'''\n    Apply the completely positive (Kraus) map\n    $\\eta(B) = \\sum_{i=1}^s A_i\\, B\\, A_i^{\\ast}$.\n\n    Parameters\n    ----------\n    B : (n, n) ndarray\n        Square matrix the map acts on.\n    A : sequence of (n, n) ndarrays or (s, n, n) ndarray\n        Kraus operators $A_i$ (no self-adjointness assumed).\n\n    Returns\n    -------\n    (n, n) ndarray\n        The value of $\\eta(B)$.\n\n    Notes\n    -----\n    \u2022 Uses $A_i^{\\ast}$ (conjugate transpose), not $A_i^{\\mathsf T}$.  \n    \u2022 Accepts a list/tuple of $(n,n)$ or a stacked array $(s,n,n)$.\n    '''\n    B = np.asarray(B)\n    if B.ndim != 2 or B.shape[0] != B.shape[1]:\n        raise ValueError(f\"B must be square (n,n); got {B.shape!r}\")\n    n = B.shape[0]\n\n    # Normalize A to a stacked array (s,n,n)\n    if isinstance(A, np.ndarray) and A.ndim == 3:\n        A_arr = A\n    elif isinstance(A, np.ndarray) and A.ndim == 2:\n        if A.shape != (n, n):\n            raise ValueError(f\"A has shape {A.shape!r}, expected {(n, n)!r}.\")\n        A_arr = A[None, ...]\n    else:\n        # Python sequence \u2192 stack\n        A_list = list(A)  # type: ignore[arg-type]\n        A_arr = np.stack(A_list, axis=0)\n\n    if A_arr.shape[1:] != (n, n):\n        raise ValueError(f\"A stack has shape {A_arr.shape!r}, expected (s,{n},{n}).\")\n\n    # Promote dtype; ensure complex paths keep conjugation correct\n    dtype = np.result_type(B.dtype, A_arr.dtype, np.complex64)\n    B = B.astype(dtype, copy=False)\n    A_arr = A_arr.astype(dtype, copy=False)\n\n    # Vectorized Kraus: sum_i A_i B A_i^*\n    A_dag = A_arr.conj().swapaxes(-1, -2)   # (s,n,n)\n    tmp = A_arr @ B                         # (s,n,n)\n    out = np.matmul(tmp, A_dag).sum(axis=0) # (n,n)\n    return out\n</code></pre>"},{"location":"api/opvalued/#free_matrix_laws.opvalued.ds_distance","title":"<code>ds_distance(A)</code>","text":"<p>Distance to doubly stochastic class $$   \\mathrm{DS}(T) \\;=\\; |T(I)-I|_F^2 \\;+\\; |T^\\ast(I)-I|_F^2, $$ for \\(T(X)=\\sum_i A_i X A_i^\\ast\\) with Kraus \\(A_i\\).</p> Source code in <code>src/free_matrix_laws/opvalued.py</code> <pre><code>def ds_distance(A):\n    r'''\n    Distance to doubly stochastic class\n    $$\n      \\mathrm{DS}(T) \\;=\\; \\|T(I)-I\\|_F^2 \\;+\\; \\|T^\\ast(I)-I\\|_F^2,\n    $$\n    for $T(X)=\\sum_i A_i X A_i^\\ast$ with Kraus $A_i$.\n    '''\n    A_stack, _ = _as_stack(A)\n    TI, TS = _TI_TstarI(A_stack)\n    n = TI.shape[0]\n    I = np.eye(n, dtype=complex)\n    return float(la.norm(TI - I, 'fro')**2 + la.norm(TS - I, 'fro')**2)\n</code></pre>"},{"location":"api/opvalued/#free_matrix_laws.opvalued.symmetric_osi","title":"<code>symmetric_osi(A, maxiter=50, tol=1e-10, eps=1e-12, return_history=False)</code>","text":"<p>Run symmetric OSI: \\(T,\\ \\mathcal S(T),\\ \\mathcal S^2(T),\\dots\\) on Kraus operators until (approximately) doubly stochastic: \\(T(I)\\approx I\\) and \\(T^\\ast(I)\\approx I\\).</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>list/tuple of (n,n) arrays or stacked (s,n,n)</code> <p>Initial Kraus operators.</p> required <code>maxiter</code> <code>int</code> <p>Maximum iterations.</p> <code>50</code> <code>tol</code> <code>float</code> <p>Stop when \\(\\|T(I)-I\\|_F^2 + \\|T^\\ast(I)-I\\|_F^2 \\le \\text{tol}\\).</p> <code>1e-10</code> <code>eps</code> <code>float</code> <p>Eigenvalue floor for inverse square roots.</p> <code>1e-12</code> <code>return_history</code> <code>bool</code> <p>If True, also return a dict with diagnostics.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>B</code> <code>stacked (s,n,n) array</code> <p>Final Kraus operators after scaling.</p> <code>info</code> <code>dict(optional)</code> <p>Keys: 'iters', 'ds', 'history' (list of DS distances).</p> Source code in <code>src/free_matrix_laws/opvalued.py</code> <pre><code>def symmetric_osi(A, maxiter=50, tol=1e-10, eps=1e-12, return_history=False):\n    r'''\n    Run **symmetric OSI**: $T,\\ \\mathcal S(T),\\ \\mathcal S^2(T),\\dots$\n    on Kraus operators until (approximately) doubly stochastic:\n    $T(I)\\approx I$ and $T^\\ast(I)\\approx I$.\n\n    Parameters\n    ----------\n    A : list/tuple of (n,n) arrays or stacked (s,n,n)\n        Initial Kraus operators.\n    maxiter : int\n        Maximum iterations.\n    tol : float\n        Stop when $\\|T(I)-I\\|_F^2 + \\|T^\\ast(I)-I\\|_F^2 \\le \\text{tol}$.\n    eps : float\n        Eigenvalue floor for inverse square roots.\n    return_history : bool\n        If True, also return a dict with diagnostics.\n\n    Returns\n    -------\n    B : stacked (s,n,n) array\n        Final Kraus operators after scaling.\n    info : dict (optional)\n        Keys: 'iters', 'ds', 'history' (list of DS distances).\n    '''\n    B_stack, _ = _as_stack(A)\n    hist = []\n    for k in range(1, maxiter+1):\n        B_stack = symmetric_sinkhorn_scale(B_stack, eps=eps, preserve_input_type=False)\n        TI, TS = _TI_TstarI(B_stack)\n        n = TI.shape[0]\n        I = np.eye(n, dtype=complex)\n        ds = float(la.norm(TI - I, 'fro')**2 + la.norm(TS - I, 'fro')**2)\n        hist.append(ds)\n        if ds &lt;= tol:\n            break\n    if return_history:\n        return B_stack, {\"iters\": k, \"ds\": ds, \"history\": hist}\n    return B_stack\n</code></pre>"},{"location":"api/opvalued/#free_matrix_laws.opvalued.symmetric_sinkhorn_apply","title":"<code>symmetric_sinkhorn_apply(X, A, eps=1e-12)</code>","text":"<p>Apply the symmetric OSI scaled map \\(\\mathcal S(T)\\) to a matrix \\(X\\).</p> <p>Given \\(T(X)=\\sum_i A_i X A_i^\\ast\\) and \\(c_1=(T(I))^{-1/4}\\), \\(c_2=(T^\\ast(I))^{-1/4}\\), this returns $$   \\mathcal S(T)(X) \\;=\\; \\sum_i (c_1 A_i c_2)\\, X \\,(c_1 A_i c_2)^\\ast. $$</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(n,n) array</code> <p>Input matrix.</p> required <code>A</code> <code>list/tuple of (n,n) arrays, or stacked array (s,n,n), or single (n,n)</code> <p>Kraus operators \\(A_i\\).</p> required <code>eps</code> <code>float</code> <p>Eigenvalue floor in the inverse square roots.</p> <code>1e-12</code> <p>Returns:</p> Name Type Description <code>Y</code> <code>(n,n) array</code> <p>\\((\\mathcal S(T))(X)\\).</p> Source code in <code>src/free_matrix_laws/opvalued.py</code> <pre><code>def symmetric_sinkhorn_apply(X, A, eps=1e-12):\n    r'''\n    Apply the **symmetric OSI** scaled map $\\mathcal S(T)$ to a matrix $X$.\n\n    Given $T(X)=\\sum_i A_i X A_i^\\ast$ and\n    $c_1=(T(I))^{-1/4}$, $c_2=(T^\\ast(I))^{-1/4}$,\n    this returns\n    $$\n      \\mathcal S(T)(X) \\;=\\; \\sum_i (c_1 A_i c_2)\\, X \\,(c_1 A_i c_2)^\\ast.\n    $$\n\n    Parameters\n    ----------\n    X : (n,n) array\n        Input matrix.\n    A : list/tuple of (n,n) arrays, or stacked array (s,n,n), or single (n,n)\n        Kraus operators $A_i$.\n    eps : float, default 1e-12\n        Eigenvalue floor in the inverse square roots.\n\n    Returns\n    -------\n    Y : (n,n) array\n        $(\\mathcal S(T))(X)$.\n    '''\n    B_stack, _, _ = symmetric_sinkhorn_scale(A, eps=eps, return_factors=True, preserve_input_type=False)\n    Y = np.zeros_like(X, dtype=complex)\n    for Bi in B_stack:\n        Y += Bi @ X @ Bi.conj().T\n    return Y\n</code></pre>"},{"location":"api/opvalued/#free_matrix_laws.opvalued.symmetric_sinkhorn_scale","title":"<code>symmetric_sinkhorn_scale(A, eps=1e-12, return_factors=False, preserve_input_type=True)</code>","text":"<p>One symmetric OSI step: given a CP map \\(T(X)=\\sum_i A_i X A_i^\\ast\\), form $$   c_1 := \\big(T(I)\\big)^{-1/4}, \\qquad   c_2 := \\big(T^\\ast(I)\\big)^{-1/4}, $$ and return Kraus operators \\(B_i := c_1 A_i c_2\\) for \\(\\mathcal S(T)=S_{c_1,c_2}(T)\\).</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>list/tuple of (n,n) arrays, or stacked array (s,n,n), or single (n,n)</code> <p>Kraus operators \\(A_i\\).</p> required <code>eps</code> <code>float</code> <p>Eigenvalue floor used in the inverse square root of \\(T(I)\\) and \\(T^\\ast(I)\\).</p> <code>1e-12</code> <code>return_factors</code> <code>bool</code> <p>If True, also return \\((c_1, c_2)\\).</p> <code>False</code> <code>preserve_input_type</code> <code>bool</code> <p>If True, return a list if input was a list; otherwise return a stacked array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>B</code> <code>same container type as A (unless preserve_input_type=False)</code> <p>Scaled Kraus operators for \\(\\mathcal S(T)\\).</p> <code>(optional) c1, c2 : (n,n) arrays</code> <p>The scaling factors.</p> Source code in <code>src/free_matrix_laws/opvalued.py</code> <pre><code>def symmetric_sinkhorn_scale(A, eps=1e-12, return_factors=False, preserve_input_type=True):\n    r'''\n    One **symmetric OSI step**: given a CP map $T(X)=\\sum_i A_i X A_i^\\ast$,\n    form\n    $$\n      c_1 := \\big(T(I)\\big)^{-1/4}, \\qquad\n      c_2 := \\big(T^\\ast(I)\\big)^{-1/4},\n    $$\n    and return Kraus operators $B_i := c_1 A_i c_2$ for $\\mathcal S(T)=S_{c_1,c_2}(T)$.\n\n    Parameters\n    ----------\n    A : list/tuple of (n,n) arrays, or stacked array (s,n,n), or single (n,n)\n        Kraus operators $A_i$.\n    eps : float, default 1e-12\n        Eigenvalue floor used in the inverse square root of $T(I)$ and $T^\\ast(I)$.\n    return_factors : bool, default False\n        If True, also return $(c_1, c_2)$.\n    preserve_input_type : bool, default True\n        If True, return a list if input was a list; otherwise return a stacked array.\n\n    Returns\n    -------\n    B : same container type as A (unless preserve_input_type=False)\n        Scaled Kraus operators for $\\mathcal S(T)$.\n    (optional) c1, c2 : (n,n) arrays\n        The scaling factors.\n    '''\n    A_stack, mode = _as_stack(A)\n    s, n, _ = A_stack.shape\n\n    TI, TS = _TI_TstarI(A_stack)\n    c1 = _inv_frac_power_psd(TI, power=0.25, eps=eps)\n    c2 = _inv_frac_power_psd(TS, power=0.25, eps=eps)\n\n    B_stack = np.empty_like(A_stack, dtype=complex)\n    for i in range(s):\n        B_stack[i] = c1 @ A_stack[i] @ c2\n\n    if preserve_input_type and mode in {\"list\", \"single\"}:\n        B_out = [B_stack[i] for i in range(B_stack.shape[0])]\n        if mode == \"single\":\n            B_out = B_out[0]\n    else:\n        B_out = B_stack\n\n    if return_factors:\n        return B_out, c1, c2\n    return B_out\n</code></pre>"},{"location":"api/transforms/","title":"API \u2014 transforms","text":"<p>Numerical transforms for matrix-/operator-valued free probability.</p>"},{"location":"api/transforms/#free_matrix_laws.transforms.solve_G","title":"<code>solve_G = solve_cauchy_semicircle</code>  <code>module-attribute</code>","text":""},{"location":"api/transforms/#free_matrix_laws.transforms.get_density","title":"<code>get_density = semicircle_density</code>  <code>module-attribute</code>","text":""},{"location":"api/transforms/#free_matrix_laws.transforms.polynomial_density","title":"<code>polynomial_density = polynomial_semicircle_density</code>  <code>module-attribute</code>","text":""},{"location":"api/transforms/#free_matrix_laws.transforms.solve_cauchy_semicircle","title":"<code>solve_cauchy_semicircle(z, A, G0=None, tol=1e-10, maxiter=500)</code>","text":"<p>Solve the operator-valued semicircle equation $$ z\\,G \\;=\\; I \\;+\\; \\eta(G)\\,G, \\qquad \\Im z&gt;0, $$ by fixed-point iteration using the half-averaged map $$ G \\;\\mapsto\\; \\tfrac12\\Big[\\,G + (\\,zI - \\eta(G)\\,)^{-1}\\Big]. $$</p> <p>This follows the numerical damping suggested by Helton-Rashidi Far\u2013Speicher (IMRN 2007).</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>complex</code> <p>Spectral parameter with \\(\\Im z&gt;0\\).</p> required <code>A</code> <code>sequence of $(n,n)$ arrays or stacked $(s,n,n)$ array</code> <p>Kraus operators \\(A_i\\) defining \\(\\eta(B)=\\sum_i A_i B A_i^\\ast\\).</p> required <code>G0</code> <code>(n,n) array</code> <p>Initial iterate (defaults to \\(-iI\\)).</p> <code>None</code> <code>tol</code> <code>float</code> <p>Relative fixed-point tolerance.</p> <code>1e-10</code> <code>maxiter</code> <code>int</code> <p>Maximum iterations.</p> <code>500</code> <p>Returns:</p> Type Description <code>(n, n) ndarray</code> <p>Approximate solution \\(G(z)\\).</p> Notes <p>The residual \\(R=zG-I-\\eta(G)G\\) should be small at convergence.</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def solve_cauchy_semicircle(z: complex, A, G0: np.ndarray | None = None,\n                            tol: float = 1e-10, maxiter: int = 500) -&gt; np.ndarray:\n    r'''\n    Solve the operator-valued semicircle equation\n    $$ z\\,G \\;=\\; I \\;+\\; \\eta(G)\\,G, \\qquad \\Im z&gt;0, $$\n    by fixed-point iteration using the half-averaged map\n    $$ G \\;\\mapsto\\; \\tfrac12\\Big[\\,G + (\\,zI - \\eta(G)\\,)^{-1}\\Big]. $$\n\n    This follows the numerical damping suggested by Helton-Rashidi Far\u2013Speicher (IMRN 2007).\n\n    Parameters\n    ----------\n    z : complex\n        Spectral parameter with $\\Im z&gt;0$.\n    A : sequence of $(n,n)$ arrays or stacked $(s,n,n)$ array\n        Kraus operators $A_i$ defining $\\eta(B)=\\sum_i A_i B A_i^\\ast$.\n    G0 : (n,n) array, optional\n        Initial iterate (defaults to $-iI$).\n    tol : float\n        Relative fixed-point tolerance.\n    maxiter : int\n        Maximum iterations.\n\n    Returns\n    -------\n    (n, n) ndarray\n        Approximate solution $G(z)$.\n\n    Notes\n    -----\n    The residual $R=zG-I-\\eta(G)G$ should be small at convergence.\n    '''\n    # infer n from A\n    n = (A[0].shape[0] if isinstance(A, (list, tuple)) else A.shape[-1])\n    G = (-1j * np.eye(n)) if G0 is None else np.array(G0, dtype=complex)\n\n    for _ in range(maxiter):\n        G_next = _hfs_map(G, z, A)\n        if la.norm(G_next - G) &lt;= tol * (1 + la.norm(G)):\n            return G_next\n        G = G_next\n    return G\n</code></pre>"},{"location":"api/transforms/#free_matrix_laws.transforms.solve_cauchy_biased","title":"<code>solve_cauchy_biased(z, a0, A, G0=None, tol=1e-12, maxiter=5000, relax=0.5, return_info=False)</code>","text":"<p>Fixed-point solver for the biased operator-valued semicircle: $$   G \\;=\\; (z I - a_0 - \\eta(G))^{-1},   \\qquad \\Im z&gt;0 . $$</p> <p>Initial guess chosen so that \\(\\Im G(z)&lt;0\\) when \\(\\Im z&gt;0\\)</p> <p>Iteration: $$   G_{k+1} \\;=\\; (1-\\text{relax})\\,G_k \\;+\\; \\text{relax}\\,[\\,z I - b\\,\\eta(G_k)\\,]^{-1} b,   \\quad b=z(z I - a_0)^{-1}. $$</p> <p>Convergence check uses the equation form $$   R(G):= (z I - a_0)\\,G - I - \\eta(G)\\,G $$ and stops when \\(\\|R(G)\\|_{\\mathrm{F}} \\le \\text{tol}\\).</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>complex</code> <p>Spectral parameter with \\(\\Im z&gt;0\\).</p> required <code>a0</code> <code>(n,n) ndarray</code> <p>Bias matrix.</p> required <code>A</code> <code>sequence[(n,n)] or (s,n,n) ndarray</code> <p>Kraus operators for \\(\\eta\\).</p> required <code>G0</code> <code>(n,n) ndarray</code> <p>Warm start; if None, uses \\((\\Im z)^{-1} i\\,I\\).</p> <code>None</code> <code>tol</code> <code>float</code> <p>Frobenius-norm tolerance on the residual.</p> <code>1e-12</code> <code>maxiter</code> <code>int</code> <p>Iteration cap.</p> <code>5000</code> <code>relax</code> <code>float</code> <p>Averaging parameter in \\((0,1]\\).</p> <code>0.5</code> <code>return_info</code> <code>bool</code> <p>If True, also return a dict with residual and iterations.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>G</code> <code>(n,n) ndarray</code> <code>info</code> <code>dict (only if return_info=True)</code> <p>Keys: <code>residual</code>, <code>iters</code>.</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def solve_cauchy_biased(z: complex,\n                        a0: np.ndarray,\n                        A,\n                        G0: np.ndarray | None = None,\n                        tol: float = 1e-12,\n                        maxiter: int = 5000,\n                        relax: float = 0.5,\n                        return_info: bool = False):\n    r'''\n    Fixed-point solver for the **biased** operator-valued semicircle:\n    $$\n      G \\;=\\; (z I - a_0 - \\eta(G))^{-1},\n      \\qquad \\Im z&gt;0 .\n    $$\n\n    Initial guess chosen so that $\\Im G(z)&lt;0$ when $\\Im z&gt;0$\n\n    Iteration:\n    $$\n      G_{k+1} \\;=\\; (1-\\text{relax})\\,G_k \\;+\\; \\text{relax}\\,[\\,z I - b\\,\\eta(G_k)\\,]^{-1} b,\n      \\quad b=z(z I - a_0)^{-1}.\n    $$\n\n    Convergence check uses the equation form\n    $$\n      R(G):= (z I - a_0)\\,G - I - \\eta(G)\\,G\n    $$\n    and stops when $\\|R(G)\\|_{\\mathrm{F}} \\le \\text{tol}$.\n\n    Parameters\n    ----------\n    z : complex\n        Spectral parameter with $\\Im z&gt;0$.\n    a0 : (n,n) ndarray\n        Bias matrix.\n    A : sequence[(n,n)] or (s,n,n) ndarray\n        Kraus operators for $\\eta$.\n    G0 : (n,n) ndarray, optional\n        Warm start; if None, uses $(\\Im z)^{-1} i\\,I$.\n    tol : float, default 1e-12\n        Frobenius-norm tolerance on the residual.\n    maxiter : int, default 5000\n        Iteration cap.\n    relax : float, default 0.5\n        Averaging parameter in $(0,1]$.\n    return_info : bool, default False\n        If True, also return a dict with residual and iterations.\n\n    Returns\n    -------\n    G : (n,n) ndarray\n    info : dict (only if return_info=True)\n        Keys: ``residual``, ``iters``.\n    '''\n    n = a0.shape[0]\n    I = np.eye(n, dtype=complex)\n    if G0 is None:\n        eps = float(np.imag(z))\n        if eps &lt;= 0:\n            eps = 1e-2\n        #(Herglotz sign: Im G(z) &lt; 0 for Im z &gt; 0)\n        G = -1j * I / eps\n    else:\n        G = G0.astype(complex, copy=True)\n\n    for k in range(1, maxiter + 1):\n        G = hfsb_map(G, z, a0, A, relax=relax)\n        # residual: (zI - a0)G - I - eta(G)G\n        r = (z * I - a0) @ G - I - eta(G, A) @ G\n        res = la.norm(r, 'fro')\n        if res &lt;= tol:\n            if return_info:\n                return G, {\"residual\": res, \"iters\": k}\n            return G\n    if return_info:\n        return G, {\"residual\": res, \"iters\": maxiter}\n    return G\n</code></pre>"},{"location":"api/transforms/#free_matrix_laws.transforms.semicircle_density","title":"<code>semicircle_density(x, A, eps=0.01, G0=None, tol=1e-10, maxiter=10000, a0=None)</code>","text":"<p>Stieltjes inversion for the matrix semicircle (optionally with bias).</p> <p>Unbiased case (\\(a_0\\) is <code>None</code>): compute \\(G(z)\\) for \\(z=x+i\\varepsilon\\) from $$ z\\,G \\;=\\; I \\;+\\; \\eta(G)\\,G, \\qquad \\eta(B)=\\sum_{i=1}^s A_i B A_i^\\ast, $$ then return the scalar density $$ f(x) \\;=\\; -\\frac{1}{\\pi}\\,\\Im!\\left(\\frac{1}{n}\\,\\mathrm{tr}\\,G(x+i\\varepsilon)\\right). $$</p> <p>Biased case (\\(a_0\\neq 0\\)): compute the Cauchy transform \\(G_{a_0+X}(z)\\) via the biased solver (internally equivalent to the half-averaged Helton\u2013Rashidi Far\u2013Speicher iteration with \\(b=z(zI-a_0)^{-1}\\)), and apply the same reduction.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Real evaluation point.</p> required <code>A</code> <code>sequence of $(n,n)$ arrays or stacked array $(s,n,n)$</code> <p>Kraus operators \\(A_i\\) (no self-adjointness required).</p> required <code>eps</code> <code>float</code> <p>Imaginary offset \\(\\varepsilon&gt;0\\) used for \\(z=x+i\\varepsilon\\).</p> <code>1e-2</code> <code>G0</code> <code>(n,n) array</code> <p>Initial iterate for \\(G\\) (passed to the solver).</p> <code>None</code> <code>tol</code> <code>float</code> <p>Relative fixed-point tolerance for the solver.</p> <code>1e-10</code> <code>maxiter</code> <code>int</code> <p>Maximum iterations.</p> <code>10000</code> <code>a0</code> <code>(n,n) array or `None`</code> <p>Bias matrix. If provided, computes the density for \\(a_0 + \\sum_i A_i \\otimes X_i\\).</p> <code>`None`</code> <p>Returns:</p> Type Description <code>float</code> <p>Approximation to \\(f(x)\\).</p> Notes <p>This computes \\(m(z)=\\tfrac{1}{n}\\mathrm{tr}\\,G(z)\\) and uses \\(f(x)=-(1/\\pi)\\Im m(x+i\\varepsilon)\\).</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def semicircle_density(\n    x: float,\n    A,\n    eps: float = 1e-2,\n    G0=None,\n    tol: float = 1e-10,\n    maxiter: int = 10_000,\n    a0=None,\n) -&gt; float:\n    r'''\n    Stieltjes inversion for the **matrix semicircle** (optionally with bias).\n\n    Unbiased case ($a_0$ is `None`): compute $G(z)$ for $z=x+i\\varepsilon$ from\n    $$ z\\,G \\;=\\; I \\;+\\; \\eta(G)\\,G, \\qquad \\eta(B)=\\sum_{i=1}^s A_i B A_i^\\ast, $$\n    then return the scalar density\n    $$ f(x) \\;=\\; -\\frac{1}{\\pi}\\,\\Im\\!\\left(\\frac{1}{n}\\,\\mathrm{tr}\\,G(x+i\\varepsilon)\\right). $$\n\n    Biased case ($a_0\\neq 0$): compute the Cauchy transform $G_{a_0+X}(z)$ via the\n    biased solver (internally equivalent to the half-averaged Helton\u2013Rashidi\n    Far\u2013Speicher iteration with $b=z(zI-a_0)^{-1}$), and apply the same reduction.\n\n    Parameters\n    ----------\n    x : float\n        Real evaluation point.\n    A : sequence of $(n,n)$ arrays or stacked array $(s,n,n)$\n        Kraus operators $A_i$ (no self-adjointness required).\n    eps : float, default 1e-2\n        Imaginary offset $\\varepsilon&gt;0$ used for $z=x+i\\varepsilon$.\n    G0 : (n,n) array, optional\n        Initial iterate for $G$ (passed to the solver).\n    tol : float, default 1e-10\n        Relative fixed-point tolerance for the solver.\n    maxiter : int, default 10000\n        Maximum iterations.\n    a0 : (n,n) array or `None`, default `None`\n        Bias matrix. If provided, computes the density for $a_0 + \\sum_i A_i \\otimes X_i$.\n\n    Returns\n    -------\n    float\n        Approximation to $f(x)$.\n\n    Notes\n    -----\n    This computes $m(z)=\\tfrac{1}{n}\\mathrm{tr}\\,G(z)$ and uses\n    $f(x)=-(1/\\pi)\\Im m(x+i\\varepsilon)$.\n    '''\n    if eps &lt;= 0:\n        raise ValueError(\"eps must be &gt; 0\")\n\n    # Infer n from A (list/tuple or stacked array)\n    if isinstance(A, np.ndarray) and A.ndim == 3:\n        n = A.shape[-1]\n    elif isinstance(A, np.ndarray) and A.ndim == 2:\n        n = A.shape[0]\n        A = A[None, ...]\n    else:\n        A = list(A)\n        n = A[0].shape[0]\n\n    if a0 is not None:\n        a0 = np.asarray(a0)\n        if a0.shape != (n, n):\n            raise ValueError(f\"a0 must have shape {(n,n)}, got {a0.shape}\")\n\n    z = float(x) + 1j * float(eps)\n\n    if a0 is None:\n        G = solve_cauchy_semicircle(z, A, G0=G0, tol=tol, maxiter=maxiter)\n    else:\n        # default return of solve_cauchy_biased is G (not (G, info))\n        G = solve_cauchy_biased(z, a0, A, G0=G0, tol=tol, maxiter=maxiter)\n\n    m = np.trace(G) / n\n    f = (-1.0 / np.pi) * np.imag(m)\n    return float(f)\n</code></pre>"},{"location":"api/transforms/#free_matrix_laws.transforms.biased_semicircle_density","title":"<code>biased_semicircle_density(x, a0, A, eps=0.01, G0=None, tol=1e-10, maxiter=10000)</code>","text":"<p>Convenience alias for the biased case: $$ f_{a_0}(x) \\;=\\; -\\frac{1}{\\pi}\\,\\Im!\\left(\\frac{1}{n}\\,\\mathrm{tr}\\,G_{a_0+X}(x+i\\varepsilon)\\right). $$ Calls <code>semicircle_density(x, A, eps, G0, tol, maxiter, a0=a0)</code>.</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def biased_semicircle_density(\n    x: float,\n    a0,\n    A,\n    eps: float = 1e-2,\n    G0=None,\n    tol: float = 1e-10,\n    maxiter: int = 10_000,\n) -&gt; float:\n    r'''\n    Convenience alias for the biased case:\n    $$ f_{a_0}(x) \\;=\\; -\\frac{1}{\\pi}\\,\\Im\\!\\left(\\frac{1}{n}\\,\\mathrm{tr}\\,G_{a_0+X}(x+i\\varepsilon)\\right). $$\n    Calls `semicircle_density(x, A, eps, G0, tol, maxiter, a0=a0)`.\n    '''\n    return semicircle_density(x, A, eps=eps, G0=G0, tol=tol, maxiter=maxiter, a0=a0)\n</code></pre>"},{"location":"api/transforms/#free_matrix_laws.transforms.semicircle_density_scalar","title":"<code>semicircle_density_scalar(x, c=1.0)</code>","text":"<p>Classical (scalar) Wigner semicircle density with variance \\(c&gt;0\\).</p> <p>Support is \\([-2\\sqrt{c},\\,2\\sqrt{c}]\\) with $$   f(x) \\;=\\; \\frac{1}{2\\pi c}\\,\\sqrt{\\,4c - x^2\\,}\\,\\mathbf 1_{{|x|\\le 2\\sqrt{c}}}. $$</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float or array_like</code> <p>Evaluation point(s).</p> required <code>c</code> <code>float</code> <p>Variance parameter (\\(c&gt;0\\)).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>\\(f(x)\\), vectorized over <code>x</code>.</p> Notes <p>Parameterization by variance \\(c\\) (so radius is \\(2\\sqrt{c}\\)).</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def semicircle_density_scalar(x, c: float = 1.0):\n    r'''\n    Classical (scalar) Wigner semicircle density with variance $c&gt;0$.\n\n    Support is $[-2\\sqrt{c},\\,2\\sqrt{c}]$ with\n    $$\n      f(x) \\;=\\; \\frac{1}{2\\pi c}\\,\\sqrt{\\,4c - x^2\\,}\\,\\mathbf 1_{\\{|x|\\le 2\\sqrt{c}\\}}.\n    $$\n\n    Parameters\n    ----------\n    x : float or array_like\n        Evaluation point(s).\n    c : float, default 1.0\n        Variance parameter ($c&gt;0$).\n\n    Returns\n    -------\n    float or ndarray\n        $f(x)$, vectorized over `x`.\n\n    Notes\n    -----\n    Parameterization by variance $c$ (so radius is $2\\sqrt{c}$).\n    '''\n    if c &lt;= 0:\n        raise ValueError(\"c must be &gt; 0\")\n    x_arr = np.asarray(x, dtype=float)\n    inside = 4.0 * c - x_arr**2\n    y = np.where(inside &gt; 0.0, (1.0 / (2.0 * np.pi * c)) * np.sqrt(inside), 0.0)\n    return y if x_arr.ndim else float(y)\n</code></pre>"},{"location":"api/transforms/#free_matrix_laws.transforms.semicircle_cauchy_scalar","title":"<code>semicircle_cauchy_scalar(z, c=1.0)</code>","text":"<p>Scalar Cauchy (Stieltjes) transform of the Wigner semicircle law with variance c&gt;0.</p> <p>For c=1 (support [-2,2]):     G(z) = (z - sqrt(z^2 - 4))/2</p> <p>More generally (support [-2sqrt(c), 2sqrt(c)]):     G(z) = (z - sqrt(z^2 - 4c)) / (2c)</p> <p>The square-root branch is chosen so that Im(z)&gt;0 =&gt; Im(G(z))&lt;0. For boundary values on the real line, use z = x + 1j*eps with eps&gt;0.</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def semicircle_cauchy_scalar(z, c: float = 1.0):\n    r\"\"\"\n    Scalar Cauchy (Stieltjes) transform of the Wigner semicircle law with variance c&gt;0.\n\n    For c=1 (support [-2,2]):\n        G(z) = (z - sqrt(z^2 - 4))/2\n\n    More generally (support [-2*sqrt(c), 2*sqrt(c)]):\n        G(z) = (z - sqrt(z^2 - 4c)) / (2c)\n\n    The square-root branch is chosen so that Im(z)&gt;0 =&gt; Im(G(z))&lt;0.\n    For boundary values on the real line, use z = x + 1j*eps with eps&gt;0.\n    \"\"\"\n    if c &lt;= 0:\n        raise ValueError(\"c must be &gt; 0\")\n\n    z_arr = np.asarray(z, dtype=np.complex128)\n    disc = np.sqrt(z_arr**2 - 4.0 * c)\n\n    # Enforce Herglotz symmetry: Im(z)&gt;0 -&gt; Im(G)&lt;0 (and vice versa)\n    disc = np.where(disc.imag * z_arr.imag &lt; 0, -disc, disc)\n\n    G = (z_arr - disc) / (2.0 * c)\n    return G if z_arr.ndim else complex(G)\n</code></pre>"},{"location":"api/transforms/#free_matrix_laws.transforms.solve_cauchy_linearized","title":"<code>solve_cauchy_linearized(z, a0, A, *, eps_reg=1e-06, block_size=1, G0=None, tol=1e-10, maxiter=10000, return_info=False)</code>","text":"<p>Solve for the regularized \u201cquasi-resolvent\u201d fixed point associated to a self-adjoint linearization \\(L_p\\).</p> <p>We assume a self-adjoint linearization $$   L_p = a_0 + \\sum_{i=1}^s A_i \\otimes X_i, $$ with semicircular \\(X_i\\), and we form $$   b_\\varepsilon(z) := z\\big(\\Lambda_\\varepsilon(z)-a_0\\big)^{-1},   \\qquad   \\eta(B) := \\sum_{i=1}^s A_i\\,B\\,A_i^\\ast, $$ where $$ \\Lambda_\\varepsilon(z)=\\operatorname{diag} \\big(z I_k,\\ i\\varepsilon\\, I_{n-k}\\big), \\qquad k=\\text{block_size}. $$</p> <p>The iteration uses the half-averaged update $$   G_{new} = \\frac12\\Big[G + \\big(zI - b_\\varepsilon(z)\\,\\eta(G)\\big)^{-1} b_\\varepsilon(z)\\Big], $$ and stops when \\(\\|G_{new}-G\\|_F \\le \\text{tol}\\,\\|G\\|_F\\) (relative Frobenius criterion).</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>complex</code> <p>Spectral parameter with \\(\\Im z&gt;0\\) (typically \\(z=x+i\\,\\varepsilon\\)).</p> required <code>a0</code> <code>(n,n) array</code> <p>The bias / constant term of the linearization.</p> required <code>A</code> <code>sequence of (n,n) arrays or stacked array (s,n,n)</code> <p>Coefficients \\(A_i\\) defining \\(\\eta(B)=\\sum_i A_i B A_i^\\ast\\).</p> required <code>eps_reg</code> <code>float</code> <p>Regularization parameter in \\(\\Lambda_\\varepsilon(z)\\) (used on the lower block).</p> <code>1e-6</code> <code>block_size</code> <code>int</code> <p>Size \\(k\\) of the distinguished top-left block (the one used to recover the scalar Cauchy transform).</p> <code>1</code> <code>G0</code> <code>(n,n) array</code> <p>Initial iterate. If None, uses \\(G_0 = (1/z)I\\).</p> <code>None</code> <code>tol</code> <code>float</code> <p>Relative tolerance.</p> <code>1e-10</code> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>10000</code> <code>return_info</code> <code>bool</code> <p>If True, also return a dict with diagnostics.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>G</code> <code>(n,n) array</code> <p>Approximation to the fixed point \\(G(z,b_\\varepsilon(z))\\).</p> <code>info</code> <code>dict(optional)</code> <p>Keys: 'iters', 'last_diff'.</p> Notes <p>After computing \\(G(z,b_\\varepsilon(z))\\), the scalar Cauchy transform of \\(p\\) is obtained from the distinguished corner via $$   m_p(z) \\approx \\frac{1}{k}\\,\\mathrm{tr}\\,\\big(G(z,b_\\varepsilon(z))\\big)_{11}, $$ with \\(k=\\text{block\\_size}\\) and \\((\\cdot)_{11}\\) the top-left \\(k\\times k\\) block.</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def solve_cauchy_linearized(\n    z: complex,\n    a0: np.ndarray,\n    A,\n    *,\n    eps_reg: float = 1e-6,\n    block_size: int = 1,\n    G0: np.ndarray | None = None,\n    tol: float = 1e-10,\n    maxiter: int = 10_000,\n    return_info: bool = False,\n):\n    r'''\n    Solve for the regularized \u201cquasi-resolvent\u201d fixed point associated to a\n    self-adjoint linearization $L_p$.\n\n    We assume a self-adjoint linearization\n    $$\n      L_p = a_0 + \\sum_{i=1}^s A_i \\otimes X_i,\n    $$\n    with semicircular $X_i$, and we form\n    $$\n      b_\\varepsilon(z) := z\\big(\\Lambda_\\varepsilon(z)-a_0\\big)^{-1},\n      \\qquad\n      \\eta(B) := \\sum_{i=1}^s A_i\\,B\\,A_i^\\ast,\n    $$\n    where\n    $$\n    \\Lambda_\\varepsilon(z)=\\operatorname{diag} \\big(z I_k,\\ i\\varepsilon\\, I_{n-k}\\big), \\qquad k=\\text{block\\_size}.\n    $$\n\n    The iteration uses the half-averaged update\n    $$\n      G_{new} = \\frac12\\Big[G + \\big(zI - b_\\varepsilon(z)\\,\\eta(G)\\big)^{-1} b_\\varepsilon(z)\\Big],\n    $$\n    and stops when $\\|G_{new}-G\\|_F \\le \\text{tol}\\,\\|G\\|_F$ (relative Frobenius criterion).\n\n    Parameters\n    ----------\n    z : complex\n        Spectral parameter with $\\Im z&gt;0$ (typically $z=x+i\\,\\varepsilon$).\n    a0 : (n,n) array\n        The bias / constant term of the linearization.\n    A : sequence of (n,n) arrays or stacked array (s,n,n)\n        Coefficients $A_i$ defining $\\eta(B)=\\sum_i A_i B A_i^\\ast$.\n    eps_reg : float, default 1e-6\n        Regularization parameter in $\\Lambda_\\varepsilon(z)$ (used on the lower block).\n    block_size : int, default 1\n        Size $k$ of the distinguished top-left block (the one used to recover the scalar Cauchy transform).\n    G0 : (n,n) array, optional\n        Initial iterate. If None, uses $G_0 = (1/z)I$.\n    tol : float, default 1e-10\n        Relative tolerance.\n    maxiter : int, default 10000\n        Maximum number of iterations.\n    return_info : bool, default False\n        If True, also return a dict with diagnostics.\n\n    Returns\n    -------\n    G : (n,n) array\n        Approximation to the fixed point $G(z,b_\\varepsilon(z))$.\n    info : dict (optional)\n        Keys: 'iters', 'last_diff'.\n\n    Notes\n    -----\n    After computing $G(z,b_\\varepsilon(z))$, the scalar Cauchy transform of $p$\n    is obtained from the distinguished corner via\n    $$\n      m_p(z) \\approx \\frac{1}{k}\\,\\mathrm{tr}\\,\\big(G(z,b_\\varepsilon(z))\\big)_{11},\n    $$\n    with $k=\\text{block\\_size}$ and $(\\cdot)_{11}$ the top-left $k\\times k$ block.\n    '''\n    a0 = np.asarray(a0)\n    if a0.ndim != 2 or a0.shape[0] != a0.shape[1]:\n        raise ValueError(f\"a0 must be square; got {a0.shape!r}\")\n    n = a0.shape[0]\n\n    if G0 is None:\n        G = (1.0 / complex(z)) * np.eye(n, dtype=complex)\n    else:\n        G = np.asarray(G0, dtype=complex)\n        if G.shape != (n, n):\n            raise ValueError(f\"G0 must have shape {(n,n)!r}; got {G.shape!r}\")\n\n    last_diff = np.inf\n    for k in range(1, maxiter + 1):\n        G1 = _hfsc_map(G, z, a0, A, eps_reg=eps_reg, block_size=block_size)\n        diff = la.norm(G1 - G, \"fro\")\n        denom = max(1.0, la.norm(G, \"fro\"))\n        last_diff = diff\n\n        if diff &lt;= tol * denom:\n            G = G1\n            break\n\n        G = G1\n\n    if return_info:\n        return G, {\"iters\": k, \"last_diff\": float(last_diff)}\n    return G\n</code></pre>"},{"location":"api/transforms/#free_matrix_laws.transforms.cauchy_matrix_semicircle_reference","title":"<code>cauchy_matrix_semicircle_reference(w, b, *, c=1.0, n_quad=256)</code>","text":"<p>Reference (brute-force) matrix Cauchy transform for a scalar semicircle.</p> <p>We compute the matrix-valued Cauchy transform $$   G(w;b)\\;=\\;\\mathbb{E}\\big[(w - bS)^{-1}\\big]   \\;=\\;\\int_{-2\\sqrt{c}}^{2\\sqrt{c}} (w - tb)^{-1}\\, f_c(t)\\,dt, $$ where \\(S\\) is a scalar Wigner semicircle with variance \\(c&gt;0\\) and density $$   f_c(t)=\\frac{1}{2\\pi c}\\sqrt{4c-t^2}\\,\\mathbf 1_{{|t|\\le 2\\sqrt c}}. $$</p> <p>For numerical stability, one typically takes \\(w\\) in the operator upper half-plane (e.g. \\(w=zI\\) with \\(\\Im z&gt;0\\)), so that all resolvents \\((w-tb)^{-1}\\) exist.</p> <p>Implementation detail (why this quadrature): Substitute \\(t=2\\sqrt c\\,x\\) to get $$   G(w;b)=\\frac{2}{\\pi}\\int_{-1}^1 (w-2\\sqrt c\\,x\\,b)^{-1}\\sqrt{1-x^2}\\,dx. $$ The weight \\(\\sqrt{1-x^2}\\) suggests Gauss\u2013Chebyshev quadrature of the 2nd kind: $$   x_k=\\cos\\frac{k\\pi}{n+1},\\qquad   \\alpha_k=\\frac{2}{n+1}\\sin^2\\frac{k\\pi}{n+1}, $$ giving $$   G(w;b)\\approx \\sum_{k=1}^n \\alpha_k\\,(w - (2\\sqrt c\\,x_k)b)^{-1}. $$ The weights satisfy \\(\\sum_k \\alpha_k = 1\\), so for \\(b=0\\) the method returns \\(w^{-1}\\) up to floating-point rounding.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>complex scalar or (n,n) ndarray</code> <p>Spectral parameter/matrix.</p> required <code>b</code> <code>(n,n) ndarray</code> <p>Deterministic matrix coefficient.</p> required <code>c</code> <code>float</code> <p>Variance of the semicircle (support is \\([-2\\sqrt c,2\\sqrt c]\\)).</p> <code>1.0</code> <code>n_quad</code> <code>int</code> <p>Number of Chebyshev nodes (larger = more accurate, slower).</p> <code>256</code> <p>Returns:</p> Type Description <code>(n,n) ndarray</code> <p>Approximation to \\(G(w;b)\\).</p> Source code in <code>src/free_matrix_laws/transforms.py</code> <pre><code>def cauchy_matrix_semicircle_reference(w, b, *, c: float = 1.0, n_quad: int = 256):\n    r\"\"\"\n    Reference (brute-force) matrix Cauchy transform for a scalar semicircle.\n\n    We compute the matrix-valued Cauchy transform\n    $$\n      G(w;b)\\;=\\;\\mathbb{E}\\big[(w - bS)^{-1}\\big]\n      \\;=\\;\\int_{-2\\sqrt{c}}^{2\\sqrt{c}} (w - tb)^{-1}\\, f_c(t)\\,dt,\n    $$\n    where $S$ is a *scalar* Wigner semicircle with variance $c&gt;0$ and density\n    $$\n      f_c(t)=\\frac{1}{2\\pi c}\\sqrt{4c-t^2}\\,\\mathbf 1_{\\{|t|\\le 2\\sqrt c\\}}.\n    $$\n\n    For numerical stability, one typically takes $w$ in the operator upper half-plane\n    (e.g. $w=zI$ with $\\Im z&gt;0$), so that all resolvents $(w-tb)^{-1}$ exist.\n\n    Implementation detail (why this quadrature):\n    Substitute $t=2\\sqrt c\\,x$ to get\n    $$\n      G(w;b)=\\frac{2}{\\pi}\\int_{-1}^1 (w-2\\sqrt c\\,x\\,b)^{-1}\\sqrt{1-x^2}\\,dx.\n    $$\n    The weight $\\sqrt{1-x^2}$ suggests Gauss\u2013Chebyshev quadrature of the 2nd kind:\n    $$\n      x_k=\\cos\\frac{k\\pi}{n+1},\\qquad\n      \\alpha_k=\\frac{2}{n+1}\\sin^2\\frac{k\\pi}{n+1},\n    $$\n    giving\n    $$\n      G(w;b)\\approx \\sum_{k=1}^n \\alpha_k\\,(w - (2\\sqrt c\\,x_k)b)^{-1}.\n    $$\n    The weights satisfy $\\sum_k \\alpha_k = 1$, so for $b=0$ the method returns $w^{-1}$\n    up to floating-point rounding.\n\n    Parameters\n    ----------\n    w : complex scalar or (n,n) ndarray\n        Spectral parameter/matrix.\n    b : (n,n) ndarray\n        Deterministic matrix coefficient.\n    c : float, default 1.0\n        Variance of the semicircle (support is $[-2\\sqrt c,2\\sqrt c]$).\n    n_quad : int, default 256\n        Number of Chebyshev nodes (larger = more accurate, slower).\n\n    Returns\n    -------\n    (n,n) ndarray\n        Approximation to $G(w;b)$.\n    \"\"\"\n    if c &lt;= 0:\n        raise ValueError(\"c must be &gt; 0\")\n    b = np.asarray(b, dtype=np.complex128)\n    if b.ndim != 2 or b.shape[0] != b.shape[1]:\n        raise ValueError(\"b must be a square (n,n) matrix\")\n    n = b.shape[0]\n\n    w_arr = np.asarray(w, dtype=np.complex128)\n    w_mat = (w_arr * np.eye(n, dtype=np.complex128)) if w_arr.ndim == 0 else w_arr\n    if w_mat.shape != (n, n):\n        raise ValueError(f\"w must be scalar or have shape {(n,n)}, got {w_mat.shape}\")\n\n    k = np.arange(1, n_quad + 1, dtype=np.float64)\n    theta = k * np.pi / (n_quad + 1.0)\n    x = np.cos(theta)                          # nodes in [-1,1]\n    alpha = (2.0 / (n_quad + 1.0)) * (np.sin(theta) ** 2)  # weights sum to 1\n    t = 2.0 * np.sqrt(c) * x\n\n    I = np.eye(n, dtype=np.complex128)\n    G = np.zeros((n, n), dtype=np.complex128)\n    for ak, tk in zip(alpha, t):\n        G += ak * la.solve(w_mat - tk * b, I)\n    return G\n</code></pre>"},{"location":"howto/","title":"How-to guides","text":"<p>Short, task-oriented notes for common calculations with free-matrix-laws. Each guide shows the minimal imports, a small example, and any caveats.</p>"},{"location":"howto/#quick-start","title":"Quick start","text":"<pre><code>%pip install -e ..\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nfrom free_matrix_laws import eta, covariance_map  # same function; pick your preferred \n</code></pre>"},{"location":"howto/biased_semicircle_density/","title":"How to: <code>biased_semicircle_density</code>","text":"<p>Density for the biased matrix semicircle \\(S=a_0+\\sum_{i=1}^s A_i\\otimes X_i\\), where \\(X_i\\) are free semicirculars.</p> <pre><code>import numpy as np\nfrom free_matrix_laws import biased_semicircle_density\n</code></pre>"},{"location":"howto/biased_semicircle_density/#minimal-example","title":"Minimal example","text":"<pre><code>n = 3\na0 = np.array([[0,0,0],[0,0,-1],[0,-1,0]], dtype=float)\nA1 = np.array([[0,1,0],[1,0,0],[0,0,0]], dtype=float)\nA2 = np.array([[0,0,1],[0,0,0],[1,0,0]], dtype=float)\nA  = [A1, A2]\n\nx  = 0.0\nfx = biased_semicircle_density(x, a0, A, eps=1e-2)\nfx\n</code></pre>"},{"location":"howto/biased_semicircle_density/#vector-of-points","title":"Vector of points","text":"<pre><code>xs  = np.linspace(-5, 5, 400)\nfxs = np.array([biased_semicircle_density(x, a0, A, eps=5e-3) for x in xs])\n</code></pre>"},{"location":"howto/biased_semicircle_density/#signature","title":"Signature","text":"<pre><code>biased_semicircle_density(\n    x: float,\n    a0,                # (n,n) bias matrix\n    A,                 # list/tuple of (n,n) or stacked (s,n,n)\n    eps: float = 1e-2,\n    G0=None,           # optional warm start for G(z)\n    tol: float = 1e-10,\n    maxiter: int = 10_000,\n) -&gt; float\n</code></pre>"},{"location":"howto/biased_semicircle_density/#what-it-does","title":"What it does","text":"<p>For \\(z=x+i\\varepsilon\\) it computes the Cauchy transform \\(G_{a_0+X}(z)\\) using the Helton\u2013Rashidi Far\u2013Speicher half-averaged step adapted to the bias: $$ b \\;=\\; z\\,(zI-a_0)^{-1},\\qquad G \\leftarrow \\tfrac{1}{2}\\Big[\\,G + (zI - b\\,\\eta(G))^{-1} b\\,\\Big], $$ then returns $$ f(x) \\;=\\; -\\frac{1}{\\pi}\\,\\Im!\\left(\\frac{1}{n}\\,\\mathrm{tr}\\,G(x+i\\varepsilon)\\right). $$</p>"},{"location":"howto/biased_semicircle_density/#tips","title":"Tips","text":"<ul> <li>Reduction: if \\(a_0=0\\), this reduces numerically to <code>semicircle_density</code>.</li> <li>Conditioning: if \\((zI-a_0)^{-1}\\) is ill-conditioned, increase \\(\\varepsilon\\) slightly.</li> <li>Use the same advice as for \\(\\varepsilon\\), <code>G0</code>, <code>tol</code>, and <code>maxiter</code>.</li> </ul>"},{"location":"howto/biased_semicircle_density/#sanity-check-reduction","title":"Sanity check (reduction)","text":"<pre><code>rng = np.random.default_rng(0)\nn, s = 4, 3\nA = [rng.standard_normal((n,n)) + 1j*rng.standard_normal((n,n)) for _ in range(s)]\na0 = np.zeros((n,n), dtype=complex)\nx  = 0.3\n\nfrom free_matrix_laws import semicircle_density\nf1 = biased_semicircle_density(x, a0, A, eps=1e-2)\nf2 = semicircle_density(x, A, eps=1e-2)\nassert abs(f1 - f2) &lt; 1e-6\n</code></pre> <p>See also: <code>semicircle_density</code>.</p>"},{"location":"howto/covariance_map/","title":"How to use <code>covariance_map</code>","text":"<p>Below is a minimal example. </p> <pre><code>import numpy as np\nfrom free_matrix_laws import covariance_map as eta\n\nn, s = 3, 2\nA1 = np.eye(n)\nA2 = 2*np.eye(n)\nB  = np.array([[0.,1.,0.],[1.,0.,0.],[0.,0.,0.]])\n\nval = eta(B, [A1, A2])          # list of A_i\n# or: val = eta(B, np.stack([A1, A2], axis=0))  # (s,n,n) stack\n\n# sanity check against a loop\nval_loop = A1 @ B @ A1 + A2 @ B @ A2\nassert np.allclose(val, val_loop)\n</code></pre>"},{"location":"howto/cp_sinkhorn/","title":"How to normalize a CP map via symmetric OSI","text":"<p>Given a CP map $$ T(X)=\\sum_{i=1}^s A_i X A_i^\\ast, $$ one symmetric OSI step forms $$ c_1 = \\big(T(I)\\big)^{-1/2}, \\qquad c_2 = \\big(T^\\ast(I)\\big)^{-1/2}, $$ and produces scaled Kraus operators $$ B_i = c_1 A_i c_2, $$ so that \\(\\mathcal S(T)(X)=\\sum_i B_i X B_i^\\ast\\). Iterating this step often moves \\(T\\) toward the doubly stochastic class (\\(T(I)=I\\), \\(T^\\ast(I)=I\\)).</p>"},{"location":"howto/cp_sinkhorn/#prerequisites","title":"Prerequisites","text":"<pre><code>import numpy as np\nfrom free_matrix_laws import (\n    symmetric_sinkhorn_scale,\n    symmetric_sinkhorn_apply,\n    symmetric_osi,\n    ds_distance,\n)\n</code></pre> <p>Your Kraus operators can be a list/tuple of shape <code>(n,n)</code> arrays or a stacked array of shape <code>(s,n,n)</code>.</p>"},{"location":"howto/cp_sinkhorn/#one-step-scale-the-kraus-operators","title":"One step: scale the Kraus operators","text":"<pre><code>n, s = 4, 3\nrng = np.random.default_rng(0)\nA = rng.standard_normal((s, n, n)) + 1j * rng.standard_normal((s, n, n))\n\nB = symmetric_sinkhorn_scale(A)     # returns scaled Kraus {B_i}\nprint(\"DS distance before:\", ds_distance(A))\nprint(\"DS distance after :\", ds_distance(B))\n</code></pre> <p>Here <code>ds_distance(T) = ||T(I)-I||_F^2 + ||T^\\ast(I)-I||_F^2</code> is a convenient progress metric.</p>"},{"location":"howto/cp_sinkhorn/#apply-the-scaled-map-to-a-matrix-x","title":"Apply the scaled map to a matrix \\(X\\)","text":"<pre><code>X = np.eye(n)\nY = symmetric_sinkhorn_apply(X, A)  # uses one symmetric scaling step internally\n# Equivalent to:\n# B = symmetric_sinkhorn_scale(A, preserve_input_type=False)\n# Y2 = sum(Bi @ X @ Bi.conj().T for Bi in B)\n</code></pre>"},{"location":"howto/cp_sinkhorn/#iterate-to-near-doubly-stochastic","title":"Iterate to near doubly stochastic","text":"<pre><code>B_final, info = symmetric_osi(A, maxiter=50, tol=1e-10, return_history=True)\nprint(\"iters:\", info[\"iters\"])\nprint(\"final DS distance:\", info[\"ds\"])\n</code></pre> <p>If <code>info[\"ds\"]</code> is tiny (e.g., \\(\\le 10^{-10}\\)), the map is numerically close to DS.</p>"},{"location":"howto/cp_sinkhorn/#tips","title":"Tips","text":"<ul> <li>Singular cases: If \\(T(I)\\) or \\(T^\\ast(I)\\) is ill-conditioned, the implementation uses a small eigenvalue floor <code>eps</code> inside the inverse square roots. Raise <code>eps</code> if you see numerical instabilities.</li> <li>Container type: If you pass a list, you\u2019ll get a list back; for a stacked array, a stacked array comes back (configurable via <code>preserve_input_type</code>).</li> <li>Composition: Returning scaled Kraus operators is convenient when you want to reuse them with other routines (e.g., covariance maps or resolvent solvers).</li> </ul>"},{"location":"howto/cp_sinkhorn/#see-also","title":"See also","text":"<ul> <li>API: opvalued (Sinkhorn helpers)</li> </ul>"},{"location":"howto/polynomial_semicircle/","title":"How to: density for a polynomial in semicircular variables (via linearization)","text":"<p>This page documents the helper iteration step <code>hfsc_map</code> and the convenience wrapper <code>polynomial_semicircle_density</code> (short aliases are <code>polynomial_density</code> and <code>get_density_C</code>), used to approximate the density of a self-adjoint polynomial in free semicircular variables after you have built a self-adjoint linearization.</p>"},{"location":"howto/polynomial_semicircle/#background-what-these-functions-compute","title":"Background (what these functions compute)","text":"<p>Let \\(p\\) be a self-adjoint polynomial in semicircular variables \\(X_1,\\dots,X_s\\). Assume you already have a self-adjoint linearization \\(L_p\\) of \\(p\\) in the form</p> \\[ L_p \\;=\\; a_0 + \\sum_{j=1}^s a_j X_j, \\] <p>where \\(a_0,a_1,\\dots,a_s\\) are fixed complex matrices (of the same size).</p> <p>To recover the scalar Cauchy transform of \\(p\\) from the linearization, one considers the regularized block-diagonal matrix</p> \\[ \\Lambda_\\varepsilon(z) = \\begin{bmatrix} z &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; i\\varepsilon &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; i\\varepsilon \\end{bmatrix}, \\qquad \\varepsilon&gt;0, \\] <p>and the quasi-resolvent</p> \\[ F_\\varepsilon(z) \\;=\\; \\big(\\Lambda_\\varepsilon(z) - L_p\\big)^{-1}. \\] <p>The scalar Cauchy transform of \\(p\\) is then obtained from the \\((1,1)\\) entry/block via the limit \\(\\varepsilon\\downarrow 0\\); numerically, we use a small \\(\\varepsilon\\) and approximate</p> \\[ G_p(z) \\;\\approx\\; \\big[F_\\varepsilon(z)\\big]_{1,1}. \\] <p>Finally, the density at a real point \\(x\\) is obtained by Stieltjes inversion: for \\(z=x+i\\varepsilon\\),</p> \\[ f(x) \\;\\approx\\; -\\frac{1}{\\pi}\\,\\Im\\,G_p(x+i\\varepsilon). \\]"},{"location":"howto/polynomial_semicircle/#what-you-need-to-provide","title":"What you need to provide","text":"<p>These functions assume you already know:</p> <ol> <li>The linearization bias matrix \\(a_0\\) (called <code>a</code> below).</li> <li>The list/stack of Kraus matrices <code>AA = (A_1,\\dots,A_s)</code> that define the covariance map    $$    \\eta(B) = \\sum_{i=1}^s A_i\\,B\\,A_i^\\ast.    $$</li> </ol> <p>In typical linearizations, the \\(A_i\\) are built from the coefficient matrices \\(a_j\\) of the linearization, but this construction is outside the scope of this how-to.</p>"},{"location":"howto/polynomial_semicircle/#api","title":"API","text":""},{"location":"howto/polynomial_semicircle/#_hfsc_mapg-z-a0-a","title":"<code>_hfsc_map(G, z, a0, A)</code>","text":"<p>This is a single fixed-point step used inside an iteration scheme.</p> <p>Conceptually, it implements a half-averaged update</p> \\[ G \\;\\mapsto\\; \\frac12\\Big[G + W(G)\\Big], \\] <p>where \\(W(G)\\) is the \u201craw\u201d update derived from the linearized equation and the map \\(\\eta\\).</p>"},{"location":"howto/polynomial_semicircle/#polynomial_semicircle_densityx-a0-a-eps1e-2","title":"<code>polynomial_semicircle_density(x, a0, A, eps=1e-2, ...)</code>","text":"<p>This computes the density at a real point \\(x\\) by: 1) solving for \\(G(z)\\) at \\(z=x+i\\,\\varepsilon\\) by iteration, and 2) returning \\(-(1/\\pi)\\Im(G(z)_{11})\\).</p>"},{"location":"howto/polynomial_semicircle/#example-anticommutator-x_1x_2-x_2x_1-via-a-3times-3-linearization","title":"Example: anticommutator \\(X_1X_2 + X_2X_1\\) via a \\(3\\times 3\\) linearization","text":"<p>A standard self-adjoint linearization for the anticommutator is the matrix</p> \\[ S = \\begin{bmatrix} 0 &amp; X_1 &amp; X_2 \\\\ X_1 &amp; 0 &amp; -1 \\\\ X_2 &amp; -1 &amp; 0 \\end{bmatrix} = a_0 + a_1 X_1 + a_2 X_2, \\] <p>with</p> \\[ a_0 = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}, \\quad a_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\quad a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. \\] <p>Once you have built the corresponding Kraus family <code>A</code> for \\(\\eta\\), you can evaluate the density numerically:</p> <pre><code>import numpy as np\nfrom free_matrix_laws.transforms import polynomial_semicircle_density\n\n# linearization bias a0:\na0 = np.array([\n    [0, 0, 0],\n    [0, 0,-1],\n    [0,-1, 0],\n], dtype=complex)\n\n# A = (A1,...,As) should be prepared for your problem.\n# Here we assume you already have it:\nA = ...  # list of (n,n) arrays or stacked array (s,n,n)\n\nx = 0.0\nfx = polynomial_semicircle_density(x, a0, A, eps=1e-2, maxiter=10_000)\nprint(\"f(x) \u2248\", fx)\n</code></pre>"},{"location":"howto/polynomial_semicircle/#tips","title":"Tips","text":"<ul> <li>Use \\(z=x+i\\varepsilon\\) with \\(\\varepsilon\\approx 10^{-2}\\)\u2013\\(10^{-3}\\).</li> <li>If the iteration stalls, increase \\(\\varepsilon\\), relax <code>tol</code>, or reduce <code>maxiter</code>.</li> <li>Warm-starting (re-using the solution for a nearby \\(z\\)) can speed up convergence a lot.</li> <li>The returned density is a numerical approximation; decreasing \\(\\varepsilon\\) sharpens it but may require tighter tolerances.</li> </ul>"},{"location":"howto/polynomial_semicircle/#what-is-actually-returned","title":"What is actually returned","text":"<p><code>polynomial_semicircle_density(x, ...)</code> returns $$ f(x)\\;\\approx\\;-\\frac{1}{\\pi}\\,\\operatorname{Im}\\,G(x+i\\varepsilon)_{11}. $$</p> <p>In many linearization setups, the \\((1,1)\\) block corresponds exactly to the scalar resolvent of the polynomial. If your linearization uses a larger \\((1,1)\\) block, replace \\(G_{11}\\) by the normalized trace of that block.</p>"},{"location":"howto/random_semicircle/","title":"Random Wigner Gaussian matrices with semicircle scaling","text":"<p>This shows how to sample GOE/GUE-type matrices whose eigenvalues follow (as \\(n\\to\\infty\\)) a semicircle law with variance \\(c\\) (support \\([-2\\sqrt{c},\\,2\\sqrt{c}]\\)).</p>"},{"location":"howto/random_semicircle/#quick-start","title":"Quick start","text":"<pre><code>import numpy as np\nimport free_matrix_laws as fml\n\nn = 300\n\n# Real symmetric (GOE-type), variance c=1\nH_goe = fml.random_semicircle(n, field=\"real\", variance=1.0, seed=0)\n\n# Complex Hermitian (GUE-type), variance c=0.5\nH_gue = fml.random_semicircle(n, field=\"complex\", variance=0.5, seed=1)\n\n# Hermitian checks\nassert np.allclose(H_goe, H_goe.T)\nassert np.allclose(H_gue, H_gue.conj().T)\n</code></pre>"},{"location":"howto/random_semicircle/#options","title":"Options","text":"<ul> <li>field: <code>\"real\"</code> (GOE) or <code>\"complex\"</code> (GUE).</li> <li>variance: \\(c&gt;0\\); spectrum concentrates on \\([-2\\sqrt{c},,2\\sqrt{c}]\\).</li> <li>seed: integer or <code>numpy.random.Generator</code> for reproducibility.</li> </ul>"},{"location":"howto/random_semicircle/#sanity-checks","title":"Sanity checks","text":"<pre><code>import numpy as np\n\nn, c = 200, 1.7\nH = fml.random_semicircle(n, field=\"real\", variance=c, seed=0)\n\n# Off-diagonal variance \u2248 c/n (finite-n tolerance)\ni, j = np.triu_indices(n, 1)\nv_emp = np.var(H[i, j])\nprint(\"empirical offdiag var ~\", v_emp, \"target ~\", c/n)\n\n# Spectral radius \u2248 2*sqrt(c) (coarse finite-n check)\nsmax = np.linalg.svd(H, compute_uv=False)[0]\nprint(\"||H|| ~\", smax, \"target ~\", 2*np.sqrt(c))\n</code></pre>"},{"location":"howto/random_semicircle/#histogram","title":"Histogram","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nn, c = 600, 1.0\nH = fml.random_semicircle(n, field=\"real\", variance=c, seed=42)\nlam = np.linalg.eigvalsh(H)\n\nplt.hist(lam, bins=60, density=True)\nplt.xlabel(\"eigenvalue\")\nplt.ylabel(\"density\")\nplt.title(\"Empirical spectrum vs. semicircle (c=1)\")\nplt.show()\n</code></pre>"},{"location":"howto/semicircle_density/","title":"How to: <code>semicircle_density</code>","text":"<p>Compute the scalar density of a matrix semicircle distribution at a real point \\(x\\) via Stieltjes inversion.</p> <pre><code>import numpy as np\nfrom free_matrix_laws import semicircle_density\n</code></pre>"},{"location":"howto/semicircle_density/#minimal-example","title":"Minimal example","text":"<pre><code>n, s = 3, 2\nA1 = np.eye(n)\nA2 = 2*np.eye(n)\nA  = [A1, A2]\n\nx  = 0.0\nfx = semicircle_density(x, A, eps=1e-2)   # \u2248 density at x\nfx\n</code></pre>"},{"location":"howto/semicircle_density/#vector-of-points","title":"Vector of points","text":"<pre><code>xs  = np.linspace(-6, 6, 400)\nfxs = np.array([semicircle_density(x, A, eps=5e-3) for x in xs])\n</code></pre>"},{"location":"howto/semicircle_density/#signature","title":"Signature","text":"<pre><code>semicircle_density(\n    x: float,\n    A,                 # list/tuple of (n,n) or stacked (s,n,n)\n    eps: float = 1e-2,\n    G0=None,           # optional warm start for G(z)\n    tol: float = 1e-10,\n    maxiter: int = 10_000,\n) -&gt; float\n</code></pre>"},{"location":"howto/semicircle_density/#what-it-does","title":"What it does","text":"<p>For \\(z=x+i\\varepsilon\\) it solves $$ z\\,G \\;=\\; I \\;+\\; \\eta(G)\\,G, \\qquad \\eta(B)=\\sum_{i=1}^s A_i\\,B\\,A_i^\\ast,\\ \\Im z&gt;0, $$ then returns $$ f(x) \\;=\\; -\\frac{1}{\\pi}\\,\\Im!\\left(\\frac{1}{n}\\,\\mathrm{tr}\\,G(x+i\\varepsilon)\\right). $$</p>"},{"location":"howto/semicircle_density/#tips","title":"Tips","text":"<ul> <li>Imaginary part: for real \\(x\\), use \\(z=x+i\\varepsilon\\) with \\(\\varepsilon\\approx 10^{-2}\\)\u2013\\(10^{-3}\\).</li> <li>Warm starts: pass <code>G0</code> (e.g., the solution at a nearby \\(z\\)) to speed up convergence.</li> <li>Input shapes: <code>A</code> may be a list/tuple of \\((n,n)\\) arrays or a stacked array of shape \\((s,n,n)\\).</li> <li>Numerical stability: if iterations stall, increase \\(\\varepsilon\\), relax <code>tol</code>, or raise <code>maxiter</code>.</li> </ul>"},{"location":"howto/semicircle_density/#scalar-reduction-sanity-check","title":"Scalar-reduction sanity check","text":"<p>If all \\(A_i=\\sigma I\\), then \\(G(z)=g(z)I\\) and the scalar semicircle Stieltjes transform is $$ g(z)=\\frac{z-\\sqrt{z^{2}-4c}}{2c}, \\qquad c=\\sigma^{2}. $$ The density matches the semicircle on \\([-2\\sqrt{c},\\,2\\sqrt{c}]\\).</p> <p>See also: <code>biased_semicircle_density</code>.</p>"},{"location":"howto/solve_cauchy_biased/","title":"Solving \\(G(z)\\) for a biased matrix semicircle","text":"<p>We consider \\(S = a_0 + \\sum_{i=1}^s A_i \\otimes X_i\\) with semicircular \\(X_i\\) and covariance \\(\\eta(B)=\\sum_i A_i B A_i^\\ast\\). The Cauchy transform solves $$ G(z) \\;=\\; (z I - a_0 - \\eta(G(z)))^{-1}, \\qquad \\Im z&gt;0. $$</p> <p>A numerically stable averaged iteration is $$ b = z(z I - a_0)^{-1},\\qquad G \\leftarrow \\tfrac12\\Big[G + \\big(z I - b\\,\\eta(G)\\big)^{-1} b\\Big]. $$</p>"},{"location":"howto/solve_cauchy_biased/#minimal-example","title":"Minimal example","text":"<pre><code>import numpy as np\nimport free_matrix_laws as fml\n\nn = 3\na0 = np.array([[0,0,0],[0,0,-1],[0,-1,0]], dtype=float)\na1 = np.array([[0,1,0],[1,0,0],[0,0,0]], dtype=float)\na2 = np.array([[0,0,1],[0,0,0],[1,0,0]], dtype=float)\nA = [a1, a2]\n\nz = 0.1 + 1j*0.02\nG, info = fml.solve_cauchy_biased(z, a0, A, tol=1e-10, return_info=True)\nprint(\"residual:\", info[\"residual\"], \"iters:\", info[\"iters\"])\n</code></pre>"},{"location":"howto/solve_cauchy_semicircle/","title":"Solving the operator-valued Cauchy transform \\(G(z)\\)","text":"<p>This shows how to compute the matrix Cauchy transform \\(G(z)\\) for the matrix semicircle via the fixed-point solver $$ z\\,G \\;=\\; I \\;+\\; \\eta(G)\\,G,\\qquad \\Im z&gt;0,\\quad \\eta(B)=\\sum_{i=1}^s A_i\\,B\\,A_i^\\ast. $$</p>"},{"location":"howto/solve_cauchy_semicircle/#minimal-example","title":"Minimal example","text":"<pre><code>import numpy as np\nimport free_matrix_laws as fml\n\nn, s = 5, 3\nrng = np.random.default_rng(0)\n\n# Kraus operators A_i (general complex case)\nA = [rng.standard_normal((n, n)) + 1j*rng.standard_normal((n, n)) for _ in range(s)]\n\nz = 0.3 + 1j*0.02  # Im z &gt; 0\nG = fml.solve_cauchy_semicircle(z, A, tol=1e-10, maxiter=2000)\n\n# Residual check: || zG - I - eta(G)G || should be small\netaG = fml.covariance_map(G, A)\nres = np.linalg.norm(z*G - np.eye(n) - etaG @ G)\nprint(\"residual:\", res)\n</code></pre>"},{"location":"howto/solve_cauchy_semicircle/#scalar-reduction-sanity-check","title":"Scalar reduction sanity check","text":"<p>When all \\(A_i=\\sigma I\\), the solution is \\(G(z)=g(z),I\\) with the scalar semicircle Stieltjes transform $$ g(z)=\\frac{z-\\sqrt{z^2-4c}}{2c},\\qquad c=\\sigma^2. $$</p> <pre><code>import numpy as np\nimport free_matrix_laws as fml\n\nn, sigma = 6, 1.3\nA = [sigma*np.eye(n)]\nc = sigma**2\n\nz = -0.5 + 1j*0.05\nG = fml.solve_cauchy_semicircle(z, A, tol=1e-12)\n\ng = (z - np.sqrt(z*z - 4*c)) / (2*c)\nerr = np.linalg.norm(G - g*np.eye(n))\nprint(\"||G - gI||:\", err)\n</code></pre>"},{"location":"howto/solve_cauchy_semicircle/#tips","title":"Tips","text":"<ul> <li>Imaginary part of \\(z\\): For densities at real \\(x\\), use \\(z = x + i,\\varepsilon\\) with \\(\\varepsilon \\approx 10^{-2}\\text{\u2013}10^{-3}\\).</li> <li>Warm starts: Pass <code>G0</code> (e.g., the solution at a nearby \\(z\\)) to accelerate convergence.</li> <li>Input shapes: \\(A\\) can be a list/tuple of \\((n,n)\\) arrays or a stacked array of shape \\((s,n,n)\\).</li> <li>Numerical stability: If the iteration stalls, increase \\(\\varepsilon\\), relax <code>tol</code>, or cap <code>maxiter</code>.</li> </ul>"},{"location":"howto/solve_cauchy_semicircle/#from-gz-to-a-scalar-stieltjes-transform-and-density","title":"From \\(G(z)\\) to a scalar Stieltjes transform and density","text":"<p>The scalar transform is $$ m(z) = \\frac{1}{n} \\mathrm{tr} \\, G(z). $$ For \\(x\\in\\mathbb{R}\\) with \\(z=x+i\\varepsilon\\), $$ f(x) = -\\frac{1}{\\pi}\\Im m(z). $$</p> <pre><code>import numpy as np\nimport free_matrix_laws as fml\n\nn = 5\nA = [np.eye(n)]           # simple case\nx, eps = 0.0, 1e-2\nz = x + 1j*eps\n\nG = fml.solve_cauchy_semicircle(z, A, tol=1e-12)\nm = np.trace(G)/n\nf = (-1/np.pi) * np.imag(m)\nprint(\"density ~\", f)\n</code></pre>"},{"location":"howto/solve_cauchy_semicircle/#or-use-the-convenience-helper","title":"Or use the convenience helper:","text":"<pre><code>f2 = fml.semicircle_density(x, A, eps=eps, tol=1e-12)\nprint(\"density (helper) ~\", f2)\n</code></pre>"},{"location":"notebooks/10_semicircle_methods/","title":"Methods for calculating the eigevalue density of matrix semicircle and of polynomials in free random variables","text":"<p>This is a collection of methods to calculate the eigenvalue distribution of matrix-valued semicircular random variable and of non-commutative self-adjoint polynomials in free random varriables, in particular in semicircle r.v.s. The methods are based on work of Belinschi, Rashidi Far, Helton, Mai, Speicher.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport numpy.linalg as la\nfrom scipy.integrate import quad\nfrom functools import partial\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt, patches\nfrom tqdm.notebook import tqdm, trange\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = [7.00, 7.00]\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n</pre> import numpy as np import numpy.linalg as la from scipy.integrate import quad from functools import partial import seaborn as sns  from matplotlib import pyplot as plt, patches from tqdm.notebook import tqdm, trange  %matplotlib inline plt.rcParams[\"figure.figsize\"] = [7.00, 7.00] plt.rcParams['image.interpolation'] = 'nearest' plt.rcParams['image.cmap'] = 'gray' <p>(1) eta: First we realize the map $\\eta: b \\to \\eta(b)$. This is the covariance function for the matrix semicircle $$S = a_1 s_1 + \\ldots + a_k s_k,$$ where $a_1, \\ldots, a_k$ are Hermitian matrices and $s_1, \\ldots, s_k$ are free semicircle variables.</p> <p>(2a) hfs_map: Next step is to write the function that will be used to solve the equation for the matrix Cauchy transform $G(z)$, $$z G = 1 + \\eta(G) G$$ by the method of iterations. The equation was derived by Speicher in R. Speicher, \u201cCombinatorial theory of the free product with amalgamation and operator-valued free probability theory,\u201d Mem. Amer. Math. Soc., vol. 132, no. 627, pp. x+88, 1998 . Another proof can be found in R. Rashidi Far, T. Oraby, W.Bryc, R. Speicher: \u201cSpectra of large block matrices,\u201d preprint 2006, cs.IT/0610045. Here the realization is for scalar $z \\in \\mathbb C$.</p> <p>The simplest approach to equation on the Cauchy transform G is to use $$ G \\to (z - \\eta(G))^{-1}.$$ The arguments in Helton, Rashidi Far, Speicher: \"Operator-valued Semicircular Elements: Solving A Quadratic Matrix Equation with Positivity Constraints.\" IMRN 2007 suggest that from the numerical viewpoint it is beneficial  to use a slightly different scheme: $$ G \\to \\frac{1}{2}\\Big[G + (z - \\eta(G))^{-1}\\Big].$$</p> <p>(3a) get_density: Then we define a function that calculate the density of a matrix semicircle at a point $x$. This program calculate the matrix Cauchy transform by iterations, then reduces it to the scalar-valued Cauchy transform by taking normalized trace, and finally calculate the density by using the Stieltjes formula.</p> <p>(2b) hfsb_map: This is a functions needed to solve a different version of the equation $$z G = 1 + \\eta(G) G,$$ namely, suppose we are interested in the matrix $S = a_0 + a_1 X_1 + \\ldots + a_n X_n$, where $X_i$ are semicircle r.v.s. For example, $$ S =  \\begin{bmatrix} 0 &amp; X_1 &amp; X_2 \\\\ X_1 &amp; 0 &amp; -1 \\\\ X_2 &amp; -1 &amp; 0 \\end{bmatrix}, $$ where $X_1$ and $X_2$ are two standard semicircular variables. In this example, we have $$ a_0 = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}, \\, a_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$</p> <p>This example is significantly different from the matrix semicircle because the matrix semicirle variable has non-trivial expectation.</p> <p>First, we need to define an analogue of the hfs map, in order to be able to calculate the Cauchy transform $G_S(z) = G_{a_0 + X}$, where $X = a_1 \\otimes X_1 + a_2 \\otimes X_2$. For this we define $b = z(z I - a_0)^{-1}$ and define the map (hfsb_map): $$ G \\mapsto \\frac{1}{2}\\Big[G + [z I - b \\eta(G)]^{-1} b\\Big]. $$ This should be iterated to convergence.</p> <p>(3b) get_density_B: uses the hsfb_map to calculate the G-transform and density of a biased matrix semicircle.</p> <p>(2c) hfsc_map: Often we use the biased matirx semicircle variable to compute the distribution of an hermitian non-commutative polynomial in semicircular variables. In order to find this distribution, note that we need to calculate $G(z, b(z))$, where $$ b_\\epsilon(z) = z(\\Lambda_\\epsilon(z) - a_0)^{-1}, $$ $\\epsilon &gt; 0$ is a small regularization paremater, and $$ \\Lambda_\\epsilon(z) := \\begin{bmatrix}  z &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; i\\epsilon &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\, &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; i\\epsilon \\end{bmatrix}. $$ By definition, $$ G(z, b) = m_0(b)z^{-1} + m_1(b) z^{-2} + m_2(b) z^{-3} + \\ldots, $$ and $m_k(b)$ are moments: $$ m_k(b) := E[b(Xb)^k] = E[bXb \\ldots Xb]. $$</p> <p>Once $G(z, b(z))$ is calculated, we can calculate the Cauchy transform of the polynomial $p$ by using formula $$ \\phi[(z - p)^{-1}] = \\lim_{\\epsilon \\to 0} \\Big[ G(z, b_\\epsilon(z)) \\Big]_{1,1}. $$</p> <p>Finally, we can extract the density by the Stieljes inversion formula.</p> <p>For the first step we will modify a little bit the hsfb_map, and the modified version is hsfc_map.</p> <p>(3c) get_density_C: This is the function that uses iterations of hsfc_map to calculate the Cauchy transform and the density of the polynomial in semicircular variables. It requires coefficients in the linearization of the polynomial, it will not construct the linearization for you.</p> <p>(4) random_semicircle: We will check that the free probability methods give a good approximation to Gaussian block matrices. So we also define a basic block for these matrices.</p> <p>(5) Lambda: In calculation of the distribution of polynomials we will also need function $$ \\Lambda_\\epsilon(z) := \\begin{bmatrix}  z &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; i\\epsilon &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\, &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; i\\epsilon \\end{bmatrix} $$</p> <p>(6) G_semicircle: In some calculations, we need the Cauchy transform of the standard semicircle variable.</p> <p>(7) G-matrix semicircle: This is the function that computes the matrix Cauchy transform of $b \\otimes S$ where $S$ is a semicircle random variable. This is a relatively sophisticated method. For more simple methods based on integration and regularization approaches, see the example about anticommutator via subordination below.</p> <p>Take unitary matrices $U_1$ and $U_2$ so that $$  U_1^\\ast B U_2 = \\begin{bmatrix}  D &amp; 0 \\\\ 0 &amp; 0,  \\end{bmatrix}  $$ where $D = diag \\{\\lambda_1, \\ldots, \\lambda_r\\}$ is an invertible diagonal $r\\times r$ matrix, and $0$'s  represent matrices of appropriate dimensions. For self-adjoint $B$ we can take $U_1 = U_2$.</p> <p>Let $$    U_1^\\ast A U_2 = \\begin{bmatrix}a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22}    \\end{bmatrix}, $$ where $a_{11}$ is an $r\\times r$ matrix and let $$ S := a_{11} - a_{12} a_{22}^{-1} a_{21}. $$ Then \\begin{equation} %\\label{equ_matrix_G1} G_{B\\otimes x}(A) = E (A - B\\otimes x)^{-1}  = U_2 \\begin{bmatrix} I &amp; 0 \\\\ -  a_{22}^{-1} a_{21} &amp; I \\end{bmatrix}  M \\begin{bmatrix} I &amp;  -  a_{12} a_{22}^{-1} \\\\ 0 &amp; I \\end{bmatrix} U_1^\\ast, \\end{equation} where $$  M = \\begin{bmatrix}E (S - D \\otimes x)^{-1} &amp; 0 \\\\ 0 &amp; a_{22}^{-1}\\end{bmatrix}.    $$</p> <p>It remains to calculate $E (S - D \\otimes x)^{-1}$, and since $D$ is an invertible diagonal matrix, we can apply our original approach. Let us assume that $D^{-1} S$ is diagonalizable (this will hold for a generic $A$), then we can write $$  D^{-1} S = V \\begin{bmatrix}\\mu_1 &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; \\mu_r  \\end{bmatrix} V^{-1},  $$ and \\begin{equation} %\\label{equ_matrix_G2}  E \\big[(S - D\\otimes X)^{-1}\\big] =  E \\big[(D^{-1} S - I \\otimes X)^{-1}\\big] D^{-1} =  V \\begin{bmatrix}G_x(\\mu_1) &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; G_x(\\mu_r)  \\end{bmatrix} V^{-1} D^{-1}.  \\end{equation}</p> <p>(8) H_matrix_Semicircle: $H_{B \\otimes S}(w) = G_{B \\otimes S}^{-1}(w) - w$.</p> <p>(9) omega: Matrix subordination function. Recall that $\\omega_1(b)$ is the fixed point of the map $$ w \\to h_y(h_x(w) + b) + b. $$ where $h_x(w) = F_x(w) - w$ and $h_y(w) = F_y(w) - w$, and $F_x, F_y$ are inverses of the corresponding Cauchy transforms.</p> <p>Given that $\\omega_1(b)$ is calculated, we can find the Cauchy transform for the sum $x + y$ as $$ G_{x + y}(b) = G_x(\\omega_1(b)). $$</p> <p>We realize $\\omega_1(b)$ as a function $\\omega\\big(b, (a_1, a_2)\\big)$. This version of omega is only for semicircle variables $x = a_1 \\otimes S_1$ and $y = a_2 \\otimes S_2$.</p> <p>(10) G_free_Poisson: Cauchy transform of the free Poisson distribution with parameter $\\lambda$.</p> <p>(11) G_matrix_fpoisson: Matrix version of the Cauchy transform for the free Poisson r.v. with parameter $\\lambda$.</p> <p>(12) H_matrix_fpoisson: $G^{-1}(w) - w$.</p> <p>(13) omega_sum: computes the subordination function $\\omega_1$ for the sum of two matrix random variables, not necessarily semicircle. Requires H-functions for these random variables.</p> <p>(14) random_fpoisson: generates a random matrix with free Poisson distribution.</p> <p>(15) random_orthogonal: generates a random orthogonal matrix</p> <p>(16) G_matrix_custom: This is a function that calculates the matrix Cauchy transform, provided that the scalar Cauchy transform is known.</p> <p>(17) H_matrix_custom: The H-function that corresponds to G_matrix_custom</p> <p>(18) cauchy_transform_discrete: The scalar Cauchy transform of an arbitrary discrete distribution.</p> In\u00a0[\u00a0]: Copied! <pre>#(1) covariance map\ndef eta(B, AA):\n  '''\n  AA is a list or a tuple of Hermitian matrices A_1, \\ldots, A_k\n  B is sent to A_1 B A_1 + \\ldots + A_k B A_k\n  '''\n  n = B.shape[0]\n  s = len(AA)\n  result = np.zeros((n, n), dtype = np.float32)\n  for i in range(s):\n    result = result + AA[i] @ B @ AA[i]\n  return result\n\n#(2a) Iteration step needed to calculate the cauchy transform for\n# a matrix semicircle r.v.\ndef hfs_map(G, z, AA):\n  ''' G is a matrix, z is a complex number with positive imaginary part,\n  G is mapped to a smoothed version of $(z - \\eta(G))^{-1}$. AA is a list of matrices\n  needed to define the function $\\eta$.\n  '''\n  n = G.shape[0]\n  #return la.inv(z * np.eye(n) - eta(G, AA))\n  return (G + la.inv(z * np.eye(n) - eta(G, AA)))/2\n\n#(3a) A function that calculates the Cauchy transform and the distribution\n#density for a matrix semicircle r.v.\ndef get_density(x, AA, eps=0.01, max_iter=10000):\n  ''' Calculate the density at the real point x, given the data\n  in the tuple of matrices $AA = (A1, \\ldots, As)$\n  Uses eps as the distance of the point $x + i eps$ from the\n  real axis.\n  '''\n  z = x + 1j * eps\n  n = AA[0].shape[0]\n  G = 1/z * np.eye(n) #initialization\n  diffs = np.zeros((max_iter, 1))\n  for i in range(max_iter):\n    G1 = hfs_map(G, z, AA)\n    diffs[i] = la.norm(G1 - G)\n    if la.norm(G1 - G) &lt; 1e-10:\n      break\n    G = G1\n    if i == max_iter - 1:\n      print(\"Warning: no convegence after \", max_iter, \"iterations\")\n  f = (-1/np.pi) * np.imag(np.trace(G)/n)\n  #plt.plot(diffs) #this is for diagnostic purposes\n  #plt.yscale(\"log\")\n  return f\n\n\n#(2b) the main iteration steps in the calculation of the density for a\n#biased matrix semicircle\ndef hfsb_map(G, z, a, AA):\n  ''' G is a matrix, z is a complex number with positive imaginary part,\n  a is a bias matrix, AA is a list of matrices\n  needed to define the function $\\eta$.\n  '''\n  n = G.shape[0]\n  b = z * la.inv(z * np.eye(n) - a)\n  W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b\n  return (G + W)/2\n  #return W\n\n#(3b) calculates the Cauchy transform and the density of a biased matrix semicircle\ndef get_density_B(x, a, AA, eps=0.01, max_iter=10000):\n  ''' Calculate the density at the real point x, given the data\n  in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.\n  Uses eps as the distance of the point $x + i eps$ from the\n  real axis.\n  '''\n  z = x + 1j * eps\n  n = AA[0].shape[0]\n  G = 1/z * np.eye(n) #initialization\n  diffs = np.zeros((max_iter, 1))\n  for i in range(max_iter):\n    G1 = hfsb_map(G, z, a, AA)\n    diffs[i] = la.norm(G1 - G)\n    if la.norm(G1 - G) &lt; 1e-14:\n      break\n    G = G1\n    if i == max_iter - 1:\n      print(\"Warning: no convegence after \", max_iter, \"iterations\")\n  f = (-1/np.pi) * np.imag(np.trace(G)/n)\n  #plt.plot(diffs) #this is for diagnostic purposes\n  #plt.yscale(\"log\")\n  return f\n\n\n#(2c) An iteration step in the calculation of the Cauchy transform\n# and the density for a polynomial in semicircle r.v.s.\ndef hfsc_map(G, z, a, AA):\n  ''' G is a matrix, z is a complex number with positive imaginary part,\n  a is a bias matrix, AA is a list of matrices\n  needed to define the function $\\eta$.\n  '''\n  n = G.shape[0]\n  b = z * la.inv(Lambda(z, n) - a)\n  W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b\n  return (G + W)/2\n  #return W\n\n#(3c) A function that computes the Cauchy transform and the density for\n#a polynomial in semicircle r.v.s.\ndef get_density_C(x, a, AA, eps=0.01, max_iter=10000):\n  ''' Calculate the density at the real point x, given the data\n  in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.\n  Uses eps as the distance of the point $x + i eps$ from the\n  real axis.\n  '''\n  z = x + 1j * eps\n  n = AA[0].shape[0]\n  G = 1/z * np.eye(n) #initialization\n  diffs = np.zeros((max_iter, 1))\n  for i in range(max_iter):\n    G1 = hfsc_map(G, z, a, AA)\n    diffs[i] = la.norm(G1 - G)\n    if la.norm(G1 - G) &lt; 1e-12:\n      break\n    G = G1\n    if i == max_iter - 1:\n      print(\"Warning: no convegence after \", max_iter, \"iterations\")\n  f = (-1/np.pi) * np.imag(G[0, 0])\n  #plt.plot(diffs) #this is for diagnostic purposes\n  #plt.yscale(\"log\")\n  return f\n\n#(4) creates a random Hermitian Gaussian matrix with approximately semicircle\n# distribution.\ndef random_semicircle(size):\n  '''generate a random Hermitian Gaussian matrix of size n-by-n normalized by 1/sqrt(n),\n  where n = size'''\n  random_matrix = np.random.randn(size, size)\n  return (random_matrix + random_matrix.T)/( np.sqrt(2 * size))\n\n#Example of usage:\nsize = 200\nA = random_semicircle(size)\nprint(la.norm(A - A.T))\ne = la.eigvalsh(A)\nplt.plot(e)\nnp.trace(A * A)\n\ndef Lambda(z, size, eps = 1E-6):\n  ''' Lambda_eps(z) needed to calculate the distribution of a polynomial\n  of free random variables.'''\n  A = eps * 1.j * np.eye(size)\n  A[0, 0] = z\n  return A\n\nprint(Lambda(1.j, 3))\n\ndef G_semicircle(z):\n    \"\"\"\n    Computes the Cauchy transform of the semicircle distribution for a given complex number z,\n    ensuring that if z has a positive imaginary part, the output has a negative imaginary part,\n    and vice versa.\n\n    Parameters:\n        z (complex or array-like): The point(s) at which to evaluate the Cauchy transform.\n\n    Returns:\n        complex or ndarray: The value(s) of the Cauchy transform at z.\n    \"\"\"\n    z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex\n\n    # Compute the discriminant\n    discriminant = np.sqrt(z**2 - 4)\n\n    # Ensure the output's imaginary part has the desired symmetry\n    discriminant = np.where(discriminant.imag * z.imag &lt; 0, -discriminant, discriminant)\n\n    # Compute the Cauchy transform\n    G = (z - discriminant) / 2\n\n    return G\n\n# Example usage\nz = 3 + 4j  # Example input\nresult = G_semicircle(z)\nprint(f\"Cauchy transform at {z} is {result}\")\n\n# Test with an array of values\nz_array = [3 + 4j, 1 + 1j, 3 - 4j, 1 - 1j]\nresult_array = G_semicircle(z_array)\nprint(f\"Cauchy transform for array {z_array} is {result_array}\")\n\nplt.figure()\nz = np.linspace(-4,4) - 0.01j\nplt.plot(np.imag(G_semicircle(z)))\nz = np.linspace(-4,4) + 0.01j\nplt.plot(np.imag(G_semicircle(z)))\n\n\ndef G_matrix_semicircle(w, B, rank):\n  ''' computes G(w) for the semicirle B \\otimes x,\n  rank is the rank of matrix B '''\n  w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n  n = B.shape[0]\n  U1, d, U2t = la.svd(B)\n  U2 = np.conj(U2t.T)\n  #print(\"U1 =\", U1)\n  #print(d)\n  #print(\"U2 =\", U2)\n  #print(\"should be D: \", np.conj(U1.T) @ B @ U2) #\n  A_transf = np.conj(U1.T) @ w @ U2\n  #print(\"A_transf = \", A_transf)\n\n  A11 = A_transf[0:rank, 0:rank]\n  A12 = A_transf[0: rank, rank:n]\n  A21 = A_transf[rank:n, 0: rank]\n  A22 = A_transf[rank:n, rank:n]\n  D = np.diag(d[0:rank])\n  #print(\"D = \", D)\n  S = A11 - A12 @ la.inv(A22) @ A21\n  #print('S = ', S)\n  mu, V = la.eig(la.inv(D) @ S)\n  #print('mu =', mu)\n  #print(V)\n  #print('S = ', V @ np.diag(mu) @ la.inv(V))\n  #print(\"G(mu) = \", G_semicircle(mu))\n  M11 = V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(D)\n  #print('M11 = ', M11)\n  M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])\n  #print(\"M = \", M)\n  G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])\n       @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)\n  return(G)\n\ndef H_matrix_semicircle(w, B, rank):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(G_matrix_semicircle(w, B, rank)) - w)\n\n\nn = 3\nrank = 2\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = G_matrix_semicircle(w, A1, rank)\nprint(\"G_matrix_semicircle = \", G)\n\nH = H_matrix_semicircle(w, A1, rank)\nprint(\"H_matrix_semicircle = \", H)\n\ndef omega(b, AA, rank, max_iter = 10000):\n  ''' This computes subordination function for the sum of two semicircle variables.\n  AA = (A1, A2), rank is a (rank1, rank2), where rank1 is the rank of matrix A1,\n  and rank2 is the rank of matrix A2.\n  '''\n  W0 = 1.j * np.eye(n) #(initialization)\n  A1 = AA[0]\n  A2 = AA[1]\n  for i in range(max_iter):\n    W1 = H_matrix_semicircle(W0, A1, rank = rank[0]) + b\n    W2 = H_matrix_semicircle(W1, A2, rank = rank[1]) + b\n    if la.norm(W2 - W0) &lt; 1e-12:\n      break\n    W0 = W2\n    if i == max_iter - 1:\n      print(\"Warning: no convergence after \", max_iter, \"iterations\")\n  return W0\n\n#an example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\nprint('result = ', omega(B, AA, rank = (2, 2)))\n\n#(10) Cauchy transform of free Poisson\ndef G_free_poisson(z, lambda_param):\n    \"\"\"\n    Explicit formula for the Cauchy transform of the free Poisson distribution\n    with parameter \u03bb.\n\n    Args:\n        z (complex): The point at which to evaluate the Cauchy transform.\n        lambda_param (float): The parameter \u03bb of the free Poisson law.\n\n    Returns:\n        G (complex): The value of the Cauchy transform G(z).\n    \"\"\"\n\n    z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex\n    # Compute the interval [a, b] of the support\n    a = (1 - np.sqrt(lambda_param))**2\n    #print(a)\n    b = (1 + np.sqrt(lambda_param))**2\n    #print(b)\n\n    # Compute the square root term with correct branch\n    sqrt_term = np.sqrt((z - a) * (z - b))\n    #sqrt_term = np.sqrt((1 + z - lambda_param)**2 - 4 * z) #alternative expression\n\n    sqrt_term = np.where(sqrt_term.imag * z.imag &lt; 0, -sqrt_term, sqrt_term)\n\n    # Explicit formula for the Cauchy transform\n    G = (1 + z - lambda_param - sqrt_term) / (2 * z)\n    if lambda_param &lt; 1: #in this case G also has an atom at 0 with weight (1 - lambda)\n      G = G + (1 - lambda_param)/z\n\n    return G\n\n\n\nlambda_param = 4  # Set \u03bb (parameter of the free Poisson law)\nz = 3 + 1j          # Complex number at which to evaluate G(z)\n\n# Compute the Cauchy transform\nG_z = G_free_poisson(z, lambda_param)\nprint(f\"Cauchy transform G({z}) for \u03bb={lambda_param}: {G_z}\")\n\n#visualization\n\nplt.figure()\n#z = np.linspace(-4,4) - 0.01j\n#plt.plot(np.imag(G_free_poisson(z, 2)))\nm = 100\nal = (1 - np.sqrt(lambda_param))**2\nau = (1 + np.sqrt(lambda_param))**2\nx =  np.linspace(al,au, m)\nz = x - 0.01j\nplt.plot(x, np.imag(G_free_poisson(z, lambda_param)))\nz = x + 0.01j\nplt.plot(x, np.imag(G_free_poisson(z, lambda_param)))\nplt.grid()\n\n#let's check that this corresponds to a valid density function.\nf = - np.imag(G_free_poisson(z, lambda_param))/np.pi\nprint(sum(f)* (au - al)/m)\n\n\n# (11) Matrix version of the Cauchy transform for the free Poisson random variable.\ndef G_matrix_fpoisson(w, B, rank, lambda_param):\n  ''' computes G(w) for the free Poisson r.v. B \\otimes x,\n  rank is the rank of matrix B '''\n  w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n  n = B.shape[0]\n  U1, d, U2t = la.svd(B)\n  U2 = np.conj(U2t.T)\n  A_transf = np.conj(U1.T) @ w @ U2\n\n  A11 = A_transf[0:rank, 0:rank]\n  A12 = A_transf[0: rank, rank:n]\n  A21 = A_transf[rank:n, 0: rank]\n  A22 = A_transf[rank:n, rank:n]\n  D = np.diag(d[0:rank])\n  S = A11 - A12 @ la.inv(A22) @ A21\n  mu, V = la.eig(la.inv(D) @ S)\n  M11 = V @ np.diag(G_free_poisson(mu, lambda_param)) @ la.inv(V) @ la.inv(D)\n  M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])\n  G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])\n       @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)\n  return(G)\n\ndef H_matrix_fpoisson(w, B, rank, lambda_param):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(G_matrix_fpoisson(w, B, rank, lambda_param)) - w)\n\n\n#(13) subordination function for the sum of two matrix random variables.\ndef omega_sub(b, AA, rank, H1_name=\"H_matrix_semicircle\", H2_name=\"H_matrix_semicircle\",\n              H1_kwargs=None, H2_kwargs=None, max_iter=10000):\n    '''\n    Computes subordination function omega_1(b) for the sum of two free random variables variables.\n\n    AA = (A1, A2), where A1 and A2 are matrices.\n    rank = (rank1, rank2), where rank1 is the rank of matrix A1, and rank2 is the rank of matrix A2.\n    H1_name, H2_name are string names of the functions to be applied.\n    H1_kwargs, H2_kwargs are dictionaries containing additional arguments for H1 and H2.\n    '''\n    n = AA[0].shape[0]  # Assuming A1 and A2 are square matrices of the same size\n    W0 = 1.j * np.eye(n)  # Initialization\n    A1, A2 = AA\n\n    # Get function references from globals()\n    H1 = globals()[H1_name]\n    H2 = globals()[H2_name]\n\n    # Initialize kwargs dictionaries if None\n    if H1_kwargs is None:\n        H1_kwargs = {}\n    if H2_kwargs is None:\n        H2_kwargs = {}\n\n    for i in range(max_iter):\n        W1 = H1(W0, A1, rank=rank[0], **H1_kwargs) + b\n        W2 = H2(W1, A2, rank=rank[1], **H2_kwargs) + b\n\n        if la.norm(W2 - W0) &lt; 1e-12:\n            break\n        W0 = W2\n\n        if i == max_iter - 1:\n            print(\"Warning: no convergence after\", max_iter, \"iterations\")\n\n    return W0\n\n#(14) generator of a free Poisson matrix\ndef random_fpoisson(size, lam):\n  '''generate a random Hermitian matrix of size n-by-n, where n = size, that have the free Poisson\n  distribution with parameter lambda.\n  '''\n  random_matrix = np.random.randn(size, int(np.floor(size * lam)))\n  return (random_matrix @ random_matrix.T) /size\n\n#Example of usage:\nsize = 200\nlam = 4\nA = random_fpoisson(size, lam)\nprint(la.norm(A - A.T))\ne = la.eigvalsh(A)\nplt.figure()\nplt.plot(e)\nnp.trace(A)/size\n\n#(15) random orthogonal matrix\ndef random_orthogonal(n):\n    # Step 1: Generate a random n x n matrix A\n    A = np.random.randn(n, n)\n\n    # Step 2: Perform QR decomposition on A\n    Q, R = np.linalg.qr(A)\n\n    # Q is the orthogonal matrix we want\n    return Q\n\n# Example usage\nn = 3  # Dimension of the matrix\nQ = random_orthogonal(n)\nprint(\"Random Orthogonal Matrix Q:\\n\", Q)\n\n# (16) This is a function that calculates the matrix Cauchy transform, provided that\n#the scalar Cauchy transform is known.\ndef G_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):\n  ''' computes G(w) for the r.v. B \\otimes x, where x has a custom measure mu_x above,\n  with the scalar Cauchy transform function $G_name$, and\n  rank is the rank of matrix B '''\n\n  # Get function references from globals()\n  G = globals()[G_name]\n\n  # Initialize kwargs dictionaries if None\n  if G_kwargs is None:\n    G_kwargs = {}\n\n\n  w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n  n = B.shape[0]\n  U1, d, U2t = la.svd(B)\n  U2 = np.conj(U2t.T)\n  A_transf = np.conj(U1.T) @ w @ U2\n\n  A11 = A_transf[0:rank, 0:rank]\n  A12 = A_transf[0: rank, rank:n]\n  A21 = A_transf[rank:n, 0: rank]\n  A22 = A_transf[rank:n, rank:n]\n  D = np.diag(d[0:rank])\n  S = A11 - A12 @ la.inv(A22) @ A21\n  mu, V = la.eig(la.inv(D) @ S)\n  M11 = V @ np.diag(G(mu, **G_kwargs)) @ la.inv(V) @ la.inv(D)\n  M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])\n  G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])\n       @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)\n  return(G)\n\n# (17) The H-function that corresponds to G_matrix_custom\ndef H_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(G_matrix_custom(w, B, rank, G_name, G_kwargs)) - w)\n\nn = 3\nrank = 2\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = G_matrix_custom(w, A1, rank, G_name=\"G_semicircle\")\nprint(\"G_matrix_custom = \", G)\n\nH = H_matrix_custom(w, A1, rank,  G_name=\"G_semicircle\")\nprint(\"H_matrix_custom = \", H)\n\n\n#(18) The scalar Cauchy transform of an arbitrary discrete distribution\ndef cauchy_transform_discrete(z, points, weights):\n    \"\"\"\n    Computes the Cauchy transform G_mu(z) for a measure defined by\n    discrete points and their corresponding weights.\n\n    Parameters:\n    z : complex or array-like\n        Evaluation point(s) in the complex plane.\n    points : list or array-like\n        Locations of the discrete measure.\n    weights : list or array-like\n        Corresponding weights of the measure.\n    \"\"\"\n    z = np.asarray(z)[:, np.newaxis]  # Ensure z is a column vector\n    points = np.asarray(points)\n    weights = np.asarray(weights)\n    return np.sum(weights / (z - points), axis=1)\n\n# Example usage\npoints = np.array([-2, -1, 1])  # Support points\nweights = np.array([2/4, 1/4, 1/4])  # Corresponding weights\nz_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane\nG_values = cauchy_transform_discrete(z_values, points, weights)\n\n# Plot real and imaginary parts\nplt.figure(figsize=(8, 5))\nplt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed')\nplt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\")\nplt.xlabel(\"Re(z)\")\nplt.ylabel(\"G_mu(z)\")\nplt.legend()\nplt.title(\"Cauchy Transform of Given Distribution\")\nplt.grid()\nplt.show()\n</pre> #(1) covariance map def eta(B, AA):   '''   AA is a list or a tuple of Hermitian matrices A_1, \\ldots, A_k   B is sent to A_1 B A_1 + \\ldots + A_k B A_k   '''   n = B.shape[0]   s = len(AA)   result = np.zeros((n, n), dtype = np.float32)   for i in range(s):     result = result + AA[i] @ B @ AA[i]   return result  #(2a) Iteration step needed to calculate the cauchy transform for # a matrix semicircle r.v. def hfs_map(G, z, AA):   ''' G is a matrix, z is a complex number with positive imaginary part,   G is mapped to a smoothed version of $(z - \\eta(G))^{-1}$. AA is a list of matrices   needed to define the function $\\eta$.   '''   n = G.shape[0]   #return la.inv(z * np.eye(n) - eta(G, AA))   return (G + la.inv(z * np.eye(n) - eta(G, AA)))/2  #(3a) A function that calculates the Cauchy transform and the distribution #density for a matrix semicircle r.v. def get_density(x, AA, eps=0.01, max_iter=10000):   ''' Calculate the density at the real point x, given the data   in the tuple of matrices $AA = (A1, \\ldots, As)$   Uses eps as the distance of the point $x + i eps$ from the   real axis.   '''   z = x + 1j * eps   n = AA[0].shape[0]   G = 1/z * np.eye(n) #initialization   diffs = np.zeros((max_iter, 1))   for i in range(max_iter):     G1 = hfs_map(G, z, AA)     diffs[i] = la.norm(G1 - G)     if la.norm(G1 - G) &lt; 1e-10:       break     G = G1     if i == max_iter - 1:       print(\"Warning: no convegence after \", max_iter, \"iterations\")   f = (-1/np.pi) * np.imag(np.trace(G)/n)   #plt.plot(diffs) #this is for diagnostic purposes   #plt.yscale(\"log\")   return f   #(2b) the main iteration steps in the calculation of the density for a #biased matrix semicircle def hfsb_map(G, z, a, AA):   ''' G is a matrix, z is a complex number with positive imaginary part,   a is a bias matrix, AA is a list of matrices   needed to define the function $\\eta$.   '''   n = G.shape[0]   b = z * la.inv(z * np.eye(n) - a)   W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b   return (G + W)/2   #return W  #(3b) calculates the Cauchy transform and the density of a biased matrix semicircle def get_density_B(x, a, AA, eps=0.01, max_iter=10000):   ''' Calculate the density at the real point x, given the data   in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.   Uses eps as the distance of the point $x + i eps$ from the   real axis.   '''   z = x + 1j * eps   n = AA[0].shape[0]   G = 1/z * np.eye(n) #initialization   diffs = np.zeros((max_iter, 1))   for i in range(max_iter):     G1 = hfsb_map(G, z, a, AA)     diffs[i] = la.norm(G1 - G)     if la.norm(G1 - G) &lt; 1e-14:       break     G = G1     if i == max_iter - 1:       print(\"Warning: no convegence after \", max_iter, \"iterations\")   f = (-1/np.pi) * np.imag(np.trace(G)/n)   #plt.plot(diffs) #this is for diagnostic purposes   #plt.yscale(\"log\")   return f   #(2c) An iteration step in the calculation of the Cauchy transform # and the density for a polynomial in semicircle r.v.s. def hfsc_map(G, z, a, AA):   ''' G is a matrix, z is a complex number with positive imaginary part,   a is a bias matrix, AA is a list of matrices   needed to define the function $\\eta$.   '''   n = G.shape[0]   b = z * la.inv(Lambda(z, n) - a)   W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b   return (G + W)/2   #return W  #(3c) A function that computes the Cauchy transform and the density for #a polynomial in semicircle r.v.s. def get_density_C(x, a, AA, eps=0.01, max_iter=10000):   ''' Calculate the density at the real point x, given the data   in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.   Uses eps as the distance of the point $x + i eps$ from the   real axis.   '''   z = x + 1j * eps   n = AA[0].shape[0]   G = 1/z * np.eye(n) #initialization   diffs = np.zeros((max_iter, 1))   for i in range(max_iter):     G1 = hfsc_map(G, z, a, AA)     diffs[i] = la.norm(G1 - G)     if la.norm(G1 - G) &lt; 1e-12:       break     G = G1     if i == max_iter - 1:       print(\"Warning: no convegence after \", max_iter, \"iterations\")   f = (-1/np.pi) * np.imag(G[0, 0])   #plt.plot(diffs) #this is for diagnostic purposes   #plt.yscale(\"log\")   return f  #(4) creates a random Hermitian Gaussian matrix with approximately semicircle # distribution. def random_semicircle(size):   '''generate a random Hermitian Gaussian matrix of size n-by-n normalized by 1/sqrt(n),   where n = size'''   random_matrix = np.random.randn(size, size)   return (random_matrix + random_matrix.T)/( np.sqrt(2 * size))  #Example of usage: size = 200 A = random_semicircle(size) print(la.norm(A - A.T)) e = la.eigvalsh(A) plt.plot(e) np.trace(A * A)  def Lambda(z, size, eps = 1E-6):   ''' Lambda_eps(z) needed to calculate the distribution of a polynomial   of free random variables.'''   A = eps * 1.j * np.eye(size)   A[0, 0] = z   return A  print(Lambda(1.j, 3))  def G_semicircle(z):     \"\"\"     Computes the Cauchy transform of the semicircle distribution for a given complex number z,     ensuring that if z has a positive imaginary part, the output has a negative imaginary part,     and vice versa.      Parameters:         z (complex or array-like): The point(s) at which to evaluate the Cauchy transform.      Returns:         complex or ndarray: The value(s) of the Cauchy transform at z.     \"\"\"     z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex      # Compute the discriminant     discriminant = np.sqrt(z**2 - 4)      # Ensure the output's imaginary part has the desired symmetry     discriminant = np.where(discriminant.imag * z.imag &lt; 0, -discriminant, discriminant)      # Compute the Cauchy transform     G = (z - discriminant) / 2      return G  # Example usage z = 3 + 4j  # Example input result = G_semicircle(z) print(f\"Cauchy transform at {z} is {result}\")  # Test with an array of values z_array = [3 + 4j, 1 + 1j, 3 - 4j, 1 - 1j] result_array = G_semicircle(z_array) print(f\"Cauchy transform for array {z_array} is {result_array}\")  plt.figure() z = np.linspace(-4,4) - 0.01j plt.plot(np.imag(G_semicircle(z))) z = np.linspace(-4,4) + 0.01j plt.plot(np.imag(G_semicircle(z)))   def G_matrix_semicircle(w, B, rank):   ''' computes G(w) for the semicirle B \\otimes x,   rank is the rank of matrix B '''   w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex   n = B.shape[0]   U1, d, U2t = la.svd(B)   U2 = np.conj(U2t.T)   #print(\"U1 =\", U1)   #print(d)   #print(\"U2 =\", U2)   #print(\"should be D: \", np.conj(U1.T) @ B @ U2) #   A_transf = np.conj(U1.T) @ w @ U2   #print(\"A_transf = \", A_transf)    A11 = A_transf[0:rank, 0:rank]   A12 = A_transf[0: rank, rank:n]   A21 = A_transf[rank:n, 0: rank]   A22 = A_transf[rank:n, rank:n]   D = np.diag(d[0:rank])   #print(\"D = \", D)   S = A11 - A12 @ la.inv(A22) @ A21   #print('S = ', S)   mu, V = la.eig(la.inv(D) @ S)   #print('mu =', mu)   #print(V)   #print('S = ', V @ np.diag(mu) @ la.inv(V))   #print(\"G(mu) = \", G_semicircle(mu))   M11 = V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(D)   #print('M11 = ', M11)   M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])   #print(\"M = \", M)   G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])        @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)   return(G)  def H_matrix_semicircle(w, B, rank):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(G_matrix_semicircle(w, B, rank)) - w)   n = 3 rank = 2 A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) z = (0.0 + 1j) w = z * np.eye(n)  G = G_matrix_semicircle(w, A1, rank) print(\"G_matrix_semicircle = \", G)  H = H_matrix_semicircle(w, A1, rank) print(\"H_matrix_semicircle = \", H)  def omega(b, AA, rank, max_iter = 10000):   ''' This computes subordination function for the sum of two semicircle variables.   AA = (A1, A2), rank is a (rank1, rank2), where rank1 is the rank of matrix A1,   and rank2 is the rank of matrix A2.   '''   W0 = 1.j * np.eye(n) #(initialization)   A1 = AA[0]   A2 = AA[1]   for i in range(max_iter):     W1 = H_matrix_semicircle(W0, A1, rank = rank[0]) + b     W2 = H_matrix_semicircle(W1, A2, rank = rank[1]) + b     if la.norm(W2 - W0) &lt; 1e-12:       break     W0 = W2     if i == max_iter - 1:       print(\"Warning: no convergence after \", max_iter, \"iterations\")   return W0  #an example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)  print('result = ', omega(B, AA, rank = (2, 2)))  #(10) Cauchy transform of free Poisson def G_free_poisson(z, lambda_param):     \"\"\"     Explicit formula for the Cauchy transform of the free Poisson distribution     with parameter \u03bb.      Args:         z (complex): The point at which to evaluate the Cauchy transform.         lambda_param (float): The parameter \u03bb of the free Poisson law.      Returns:         G (complex): The value of the Cauchy transform G(z).     \"\"\"      z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex     # Compute the interval [a, b] of the support     a = (1 - np.sqrt(lambda_param))**2     #print(a)     b = (1 + np.sqrt(lambda_param))**2     #print(b)      # Compute the square root term with correct branch     sqrt_term = np.sqrt((z - a) * (z - b))     #sqrt_term = np.sqrt((1 + z - lambda_param)**2 - 4 * z) #alternative expression      sqrt_term = np.where(sqrt_term.imag * z.imag &lt; 0, -sqrt_term, sqrt_term)      # Explicit formula for the Cauchy transform     G = (1 + z - lambda_param - sqrt_term) / (2 * z)     if lambda_param &lt; 1: #in this case G also has an atom at 0 with weight (1 - lambda)       G = G + (1 - lambda_param)/z      return G    lambda_param = 4  # Set \u03bb (parameter of the free Poisson law) z = 3 + 1j          # Complex number at which to evaluate G(z)  # Compute the Cauchy transform G_z = G_free_poisson(z, lambda_param) print(f\"Cauchy transform G({z}) for \u03bb={lambda_param}: {G_z}\")  #visualization  plt.figure() #z = np.linspace(-4,4) - 0.01j #plt.plot(np.imag(G_free_poisson(z, 2))) m = 100 al = (1 - np.sqrt(lambda_param))**2 au = (1 + np.sqrt(lambda_param))**2 x =  np.linspace(al,au, m) z = x - 0.01j plt.plot(x, np.imag(G_free_poisson(z, lambda_param))) z = x + 0.01j plt.plot(x, np.imag(G_free_poisson(z, lambda_param))) plt.grid()  #let's check that this corresponds to a valid density function. f = - np.imag(G_free_poisson(z, lambda_param))/np.pi print(sum(f)* (au - al)/m)   # (11) Matrix version of the Cauchy transform for the free Poisson random variable. def G_matrix_fpoisson(w, B, rank, lambda_param):   ''' computes G(w) for the free Poisson r.v. B \\otimes x,   rank is the rank of matrix B '''   w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex   n = B.shape[0]   U1, d, U2t = la.svd(B)   U2 = np.conj(U2t.T)   A_transf = np.conj(U1.T) @ w @ U2    A11 = A_transf[0:rank, 0:rank]   A12 = A_transf[0: rank, rank:n]   A21 = A_transf[rank:n, 0: rank]   A22 = A_transf[rank:n, rank:n]   D = np.diag(d[0:rank])   S = A11 - A12 @ la.inv(A22) @ A21   mu, V = la.eig(la.inv(D) @ S)   M11 = V @ np.diag(G_free_poisson(mu, lambda_param)) @ la.inv(V) @ la.inv(D)   M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])   G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])        @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)   return(G)  def H_matrix_fpoisson(w, B, rank, lambda_param):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(G_matrix_fpoisson(w, B, rank, lambda_param)) - w)   #(13) subordination function for the sum of two matrix random variables. def omega_sub(b, AA, rank, H1_name=\"H_matrix_semicircle\", H2_name=\"H_matrix_semicircle\",               H1_kwargs=None, H2_kwargs=None, max_iter=10000):     '''     Computes subordination function omega_1(b) for the sum of two free random variables variables.      AA = (A1, A2), where A1 and A2 are matrices.     rank = (rank1, rank2), where rank1 is the rank of matrix A1, and rank2 is the rank of matrix A2.     H1_name, H2_name are string names of the functions to be applied.     H1_kwargs, H2_kwargs are dictionaries containing additional arguments for H1 and H2.     '''     n = AA[0].shape[0]  # Assuming A1 and A2 are square matrices of the same size     W0 = 1.j * np.eye(n)  # Initialization     A1, A2 = AA      # Get function references from globals()     H1 = globals()[H1_name]     H2 = globals()[H2_name]      # Initialize kwargs dictionaries if None     if H1_kwargs is None:         H1_kwargs = {}     if H2_kwargs is None:         H2_kwargs = {}      for i in range(max_iter):         W1 = H1(W0, A1, rank=rank[0], **H1_kwargs) + b         W2 = H2(W1, A2, rank=rank[1], **H2_kwargs) + b          if la.norm(W2 - W0) &lt; 1e-12:             break         W0 = W2          if i == max_iter - 1:             print(\"Warning: no convergence after\", max_iter, \"iterations\")      return W0  #(14) generator of a free Poisson matrix def random_fpoisson(size, lam):   '''generate a random Hermitian matrix of size n-by-n, where n = size, that have the free Poisson   distribution with parameter lambda.   '''   random_matrix = np.random.randn(size, int(np.floor(size * lam)))   return (random_matrix @ random_matrix.T) /size  #Example of usage: size = 200 lam = 4 A = random_fpoisson(size, lam) print(la.norm(A - A.T)) e = la.eigvalsh(A) plt.figure() plt.plot(e) np.trace(A)/size  #(15) random orthogonal matrix def random_orthogonal(n):     # Step 1: Generate a random n x n matrix A     A = np.random.randn(n, n)      # Step 2: Perform QR decomposition on A     Q, R = np.linalg.qr(A)      # Q is the orthogonal matrix we want     return Q  # Example usage n = 3  # Dimension of the matrix Q = random_orthogonal(n) print(\"Random Orthogonal Matrix Q:\\n\", Q)  # (16) This is a function that calculates the matrix Cauchy transform, provided that #the scalar Cauchy transform is known. def G_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):   ''' computes G(w) for the r.v. B \\otimes x, where x has a custom measure mu_x above,   with the scalar Cauchy transform function $G_name$, and   rank is the rank of matrix B '''    # Get function references from globals()   G = globals()[G_name]    # Initialize kwargs dictionaries if None   if G_kwargs is None:     G_kwargs = {}     w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex   n = B.shape[0]   U1, d, U2t = la.svd(B)   U2 = np.conj(U2t.T)   A_transf = np.conj(U1.T) @ w @ U2    A11 = A_transf[0:rank, 0:rank]   A12 = A_transf[0: rank, rank:n]   A21 = A_transf[rank:n, 0: rank]   A22 = A_transf[rank:n, rank:n]   D = np.diag(d[0:rank])   S = A11 - A12 @ la.inv(A22) @ A21   mu, V = la.eig(la.inv(D) @ S)   M11 = V @ np.diag(G(mu, **G_kwargs)) @ la.inv(V) @ la.inv(D)   M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])   G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])        @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)   return(G)  # (17) The H-function that corresponds to G_matrix_custom def H_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(G_matrix_custom(w, B, rank, G_name, G_kwargs)) - w)  n = 3 rank = 2 A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) z = (0.0 + 1j) w = z * np.eye(n)  G = G_matrix_custom(w, A1, rank, G_name=\"G_semicircle\") print(\"G_matrix_custom = \", G)  H = H_matrix_custom(w, A1, rank,  G_name=\"G_semicircle\") print(\"H_matrix_custom = \", H)   #(18) The scalar Cauchy transform of an arbitrary discrete distribution def cauchy_transform_discrete(z, points, weights):     \"\"\"     Computes the Cauchy transform G_mu(z) for a measure defined by     discrete points and their corresponding weights.      Parameters:     z : complex or array-like         Evaluation point(s) in the complex plane.     points : list or array-like         Locations of the discrete measure.     weights : list or array-like         Corresponding weights of the measure.     \"\"\"     z = np.asarray(z)[:, np.newaxis]  # Ensure z is a column vector     points = np.asarray(points)     weights = np.asarray(weights)     return np.sum(weights / (z - points), axis=1)  # Example usage points = np.array([-2, -1, 1])  # Support points weights = np.array([2/4, 1/4, 1/4])  # Corresponding weights z_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane G_values = cauchy_transform_discrete(z_values, points, weights)  # Plot real and imaginary parts plt.figure(figsize=(8, 5)) plt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed') plt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\") plt.xlabel(\"Re(z)\") plt.ylabel(\"G_mu(z)\") plt.legend() plt.title(\"Cauchy Transform of Given Distribution\") plt.grid() plt.show() <p>Let us try an example (borrowed from Mai-Speicher) $$ b = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} $$ $$ a_1 = \\begin{bmatrix} 0 &amp; 2 i &amp; 0 \\\\ -2i &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; i \\\\ 0 &amp; 0 &amp; 0 \\\\ -i &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_3 = \\begin{bmatrix} 0 &amp; i &amp; 0 \\\\ -i &amp; 0 &amp; -i \\\\ 0 &amp; i &amp; 0 \\end{bmatrix} $$ The matrix $S$ is then $$ S = i \\begin{bmatrix} 0 &amp; 2s_1 + s_3 &amp; s_2 \\\\ -2s_1 - s_3 &amp; 0 &amp; -s_3 \\\\ -s_2 &amp; s_3 &amp; 0 \\end{bmatrix} $$</p> In\u00a0[\u00a0]: Copied! <pre>B = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]], dtype=np.float32)\nprint(B)\nA1 = 2.j * np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 0]])\nprint(A1)\nA2 = 1.j * np.array([[0, 0, 1], [0, 0, 0], [-1, 0, 0]])\nA3 = 1.j * np.array([[0, 1, 0], [-1, 0, -1], [0, 1, 0]])\nAA = (A1, A2, A3)\netaB = eta(B, AA)\netaB\n</pre> B = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]], dtype=np.float32) print(B) A1 = 2.j * np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 0]]) print(A1) A2 = 1.j * np.array([[0, 0, 1], [0, 0, 0], [-1, 0, 0]]) A3 = 1.j * np.array([[0, 1, 0], [-1, 0, -1], [0, 1, 0]]) AA = (A1, A2, A3) etaB = eta(B, AA) etaB <p>Let us try the itetative method for solving the basic equation and iterate it to convergence.</p> In\u00a0[\u00a0]: Copied! <pre>z = .01j\nn = 3\nG0 = .1/z * np.eye(n)\nG = hfs_map(G0, z, AA)\nprint(G)\n\nmax_iter = 150\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  G1 = hfs_map(G, z, AA)\n  diffs[i] = la.norm(G1 - G)\n  G = G1\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(G)\n</pre> z = .01j n = 3 G0 = .1/z * np.eye(n) G = hfs_map(G0, z, AA) print(G)  max_iter = 150 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   G1 = hfs_map(G, z, AA)   diffs[i] = la.norm(G1 - G)   G = G1 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(G) In\u00a0[\u00a0]: Copied! <pre>f = get_density(0., AA)\nprint(f)\n</pre> f = get_density(0., AA) print(f) <p>Let us get the plot of the density. Two interesting features: (1) the density is symmetric.</p> <p>(2) the density might have a singularity at zero. -- I am not actually sure about this.</p> In\u00a0[\u00a0]: Copied! <pre>a = 8\nm = 100\nXX = np.linspace(-a, a, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n  f[i] = get_density(x, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\n</pre> a = 8 m = 100 XX = np.linspace(-a, a, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):   f[i] = get_density(x, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f)  <p>Let us now see if this aggrees with numerical data from simulations. Recall that matrix $S$ is $$ S = i \\begin{bmatrix} 0 &amp; 2s_1 + s_3 &amp; s_2 \\\\ -2s_1 - s_3 &amp; 0 &amp; -s_3 \\\\ -s_2 &amp; s_3 &amp; 0 \\end{bmatrix} $$ We will build it using our basic blocks and calculate eigenvalues. We will repeat it T times and plot the histogram of the results.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nT = 10\nEE = np.zeros((3 * size,T))\nfor count in range(T):\n  S1 = random_semicircle(size)\n  S2 = random_semicircle(size)\n  S3 = random_semicircle(size)\n  S = 1.j * np.block([[zero_m, 2 * S1 + S3, S2], [-2 * S1 - S3, zero_m, -S3], [-S2, S3, zero_m]])\n  #la.norm(S - np.conj(S.T))\n  e = la.eigvalsh(S)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\n#plt.plot(EE)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=20, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) T = 10 EE = np.zeros((3 * size,T)) for count in range(T):   S1 = random_semicircle(size)   S2 = random_semicircle(size)   S3 = random_semicircle(size)   S = 1.j * np.block([[zero_m, 2 * S1 + S3, S2], [-2 * S1 - S3, zero_m, -S3], [-S2, S3, zero_m]])   #la.norm(S - np.conj(S.T))   e = la.eigvalsh(S)   EE[:,count] = e  EE = EE.reshape(-1)  #plt.plot(EE)  plt.figure() # Plot histogram with density plt.hist(EE, bins=20, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend()   <p>Here we will be interested in the matrix $$ S =  \\begin{bmatrix} A &amp; B &amp; C \\\\ B &amp; A &amp; B \\\\ C &amp; B &amp; A \\end{bmatrix}, $$ where $A$, $B$, and $C$ are free semicircular. In this case we have $$ a_1 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}, \\, a_3 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$</p> In\u00a0[\u00a0]: Copied! <pre>A1 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nA2 = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\nA3 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A1)\nprint(A2)\nprint(A3)\nAA = (A1, A2, A3)\na = 5\nXX = np.linspace(-a, a, 100)\nm = XX.size\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n  f[i] = get_density(x, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\n</pre> A1 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) A2 = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]) A3 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A1) print(A2) print(A3) AA = (A1, A2, A3) a = 5 XX = np.linspace(-a, a, 100) m = XX.size f = np.zeros(XX.shape) for i, x in enumerate(XX):   f[i] = get_density(x, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) <p>Now we will check this result using numerical simulations</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nT = 30\nEE = np.zeros((3 * size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  C = random_semicircle(size)\n  S = np.block([[A, B, C], [B, A, B], [C, B, A]])\n  #la.norm(S - np.conj(S.T))\n  e = la.eigvalsh(S)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\n#plt.plot(EE)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) T = 30 EE = np.zeros((3 * size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   C = random_semicircle(size)   S = np.block([[A, B, C], [B, A, B], [C, B, A]])   #la.norm(S - np.conj(S.T))   e = la.eigvalsh(S)   EE[:,count] = e  EE = EE.reshape(-1)  #plt.plot(EE)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend()  <p>Here we will be interested in the matrix $$ S =  \\begin{bmatrix} 0 &amp; X_1 &amp; X_2 \\\\ X_1 &amp; 0 &amp; -1 \\\\ X_2 &amp; -1 &amp; 0 \\end{bmatrix}, $$ where $X_1$ and $X_2$ are two standard semicircular variables. In this case we have $$ a_0 = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}, \\, a_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$</p> <p>This example is significantly different from the previous example because the matrix semicirle variable has non-trivial expectation.</p> <p>First, we need to define an analogue of the hfs map, in order to be able to calculate the Cauchy transform $G_S(z) = G_{a_0 + X}$, where $X = a_1 \\otimes X_1 + a_2 \\otimes X_2$. For this we define $b = z(z I - a_0)^{-1}$ and define the map: $$ G \\mapsto \\frac{1}{2}\\Big[G + [z I - b \\eta(G)]^{-1} b\\Big]. $$ This should be iterated to convergence.</p> <p>Next we define the function that calculates the density. This function is similar defined for the matrix semicircle without the bias term but it uses hsfb map instead of hsf map.</p> <p>Let us plot density. While the result looks somewhat doubtful, however it is supported by numerical simulations.</p> In\u00a0[\u00a0]: Copied! <pre>#usage example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\n\n'''\n#checking convergence\nz = 1 + .01j\nn = 3\nG0 = 1/z * np.eye(n)\nG = hfsb_map(G0, z, A0, AA)\nprint(G)\nmax_iter = 1500\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  G1 = hfsb_map(G, z, A0, AA)\n  diffs[i] = la.norm(G1 - G)\n  G = G1\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(G)\n'''\nf = get_density_B(1., A0, AA)\nprint(f)\na = 4\nm = 100\nXX = np.linspace(-a, a, 200)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_B(x, A0, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> #usage example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2)  ''' #checking convergence z = 1 + .01j n = 3 G0 = 1/z * np.eye(n) G = hfsb_map(G0, z, A0, AA) print(G) max_iter = 1500 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   G1 = hfsb_map(G, z, A0, AA)   diffs[i] = la.norm(G1 - G)   G = G1 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(G) ''' f = get_density_B(1., A0, AA) print(f) a = 4 m = 100 XX = np.linspace(-a, a, 200) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_B(x, A0, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check.</p> <p>It appears that the results of numeric simulations are in good agreement with my theoretical results.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((3 * size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  C = random_semicircle(size)\n  S = np.block([[zero_m, B, C], [B, zero_m, -ones_m], [C, -ones_m, zero_m]])\n  #print(la.norm(S - np.conj(S.T)))\n  e = la.eigvalsh(S)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\n#plt.plot(EE)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((3 * size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   C = random_semicircle(size)   S = np.block([[zero_m, B, C], [B, zero_m, -ones_m], [C, -ones_m, zero_m]])   #print(la.norm(S - np.conj(S.T)))   e = la.eigvalsh(S)   EE[:,count] = e  EE = EE.reshape(-1)  #plt.plot(EE)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>In order to find this distribution, note that we need to calculate $G(z, b(z))$, where $$ b_\\epsilon(z) = z(\\Lambda_\\epsilon(z) - a_0)^{-1}, $$ $\\epsilon &gt; 0$ is a small regularization paremater, and $$ \\Lambda_\\epsilon(z) := \\begin{bmatrix}  z &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; i\\epsilon &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\, &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; i\\epsilon \\end{bmatrix}. $$ By definition, $$ G(z, b) = m_0(b)z^{-1} + m_1(b) z^{-2} + m_2(b) z^{-3} + \\ldots, $$ and $m_k(b)$ are moments: $$ m_k(b) := E[b(Xb)^k] = E[bXb \\ldots Xb]. $$</p> <p>Once $G(z, b(z))$ is calculated, we can calculate the Cauchy transform of the polynomial $p$ by using formula $$ \\phi[(z - p)^{-1}] = \\lim_{\\epsilon \\to 0} \\Big[ G(z, b_\\epsilon(z)) \\Big]_{1,1}. $$</p> <p>Finally, we can extract the density by the Stieljes inversion formula.</p> <p>See the realization of the functions hsfc_map and get_density_C above.</p> In\u00a0[\u00a0]: Copied! <pre>#usage example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nz = .5 + .01j\nn = 3\nG0 = 1/z * np.eye(n)\nG = hfsc_map(G0, z, A0, AA)\nprint(G)\n\nmax_iter = 200\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  G1 = hfsc_map(G, z, A0, AA)\n  diffs[i] = la.norm(G1 - G)\n  G = G1\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(G)\n</pre> #usage example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) z = .5 + .01j n = 3 G0 = 1/z * np.eye(n) G = hfsc_map(G0, z, A0, AA) print(G)  max_iter = 200 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   G1 = hfsc_map(G, z, A0, AA)   diffs[i] = la.norm(G1 - G)   G = G1 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(G) In\u00a0[\u00a0]: Copied! <pre>f = get_density_C(1., A0, AA)\nprint(f)\na = 4\nm = 200\nXX = np.linspace(-a, a, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_C(x, A0, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n\nget_density_C(.5, A0, AA)\n</pre> f = get_density_C(1., A0, AA) print(f) a = 4 m = 200 XX = np.linspace(-a, a, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_C(x, A0, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True)  get_density_C(.5, A0, AA) <p>And here is the numerical check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we replicate the results of the previous example by using the method of subordination functions from Belinschi-Mai-Speicher paper.</p> <p>The setup is the same. $$ S =  \\begin{bmatrix} 0 &amp; X_1 &amp; X_2 \\\\ X_1 &amp; 0 &amp; -1 \\\\ X_2 &amp; -1 &amp; 0 \\end{bmatrix}, $$ where $X_1$ and $X_2$ are two standard semicircular variables. In this case we have $$ a_0 = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}, \\, a_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$ However, we aim to calculate the matrix Cauchy transform differently.</p> <p>The subordination method is based on finding the fixed point of the map $$ w \\to h_y(h_x(w) + b) + b. $$ where $$h_x(w) = F_x(w) - w = \\Big[G_x(w)\\Big]^{-1} - w $$ and $h_y(w) = F_y(w) - w$.  ($x = a_1 \\otimes X_1$ and $y = a_2 \\otimes X_2$). This fixed point is the subordination function $\\omega_1(b)$ and the Cauchy transform of $x + y$ is given by $G_{x + y}(b) = G_x(\\omega_1(b))$. We want to a apply it to $b = \\Lambda_\\epsilon(z) - a_0$.</p> <p>The first thing to do is to calculate $G_x(w)$.</p> <p>In the basic approach we use the ingegral formula $$ E\\Big[ (w - b \\otimes x)^{-1}\\Big] = \\lim_{\\epsilon \\to 0}\\frac{-1}{\\pi} \\int_R (w - t b)^{-1} \\Im (G_x(t + i\\epsilon))\\, dt. $$</p> In\u00a0[\u00a0]: Copied! <pre>A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nA_test = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nprint(A_test)\nAA = (A1, A2)\nn = A0.shape[0]\nw = (0.5 + 0.1j) * np.eye(n)\nz = 0.01j\nprint(la.inv(w - z * A_test))\n\n# Define the matrix-valued function\ndef matrix_function(z, w, b):\n    \"\"\"Matrix-valued function of a complex variable z.\"\"\"\n    return -(1/np.pi) * la.inv(w - z * b) * np.imag(G_semicircle(z))\n\nprint(\"test:\", matrix_function(z, w, A_test))\n\n# Define the real path and shift it slightly above the real line\nepsilon = 1e-3  # Small imaginary shift\ndef shifted_path(x):\n    return x + 1j * epsilon\n\n# Integration limits\nal, au = -4, 4  # Example real integration limits\n\n# Perform the integration for each matrix entry\ndef cauchy_matrix_semicircle_0(w, b):\n    w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n    matrix_size = matrix_function(0, w, b).shape  # Get the shape of the matrix\n    result = np.zeros(matrix_size, dtype=complex)  # Initialize result matrix\n    for i in range(matrix_size[0]):\n        for j in range(matrix_size[1]):\n            # Define the scalar function for the (i, j)-th entry\n            def scalar_function(x, w, b):\n                z = shifted_path(x)\n                return matrix_function(z, w, b)[i, j]\n\n            scalar_func_with_params = partial(scalar_function, w = w, b = b)\n            # Perform the numerical integration\n            integral_real, _ = quad(lambda x: scalar_func_with_params(x).real, al, au)\n            integral_imag, _ = quad(lambda x: scalar_func_with_params(x).imag, al, au)\n            result[i, j] = integral_real + 1j * integral_imag  # Save the result\n    return result\n\ndef H_matrix_semicircle_0(w, A1, eps = 1E-8):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(cauchy_matrix_semicircle_0(w, A1)) - w)\n\n\n# Compute the cauchy transform via integral\nresult_matrix = cauchy_matrix_semicircle_0(w, A1)\nprint(\"Resultant matrix G after integration:\")\nprint(result_matrix)\n\n# Compute the h-function\nresult_matrix = H_matrix_semicircle_0(w, A1)\nprint(\"Resultant matrix H after integration:\")\nprint(result_matrix)\n\n\n\n#Let us visualize the result:\n\nm = 10\nx = np.linspace(-2, 2, m)\nGG = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  GG[i] = result[0, 0]\nplt.plot(np.imag(GG))\nprint(GG)\n</pre> A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) A_test = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]]) print(A0) print(A1) print(A2) print(A_test) AA = (A1, A2) n = A0.shape[0] w = (0.5 + 0.1j) * np.eye(n) z = 0.01j print(la.inv(w - z * A_test))  # Define the matrix-valued function def matrix_function(z, w, b):     \"\"\"Matrix-valued function of a complex variable z.\"\"\"     return -(1/np.pi) * la.inv(w - z * b) * np.imag(G_semicircle(z))  print(\"test:\", matrix_function(z, w, A_test))  # Define the real path and shift it slightly above the real line epsilon = 1e-3  # Small imaginary shift def shifted_path(x):     return x + 1j * epsilon  # Integration limits al, au = -4, 4  # Example real integration limits  # Perform the integration for each matrix entry def cauchy_matrix_semicircle_0(w, b):     w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex     matrix_size = matrix_function(0, w, b).shape  # Get the shape of the matrix     result = np.zeros(matrix_size, dtype=complex)  # Initialize result matrix     for i in range(matrix_size[0]):         for j in range(matrix_size[1]):             # Define the scalar function for the (i, j)-th entry             def scalar_function(x, w, b):                 z = shifted_path(x)                 return matrix_function(z, w, b)[i, j]              scalar_func_with_params = partial(scalar_function, w = w, b = b)             # Perform the numerical integration             integral_real, _ = quad(lambda x: scalar_func_with_params(x).real, al, au)             integral_imag, _ = quad(lambda x: scalar_func_with_params(x).imag, al, au)             result[i, j] = integral_real + 1j * integral_imag  # Save the result     return result  def H_matrix_semicircle_0(w, A1, eps = 1E-8):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(cauchy_matrix_semicircle_0(w, A1)) - w)   # Compute the cauchy transform via integral result_matrix = cauchy_matrix_semicircle_0(w, A1) print(\"Resultant matrix G after integration:\") print(result_matrix)  # Compute the h-function result_matrix = H_matrix_semicircle_0(w, A1) print(\"Resultant matrix H after integration:\") print(result_matrix)    #Let us visualize the result:  m = 10 x = np.linspace(-2, 2, m) GG = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   GG[i] = result[0, 0] plt.plot(np.imag(GG)) print(GG)  <p>Now we want to run the map $$ w \\to h_y(h_x(w) + b) + b. $$ to convergence. Again, we are interested in $b = \\Lambda_\\epsilon(z) - a_0$. This is supposed to give a subordination function $\\omega_1(b)$.</p> <p>Here we do have convergence alghough it is not particularly fast.</p> <p>Run the subordination functional mapping $$ w \\to h_y(h_x(w) + b) + b. $$ iteratively until convergence. Around 30 iterations is needed.</p> In\u00a0[\u00a0]: Copied! <pre>z = .5 + .01j\nn = 3\nB = Lambda(z, n) - A0\nprint(B)\n\nW0 = 1.j * np.eye(n) #(initialization)\nprint(W0)\n\nW1 = H_matrix_semicircle_0(W0, A1) + B\nW2 = H_matrix_semicircle_0(W1, A2) + B\nprint(W2)\n\nmax_iter = 30\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  W1 = H_matrix_semicircle_0(W0, A1) + B\n  #print(\"W1 = \", W1)\n  W2 = H_matrix_semicircle_0(W1, A2) + B\n  #print(\"W2 = \", W2)\n  diffs[i] = la.norm(W2 - W0)\n  W0 = W2\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(W0)\n</pre> z = .5 + .01j n = 3 B = Lambda(z, n) - A0 print(B)  W0 = 1.j * np.eye(n) #(initialization) print(W0)  W1 = H_matrix_semicircle_0(W0, A1) + B W2 = H_matrix_semicircle_0(W1, A2) + B print(W2)  max_iter = 30 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   W1 = H_matrix_semicircle_0(W0, A1) + B   #print(\"W1 = \", W1)   W2 = H_matrix_semicircle_0(W1, A2) + B   #print(\"W2 = \", W2)   diffs[i] = la.norm(W2 - W0)   W0 = W2 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(W0) <p>Then we can compute $G_{x + y}(b)$ as $G_x(\\omega_1(b))$. Then we take $G_{11}$ and apply Stieltjes inversion formula. Theh number is more or less in agreement with what we seen in the method that avoided using subordination.</p> In\u00a0[\u00a0]: Copied! <pre>-cauchy_matrix_semicircle_0(W0, A1)[0,0].imag/np.pi\n</pre> -cauchy_matrix_semicircle_0(W0, A1)[0,0].imag/np.pi <p>We look for $E(w - a_1 \\otimes x_1)^{-1}$. We will regularize $a_1$ so that it is invertible. Then $$  a_1^{-1} w = V \\begin{bmatrix}  \\mu_1 &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; \\mu_r  \\end{bmatrix} V^{-1}, $$ and then \\begin{equation} %\\label{equ_matrix_G2}  E \\big[(w - a_1\\otimes x_1)^{-1}\\big] =  E \\big[(a_1^{-1} w - I \\otimes x_1)^{-1}\\big] a_1^{-1} = V \\begin{bmatrix}G_x(\\mu_1) &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; G_x(\\mu_r)\\end{bmatrix} V^{-1} a_1^{-1}.  \\end{equation}</p> In\u00a0[\u00a0]: Copied! <pre>def cauchy_matrix_semicircle_1(w, A1, eps = 1E-8):\n  ''' This is function that computes the Cauchy transform of $A1 \\otimes X$, where\n  X is the standard semicirlce and A1 is an $n\\times n$ matrix. The argument is w,\n  so we calculate E(w - A1 \\otimes X)^{-1}. The parameter eps is for regularization to handle\n  the case when A1 is not inverible.'''\n  n = A1.shape[0]\n  mu, V = la.eig(la.inv(A1 + 1.j * eps * np.eye(n)) @ w)\n  return V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(A1 + 1.j * eps * np.eye(n))\n\ndef H_matrix_semicircle_1(w, A1, eps = 1E-8):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(Cauchy_matrix_semicircle_1(w, A1, eps)) - w)\n\n\nn = 2\nA1 = np.eye(n)\nA1 = np.array([[0, 1], [1, 0]])\n\nz = (0.0 + 0.01j)\nw = z * np.eye(n)\n\n#np.set_printoptions(precision=2)\narr = Cauchy_matrix_semicircle_1(w, A1)\nprint(arr)\nH_matrix_semicircle_1(w, A1)\n</pre> def cauchy_matrix_semicircle_1(w, A1, eps = 1E-8):   ''' This is function that computes the Cauchy transform of $A1 \\otimes X$, where   X is the standard semicirlce and A1 is an $n\\times n$ matrix. The argument is w,   so we calculate E(w - A1 \\otimes X)^{-1}. The parameter eps is for regularization to handle   the case when A1 is not inverible.'''   n = A1.shape[0]   mu, V = la.eig(la.inv(A1 + 1.j * eps * np.eye(n)) @ w)   return V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(A1 + 1.j * eps * np.eye(n))  def H_matrix_semicircle_1(w, A1, eps = 1E-8):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(Cauchy_matrix_semicircle_1(w, A1, eps)) - w)   n = 2 A1 = np.eye(n) A1 = np.array([[0, 1], [1, 0]])  z = (0.0 + 0.01j) w = z * np.eye(n)  #np.set_printoptions(precision=2) arr = Cauchy_matrix_semicircle_1(w, A1) print(arr) H_matrix_semicircle_1(w, A1)   <p>Let us do some checking.</p> In\u00a0[\u00a0]: Copied! <pre>n = 2\nrank = 2\nA1 = np.eye(n)\nA1 = np.array([[0, 1], [1, 0]])\n\n#A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nn = 3\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n\n\n\n\n\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = cauchy_matrix_semicircle_0(w, A1)\nprint(G)\nprint('old appoach = \\n', cauchy_matrix_semicircle_0(w, A1))\nprint(G_semicircle(z))\n\n#H = H_matrix_semicircle_0(w, A1, rank)\n#print(H)\n\n#Let us visualize the result:\nm = 10\nx = np.linspace(-2, 2, m)\nGG = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  GG[i] = result[0, 0]\nplt.plot(np.imag(GG))\nprint(GG)\n\nm = 10\nx = np.linspace(-2, 2, m)\nGG_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_1((x[i]+ 0.1j) * np.eye(n), A1)\n  GG_new[i] = result[0, 0]\nplt.plot(np.imag(GG_new) + 0.01)\nprint(GG_new)\n</pre> n = 2 rank = 2 A1 = np.eye(n) A1 = np.array([[0, 1], [1, 0]])  #A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) n = 3 A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])      z = (0.0 + 1j) w = z * np.eye(n)  G = cauchy_matrix_semicircle_0(w, A1) print(G) print('old appoach = \\n', cauchy_matrix_semicircle_0(w, A1)) print(G_semicircle(z))  #H = H_matrix_semicircle_0(w, A1, rank) #print(H)  #Let us visualize the result: m = 10 x = np.linspace(-2, 2, m) GG = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   GG[i] = result[0, 0] plt.plot(np.imag(GG)) print(GG)  m = 10 x = np.linspace(-2, 2, m) GG_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_1((x[i]+ 0.1j) * np.eye(n), A1)   GG_new[i] = result[0, 0] plt.plot(np.imag(GG_new) + 0.01) print(GG_new) <p>This was explained above, in the beginning of the notebook, where we listed the most basic functions.</p> <p>The code below seems to indicate that $G(w)$ is the same as calculated by the brute-force method.</p> In\u00a0[\u00a0]: Copied! <pre>A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n\n\n\n#Let us visualize the result:\nm = 10\nx = np.linspace(-2, 2, m)\nGG = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  GG[i] = result[0, 0]\nplt.plot(np.imag(GG))\nprint(GG)\n\nm = 10\nx = np.linspace(-2, 2, m)\nGG_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = G_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)\n  GG_new[i] = result[0, 0]\nplt.plot(np.imag(GG_new) + 0.01)\nprint(GG_new)\n\nplt.figure()\n\nm = 10\nx = np.linspace(-2, 2, m)\nHH = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = H_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  HH[i] = result[0, 0]\nplt.plot(np.imag(HH))\nprint(HH)\n\nm = 10\nx = np.linspace(-2, 2, m)\nHH_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = H_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)\n  HH_new[i] = result[0, 0]\nplt.plot(np.imag(HH_new) + 0.01)\nprint(HH_new)\n</pre>  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])    #Let us visualize the result: m = 10 x = np.linspace(-2, 2, m) GG = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   GG[i] = result[0, 0] plt.plot(np.imag(GG)) print(GG)  m = 10 x = np.linspace(-2, 2, m) GG_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = G_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)   GG_new[i] = result[0, 0] plt.plot(np.imag(GG_new) + 0.01) print(GG_new)  plt.figure()  m = 10 x = np.linspace(-2, 2, m) HH = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = H_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   HH[i] = result[0, 0] plt.plot(np.imag(HH)) print(HH)  m = 10 x = np.linspace(-2, 2, m) HH_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = H_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)   HH_new[i] = result[0, 0] plt.plot(np.imag(HH_new) + 0.01) print(HH_new)   <p>Let us get the subordination function. Recall that $\\omega_1(b)$ is the fixed point of the map $$ w \\to h_y(h_x(w) + b) + b. $$ where $h_x(w) = F_x(w) - w$ and $h_y(w) = F_y(w) - w$, and $F_x, F_y$ are inverses of the corresponding Cauchy transforms.</p> <p>Given that $\\omega_1(b)$ is calculated, we can find the Cauchy transform for the sum $x + y$ as $$ G_{x + y}(b) = G_x(\\omega_1(b)). $$</p> <p>We realize $\\omega_1(b)$ as a function $\\omega\\big(b, (x, y)\\big)$.</p> <p>First, let us check convergence of the method here.</p> In\u00a0[\u00a0]: Copied! <pre>def Lambda(z, size, eps = 1E-6):\n  A = eps * 1.j * np.eye(size)\n  A[0, 0] = z\n  return A\n\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\n\nW0 = 1.j * np.eye(n) #(initialization)\n  #print(W0)\nA1 = AA[0]\nA2 = AA[1]\n  #W1 = H_matrix_semicircle(W0, A1, rank = 2) + B\n  #W2 = H_matrix_semicircle(W1, A2, rank = 2) + B\n  #print(W2)\nmax_iter = 40\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  W1 = H_matrix_semicircle(W0, A1, rank = 2) + B\n  #print(\"W1 = \", W1)\n  W2 = H_matrix_semicircle(W1, A2, rank = 2) + B\n  #print(\"W2 = \", W2)\n  diffs[i] = la.norm(W2 - W0)\n  W0 = W2\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(W0)\n</pre> def Lambda(z, size, eps = 1E-6):   A = eps * 1.j * np.eye(size)   A[0, 0] = z   return A  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)   W0 = 1.j * np.eye(n) #(initialization)   #print(W0) A1 = AA[0] A2 = AA[1]   #W1 = H_matrix_semicircle(W0, A1, rank = 2) + B   #W2 = H_matrix_semicircle(W1, A2, rank = 2) + B   #print(W2) max_iter = 40 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   W1 = H_matrix_semicircle(W0, A1, rank = 2) + B   #print(\"W1 = \", W1)   W2 = H_matrix_semicircle(W1, A2, rank = 2) + B   #print(\"W2 = \", W2)   diffs[i] = la.norm(W2 - W0)   W0 = W2 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(W0) <p>Now we define the subordination function $\\omega_1(b) = \\omega\\big(b, (a_1, a_2)\\big)$.</p> In\u00a0[\u00a0]: Copied! <pre>#See the definition of the function in the beginning of the notebook\n\n#let us do some visualization\nm = 50\ntt = np.linspace(-2, 2, m)\nom = np.zeros((m, 1), dtype = np.complex128)\nfor i in range(m):\n  om[i] = omega(Lambda(tt[i] + 0.01j, n) - A0, (AA), rank = (2,2))[0,0]\n#print(om)\nplt.plot(np.imag(om))\nplt.plot(np.real(om))\n</pre> #See the definition of the function in the beginning of the notebook  #let us do some visualization m = 50 tt = np.linspace(-2, 2, m) om = np.zeros((m, 1), dtype = np.complex128) for i in range(m):   om[i] = omega(Lambda(tt[i] + 0.01j, n) - A0, (AA), rank = (2,2))[0,0] #print(om) plt.plot(np.imag(om)) plt.plot(np.real(om)) <p>Now our goal is to write the function that would compute density of the anticommutator at a given point.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator(x)\nprint(f)\n\n#visualization\nm = 50\na = 4\nXX = np.linspace(-a, a, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator(x)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator(x) print(f)  #visualization m = 50 a = 4 XX = np.linspace(-a, a, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator(x)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>And the numerical check.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>In this example, $X$ and $Y$ are free Poisson with parameters $\\lambda_1$ and $\\lambda_2$. We want to calculate their anticommutator. Here we are forced to use the subordination method.</p> <p>We havd defined the matrix version of the Cauchy transform for the free Poisson random variable $b \\otimes x$ (see above). Here we do some visualization.</p> In\u00a0[\u00a0]: Copied! <pre>n = 3\nlambda_param = 4\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nrank = 2\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = G_matrix_fpoisson(w, A1, rank, lambda_param)\nprint(\"G_matrix_fpoisson = \", G)\n\n\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n#A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nm = 100\nx = np.linspace(-2, 2, m)\nGG_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = H_matrix_fpoisson((x[i]+ 0.1j) * np.eye(n), A1, rank = 2, lambda_param = 0.5)\n  GG_new[i] = result[0, 0]\nplt.plot(np.imag(GG_new) + 0.01)\nprint(GG_new)\n</pre> n = 3 lambda_param = 4 A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) rank = 2 z = (0.0 + 1j) w = z * np.eye(n)  G = G_matrix_fpoisson(w, A1, rank, lambda_param) print(\"G_matrix_fpoisson = \", G)   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) #A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) m = 100 x = np.linspace(-2, 2, m) GG_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = H_matrix_fpoisson((x[i]+ 0.1j) * np.eye(n), A1, rank = 2, lambda_param = 0.5)   GG_new[i] = result[0, 0] plt.plot(np.imag(GG_new) + 0.01) print(GG_new) <p>Now we want to use the the general purpose subordination function omega_sub.</p> In\u00a0[\u00a0]: Copied! <pre>#an example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\nresult = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={\"lambda_param\":4},\n                   H2_kwargs={\"lambda_param\":4})\n</pre> #an example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)  result = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={\"lambda_param\":4},                    H2_kwargs={\"lambda_param\":4}) In\u00a0[\u00a0]: Copied! <pre>#let us do some visualization\nm = 50\ntt = np.linspace(1, 9, m)\nom = np.zeros((m, 1), dtype = np.complex128)\nfor i in range(m):\n  B = Lambda(tt[i] + 0.01j, n) - A0\n  om[i] = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={\"lambda_param\":4},\n                   H2_kwargs={\"lambda_param\":4})[0,0]\n#print(om)\nplt.plot(tt, np.imag(om))\nplt.plot(tt, np.real(om))\n</pre> #let us do some visualization m = 50 tt = np.linspace(1, 9, m) om = np.zeros((m, 1), dtype = np.complex128) for i in range(m):   B = Lambda(tt[i] + 0.01j, n) - A0   om[i] = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={\"lambda_param\":4},                    H2_kwargs={\"lambda_param\":4})[0,0] #print(om) plt.plot(tt, np.imag(om)) plt.plot(tt, np.real(om)) <p>Now let us try to calculate the density of the anticommutator.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_fpoisson(x, lambda_param, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={\"lambda_param\":lambda_param},\n                   H2_kwargs={\"lambda_param\":lambda_param})\n  Gxy = G_matrix_fpoisson(om, A1, rank = 2, lambda_param = lambda_param)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = 4\nf = get_density_anticommutator_fpoisson(x, lambda_param = 4)\nprint(f)\n\n#visualization\n\nm = 50\nal = 2\nau = 105\nlambda_param = 4\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_fpoisson(x,lambda_param)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n\n\nexpect = sum(XX * f)*(au - al)/m\nprint(\"expectation = \", expect)\n\n#g = - G_free_poisson(XX + .01j, 100).imag/(np.pi)\n#plt.plot(XX, g)\n</pre> def get_density_anticommutator_fpoisson(x, lambda_param, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={\"lambda_param\":lambda_param},                    H2_kwargs={\"lambda_param\":lambda_param})   Gxy = G_matrix_fpoisson(om, A1, rank = 2, lambda_param = lambda_param)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = 4 f = get_density_anticommutator_fpoisson(x, lambda_param = 4) print(f)  #visualization  m = 50 al = 2 au = 105 lambda_param = 4 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_fpoisson(x,lambda_param)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True)   expect = sum(XX * f)*(au - al)/m print(\"expectation = \", expect)  #g = - G_free_poisson(XX + .01j, 100).imag/(np.pi) #plt.plot(XX, g)  <p>Numerical check:</p> <p>Finding the distribution of the anti-commutator of the free Poisson r.v.s by using large random matrices.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nlam = 4\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_fpoisson(size, lam)\n  B = random_fpoisson(size, lam)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 lam = 4 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_fpoisson(size, lam)   B = random_fpoisson(size, lam)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Let us use the subordination function for this pair.</p> In\u00a0[\u00a0]: Copied! <pre>#an example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\nresult = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={},\n                   H2_kwargs={\"lambda_param\":4})\n</pre> #an example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)  result = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={},                    H2_kwargs={\"lambda_param\":4}) In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_S_FP(x, lambda_param, eps = 0.01):\n  '''calculates the density of the anticommutator of a semicircle and\n  a free poisson r.v. with parameter lambda_param'''\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={},\n                   H2_kwargs={\"lambda_param\":lambda_param})\n  Gxy = G_matrix_semicircle(om, A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = 4\nf = get_density_anticommutator_fpoisson(x, lambda_param = 4)\nprint(f)\n\n#visualization\n\nm = 50\nal = -20\nau = 20\nlambda_param = 4\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_S_FP(x,lambda_param)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_S_FP(x, lambda_param, eps = 0.01):   '''calculates the density of the anticommutator of a semicircle and   a free poisson r.v. with parameter lambda_param'''   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={},                    H2_kwargs={\"lambda_param\":lambda_param})   Gxy = G_matrix_semicircle(om, A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = 4 f = get_density_anticommutator_fpoisson(x, lambda_param = 4) print(f)  #visualization  m = 50 al = -20 au = 20 lambda_param = 4 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_S_FP(x,lambda_param)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nlam = 4\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_fpoisson(size, lam)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 lam = 4 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_fpoisson(size, lam)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we consider the deformed anticommutator $$ p(X, Y) = X Y + Y X + X^2 $$ (This is an example 5.2 from Belinschi-Mai-Speicher 2013).</p> <p>This polynomial has a nice linearization: $$ L = \\begin{bmatrix} 0 &amp; X &amp; \\frac{1}{2}X + Y \\\\ X &amp; 0 &amp; -1 \\\\ \\frac{1}{2}X + Y &amp; - 1 &amp; 0 \\end{bmatrix} =  \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; - 1 &amp; 0 \\end{bmatrix} + \\begin{bmatrix} 0 &amp; 1 &amp; \\frac{1}{2} \\\\ 1 &amp; 0 &amp; 0 \\\\ \\frac{1}{2} &amp; 0 &amp; 0 \\end{bmatrix} X + \\begin{bmatrix} 0 &amp; 0 &amp;  1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix} Y. $$</p> In\u00a0[\u00a0]: Copied! <pre>#usage example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nf = get_density_C(1., A0, AA)\nprint(f)\nal = -3\nau = 8\nm = 200\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_C(x, A0, AA)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> #usage example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) f = get_density_C(1., A0, AA) print(f) al = -3 au = 8 m = 200 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_C(x, A0, AA)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Nymerical check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Now we repeat the calculation using the subordination method.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator_deform(x)\nprint(f)\n\n#visualization\nm = 50\nal = -3\nau = 7\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform(x)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator_deform(x) print(f)  #visualization m = 50 al = -3 au = 7 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform(x)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we will use $X$ -- semicircle and $Y$ -- free Poisson.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform_SFP(x, lambda_param, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={},\n                   H2_kwargs={\"lambda_param\":lambda_param})\n  Gxy = G_matrix_semicircle(om, A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nlam = 4\nf = get_density_anticommutator_deform_SFP(x, lambda_param = lam)\nprint(f)\n\n#visualization\nm = 100\nal = -18\nau = 25\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform_SFP(x, lam)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform_SFP(x, lambda_param, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={},                    H2_kwargs={\"lambda_param\":lambda_param})   Gxy = G_matrix_semicircle(om, A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 lam = 4 f = get_density_anticommutator_deform_SFP(x, lambda_param = lam) print(f)  #visualization m = 100 al = -18 au = 25 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform_SFP(x, lam)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nlam = 4\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_fpoisson(size, lam)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 lam = 4 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_fpoisson(size, lam)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we considere the deformed anticommutator $$ p(X, Y) = X Y + Y X + X^2, $$ when $X$ has the distribution $\\mu_X = \\frac{1}{4}(2 \\delta_{-2} + \\delta_{-1} + \\delta_{+1})$ and $Y$ is the standard semicircle.</p> <p>This is Example 10.4. from Speicher LN on Non-commutative distributions.</p> In\u00a0[\u00a0]: Copied! <pre>def cauchy_transform_custom(z):\n    \"\"\"\n    Computes the Cauchy transform G_mu(z) of the measure\n    mu_X = (1/4)(2\u03b4_{-2} + \u03b4_{-1} + \u03b4_{+1})\n    \"\"\"\n    return (1/4) * (2 / (z + 2) + 1 / (z + 1) + 1 / (z - 1))\n\n# Example usage\nz_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane\nG_values = np.array([cauchy_transform_custom(z) for z in z_values])\n\n# Plot real and imaginary parts\nplt.figure(figsize=(8, 5))\nplt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed')\nplt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\")\nplt.xlabel(\"Re(z)\")\nplt.ylabel(\"G_mu(z)\")\nplt.legend()\nplt.title(\"Cauchy Transform of Given Distribution\")\nplt.grid()\nplt.show()\n</pre> def cauchy_transform_custom(z):     \"\"\"     Computes the Cauchy transform G_mu(z) of the measure     mu_X = (1/4)(2\u03b4_{-2} + \u03b4_{-1} + \u03b4_{+1})     \"\"\"     return (1/4) * (2 / (z + 2) + 1 / (z + 1) + 1 / (z - 1))  # Example usage z_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane G_values = np.array([cauchy_transform_custom(z) for z in z_values])  # Plot real and imaginary parts plt.figure(figsize=(8, 5)) plt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed') plt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\") plt.xlabel(\"Re(z)\") plt.ylabel(\"G_mu(z)\") plt.legend() plt.title(\"Cauchy Transform of Given Distribution\") plt.grid() plt.show()   In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform_custom(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",\n                   H2_name = \"H_matrix_semicircle\",\n                   H1_kwargs={\"G_name\":\"cauchy_transform_custom\"},\n                   H2_kwargs={})\n  Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_custom\")\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator_deform_custom(x)\nprint(f)\n\n#visualization\nm = 100\nal = -4\nau = 11\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform_custom(x)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform_custom(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",                    H2_name = \"H_matrix_semicircle\",                    H1_kwargs={\"G_name\":\"cauchy_transform_custom\"},                    H2_kwargs={})   Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_custom\")   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator_deform_custom(x) print(f)  #visualization m = 100 al = -4 au = 11 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform_custom(x)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical Check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  # Define the values for the diagonal\n  diagonal_values = np.concatenate([np.full(100, -2), np.full(50, -1), np.full(50, 1)])\n  A = np.diag(diagonal_values)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   # Define the values for the diagonal   diagonal_values = np.concatenate([np.full(100, -2), np.full(50, -1), np.full(50, 1)])   A = np.diag(diagonal_values)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here as in previous example, $X$ has measure $\\mu_X = \\frac{1}{4}(2 \\delta_{-2} + \\delta_{-1} + \\delta_{+1})$ and $Y$ has measure $\\mu_Y = \\frac{1}{2}(\\delta_1 + \\delta_3)$</p> <p>Calculation using subordination function. (This is somewhat different from the result in Speicher's Lecture notes. Maybe because X and Y switch places?)</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform_custom2(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",\n                   H2_name = \"H_matrix_custom\",\n                   H1_kwargs={\"G_name\":\"cauchy_transform_discrete\",\n                              \"G_kwargs\":{\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])}},\n                   H2_kwargs={\"G_name\":\"cauchy_transform_discrete\",\n                              \"G_kwargs\":{\"points\":np.array([1, 3]), \"weights\": np.array([1/2, 1/2])}})\n  Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_discrete\",\n                        G_kwargs={\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])})\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator_deform_custom2(x)\nprint(f)\n\n#visualization\nm = 100\nal = -10\nau = 10\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform_custom2(x)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform_custom2(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",                    H2_name = \"H_matrix_custom\",                    H1_kwargs={\"G_name\":\"cauchy_transform_discrete\",                               \"G_kwargs\":{\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])}},                    H2_kwargs={\"G_name\":\"cauchy_transform_discrete\",                               \"G_kwargs\":{\"points\":np.array([1, 3]), \"weights\": np.array([1/2, 1/2])}})   Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_discrete\",                         G_kwargs={\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])})   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator_deform_custom2(x) print(f)  #visualization m = 100 al = -10 au = 10 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform_custom2(x)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check</p> In\u00a0[\u00a0]: Copied! <pre>size = 400\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  # Define the values for the diagonal\n  diagonal_values = np.concatenate([np.full(200, -2), np.full(100, -1), np.full(100, 1)])\n  A = np.diag(diagonal_values)\n  diagonal_values = np.concatenate([np.full(200, 1), np.full(200, 3)])\n  B = np.diag(diagonal_values)\n  Q = random_orthogonal(size)\n  B = Q @ B @ Q.T\n\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 400 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   # Define the values for the diagonal   diagonal_values = np.concatenate([np.full(200, -2), np.full(100, -1), np.full(100, 1)])   A = np.diag(diagonal_values)   diagonal_values = np.concatenate([np.full(200, 1), np.full(200, 3)])   B = np.diag(diagonal_values)   Q = random_orthogonal(size)   B = Q @ B @ Q.T    e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend()"},{"location":"notebooks/10_semicircle_methods/#methods-for-calculating-the-eigevalue-density-of-matrix-semicircle-and-of-polynomials-in-free-random-variables","title":"Methods for calculating the eigevalue density of matrix semicircle and of polynomials in free random variables\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#theoretical-background-and-functions","title":"Theoretical background and functions\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-1-eigenvalue-distribution-of-a-matrix-semicircle","title":"Example 1 (Eigenvalue distribution of a matrix semicircle).\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-2-toeplitz-semicircle-matrix","title":"Example 2: Toeplitz Semicircle Matrix\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-3a-this-is-the-matrix-semicircle-that-arises-in-the-study-of-the-anti-commutator","title":"Example 3a: This is the matrix semicircle that arises in the study of the anti-commutator\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-3b-the-eigenvalue-distribution-of-the-anticommutator-of-two-semicircle-rvs-via-the-equation-for-the-biased-matrix-semicircle","title":"Example 3b: The eigenvalue distribution  of the anticommutator of two semicircle r.v.s. via the equation for the biased matrix semicircle.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-4-anticommutator-using-subordination","title":"Example 4: Anticommutator using subordination.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#the-most-basic-approach-to-calculation-of-the-cauchy-transform-gw-of-b-otimes-x","title":"The most basic approach to calculation of the Cauchy transform $G(w)$ of $b \\otimes X$\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#here-is-the-cauchy-transform-of-a-otimes-x-using-regularization","title":"Here is the Cauchy transform of $a \\otimes X$ using regularization.\u00b6","text":"<p>It works OK. May be preferable since does not require specifying or estimating the rank of a.</p>"},{"location":"notebooks/10_semicircle_methods/#more-sophisticated-approach-to-the-calculation-of-cauchy-transform-gw-of-b-otimes-x","title":"More sophisticated approach to the calculation of Cauchy transform $G(w)$ of $b \\otimes X$\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#matrix-subordination-function","title":"Matrix Subordination Function\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#calculating-the-density-of-the-anticommutator","title":"Calculating the density of the anticommutator.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-4b-anticommutator-of-poisson-rvs","title":"Example 4b. Anticommutator of Poisson r.v.s.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-4c-anticommutator-of-a-semicircle-and-a-free-poisson-rvs","title":"Example 4c: Anticommutator of a semicircle and a free Poisson r.v.s\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-5-deformed-anticommutator","title":"Example 5: Deformed anticommutator\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#5a-first-we-consider-the-situation-when-x-and-y-are-free-semicircles","title":"5a. First we consider the situation when $X$ and $Y$ are free semicircles.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-5b-deformed-anticommutator-of-the-semicircle-and-the-free-poisson","title":"Example 5b: Deformed anticommutator of the semicircle and the free Poisson.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-5c-another-deformed-anticommutator","title":"Example 5c. Another deformed anticommutator.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-59-yet-another-deformed-anticommutator","title":"Example 5.9 Yet another deformed anticommutator\u00b6","text":""}]}