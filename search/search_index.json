{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/opvalued/","title":"API \u2014 operator-valued maps","text":""},{"location":"api/opvalued/#free_matrix_laws.opvalued.covariance_map","title":"<code>covariance_map(B, A)</code>","text":"<p>Apply the operator-valued covariance map \\[     \\eta(B) \\;=\\; \\sum_{i=1}^s A_i\\, B\\, A_i, \\] where \\(A_1,\\ldots,A_s\\) are (typically Hermitian) matrices.</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>(n, n) ndarray</code> <p>Matrix the map acts on.</p> required <code>A</code> <code>sequence of (n, n) ndarrays **or** a single array of shape (s, n, n)</code> <p>The list/stack of matrices \\(A_i\\). In many applications the \\(A_i\\) are Hermitian, so \\(A_i B A_i = A_i B A_i^*\\). (The usual Kraus form uses \\(A_i B A_i^*\\); here Hermitian makes the two coincide.)</p> required <p>Returns:</p> Type Description <code>(n, n) ndarray</code> <p>The value of \\(\\eta(B)\\), with dtype promoted to <code>np.result_type(B.dtype, A.dtype)</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If shapes are inconsistent.</p> Notes <p>\u2022 Complexity: \\(\\mathcal O(s\\,n^3)\\). \u2022 If each \\(A_i\\) is Hermitian, \\(\\eta\\) is self-adjoint w.r.t. the Hilbert--Schmidt inner product. \u2022 Implementation uses batched matrix multiplies and sums over the stack of \\(A_i\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; A = [np.eye(2), 2*np.eye(2)]\n&gt;&gt;&gt; B = np.array([[0., 1.],[1., 0.]])\n&gt;&gt;&gt; covariance_map(B, A)\narray([[0., 5.],\n       [5., 0.]])\n</code></pre> Source code in <code>src/free_matrix_laws/opvalued.py</code> <pre><code>def covariance_map(B: np.ndarray, A: Sequence[np.ndarray] | np.ndarray) -&gt; np.ndarray:\n    r\"\"\"\n    Apply the operator-valued *covariance map*\n    \\\\[\n        \\eta(B) \\;=\\; \\sum_{i=1}^s A_i\\, B\\, A_i,\n    \\\\]\n    where \\\\(A_1,\\\\ldots,A_s\\\\) are (typically Hermitian) matrices.\n\n    Parameters\n    ----------\n    B : (n, n) ndarray\n        Matrix the map acts on.\n    A : sequence of (n, n) ndarrays **or** a single array of shape (s, n, n)\n        The list/stack of matrices $A_i$. In many applications the $A_i$ are Hermitian,\n        so $A_i B A_i = A_i B A_i^*$. (The usual Kraus form uses $A_i B A_i^*$;\n        here Hermitian makes the two coincide.)\n\n    Returns\n    -------\n    (n, n) ndarray\n        The value of $\\eta(B)$, with dtype promoted to `np.result_type(B.dtype, A.dtype)`.\n\n    Raises\n    ------\n    ValueError\n        If shapes are inconsistent.\n\n    Notes\n    -----\n    \u2022 Complexity: $\\mathcal O(s\\,n^3)$.  \n    \u2022 If each $A_i$ is Hermitian, $\\eta$ is self-adjoint w.r.t. the Hilbert--Schmidt inner product.\n    \u2022 Implementation uses batched matrix multiplies and sums over the stack of $A_i$.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; A = [np.eye(2), 2*np.eye(2)]\n    &gt;&gt;&gt; B = np.array([[0., 1.],[1., 0.]])\n    &gt;&gt;&gt; covariance_map(B, A)\n    array([[0., 5.],\n           [5., 0.]])\n    \"\"\"\n    B = np.asarray(B)\n    A_arr = np.asarray(A)\n\n    if B.ndim != 2 or B.shape[0] != B.shape[1]:\n        raise ValueError(f\"B must be square (n,n); got shape {B.shape!r}\")\n    n = B.shape[0]\n\n    # Accept either (s,n,n) or a single (n,n) as A\n    if A_arr.ndim == 2:\n        if A_arr.shape != (n, n):\n            raise ValueError(f\"A has shape {A_arr.shape!r}, expected {(n, n)!r} to match B.\")\n        A_arr = A_arr[None, ...]  # (1,n,n)\n    elif A_arr.ndim == 3:\n        if A_arr.shape[1:] != (n, n):\n            raise ValueError(f\"A stack has shape {A_arr.shape!r}, expected (s,{n},{n}).\")\n    else:\n        raise ValueError(\"A must be sequence of (n,n) matrices or a (s,n,n) array.\")\n\n    # Promote dtype sensibly (keep complex if present)\n    dtype = np.result_type(B.dtype, A_arr.dtype)\n    B = B.astype(dtype, copy=False)\n    A_arr = A_arr.astype(dtype, copy=False)\n\n    # Vectorized: (s,n,n) @ (n,n) -&gt; (s,n,n); then @ (s,n,n) -&gt; (s,n,n); sum over s -&gt; (n,n)\n    tmp = A_arr @ B\n    out = np.matmul(tmp, A_arr).sum(axis=0)\n    return out\n</code></pre>"},{"location":"howto/","title":"How-to guides","text":"<p>Short, task-oriented notes for common calculations with free-matrix-laws. Each guide shows the minimal imports, a small example, and any caveats.</p>"},{"location":"howto/#quick-start","title":"Quick start","text":"<pre><code>%pip install -e ..\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nfrom free_matrix_laws import eta, covariance_map  # same function; pick your preferred \n</code></pre>"},{"location":"howto/covariance_map/","title":"How to use <code>covariance_map</code>","text":"<p>Below is a minimal example. If you can't see the code block, your Markdown extensions/nav paths aren't wired correctly.</p> <pre><code>print(\"hello!\")\n</code></pre> <pre><code>import numpy as np\nfrom free_matrix_laws import covariance_map as eta\n\nn, s = 3, 2\nA1 = np.eye(n)\nA2 = 2*np.eye(n)\nB  = np.array([[0.,1.,0.],[1.,0.,0.],[0.,0.,0.]])\n\nval = eta(B, [A1, A2])          # list of A_i\n# or: val = eta(B, np.stack([A1, A2], axis=0))  # (s,n,n) stack\n\n# sanity check against a loop\nval_loop = A1 @ B @ A1 + A2 @ B @ A2\nassert np.allclose(val, val_loop)\n</code></pre>"},{"location":"notebooks/10_semicircle_methods/","title":"Methods for calculating the eigevalue density of matrix semicircle and of polynomials in free random variables","text":"<p>This is a collection of methods to calculate the eigenvalue distribution of matrix-valued semicircular random variable and of non-commutative self-adjoint polynomials in free random varriables, in particular in semicircle r.v.s. The methods are based on work of Belinschi, Rashidi Far, Helton, Mai, Speicher.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport numpy.linalg as la\nfrom scipy.integrate import quad\nfrom functools import partial\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt, patches\nfrom tqdm.notebook import tqdm, trange\n\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = [7.00, 7.00]\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n</pre> import numpy as np import numpy.linalg as la from scipy.integrate import quad from functools import partial import seaborn as sns  from matplotlib import pyplot as plt, patches from tqdm.notebook import tqdm, trange  %matplotlib inline plt.rcParams[\"figure.figsize\"] = [7.00, 7.00] plt.rcParams['image.interpolation'] = 'nearest' plt.rcParams['image.cmap'] = 'gray' <p>(1) eta: First we realize the map $\\eta: b \\to \\eta(b)$. This is the covariance function for the matrix semicircle $$S = a_1 s_1 + \\ldots + a_k s_k,$$ where $a_1, \\ldots, a_k$ are Hermitian matrices and $s_1, \\ldots, s_k$ are free semicircle variables.</p> <p>(2a) hfs_map: Next step is to write the function that will be used to solve the equation for the matrix Cauchy transform $G(z)$, $$z G = 1 + \\eta(G) G$$ by the method of iterations. The equation was derived by Speicher in R. Speicher, \u201cCombinatorial theory of the free product with amalgamation and operator-valued free probability theory,\u201d Mem. Amer. Math. Soc., vol. 132, no. 627, pp. x+88, 1998 . Another proof can be found in R. Rashidi Far, T. Oraby, W.Bryc, R. Speicher: \u201cSpectra of large block matrices,\u201d preprint 2006, cs.IT/0610045. Here the realization is for scalar $z \\in \\mathbb C$.</p> <p>The simplest approach to equation on the Cauchy transform G is to use $$ G \\to (z - \\eta(G))^{-1}.$$ The arguments in Helton, Rashidi Far, Speicher: \"Operator-valued Semicircular Elements: Solving A Quadratic Matrix Equation with Positivity Constraints.\" IMRN 2007 suggest that from the numerical viewpoint it is beneficial  to use a slightly different scheme: $$ G \\to \\frac{1}{2}\\Big[G + (z - \\eta(G))^{-1}\\Big].$$</p> <p>(3a) get_density: Then we define a function that calculate the density of a matrix semicircle at a point $x$. This program calculate the matrix Cauchy transform by iterations, then reduces it to the scalar-valued Cauchy transform by taking normalized trace, and finally calculate the density by using the Stieltjes formula.</p> <p>(2b) hfsb_map: This is a functions needed to solve a different version of the equation $$z G = 1 + \\eta(G) G,$$ namely, suppose we are interested in the matrix $S = a_0 + a_1 X_1 + \\ldots + a_n X_n$, where $X_i$ are semicircle r.v.s. For example, $$ S =  \\begin{bmatrix} 0 &amp; X_1 &amp; X_2 \\\\ X_1 &amp; 0 &amp; -1 \\\\ X_2 &amp; -1 &amp; 0 \\end{bmatrix}, $$ where $X_1$ and $X_2$ are two standard semicircular variables. In this example, we have $$ a_0 = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}, \\, a_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$</p> <p>This example is significantly different from the matrix semicircle because the matrix semicirle variable has non-trivial expectation.</p> <p>First, we need to define an analogue of the hfs map, in order to be able to calculate the Cauchy transform $G_S(z) = G_{a_0 + X}$, where $X = a_1 \\otimes X_1 + a_2 \\otimes X_2$. For this we define $b = z(z I - a_0)^{-1}$ and define the map (hfsb_map): $$ G \\mapsto \\frac{1}{2}\\Big[G + [z I - b \\eta(G)]^{-1} b\\Big]. $$ This should be iterated to convergence.</p> <p>(3b) get_density_B: uses the hsfb_map to calculate the G-transform and density of a biased matrix semicircle.</p> <p>(2c) hfsc_map: Often we use the biased matirx semicircle variable to compute the distribution of an hermitian non-commutative polynomial in semicircular variables. In order to find this distribution, note that we need to calculate $G(z, b(z))$, where $$ b_\\epsilon(z) = z(\\Lambda_\\epsilon(z) - a_0)^{-1}, $$ $\\epsilon &gt; 0$ is a small regularization paremater, and $$ \\Lambda_\\epsilon(z) := \\begin{bmatrix}  z &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; i\\epsilon &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\, &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; i\\epsilon \\end{bmatrix}. $$ By definition, $$ G(z, b) = m_0(b)z^{-1} + m_1(b) z^{-2} + m_2(b) z^{-3} + \\ldots, $$ and $m_k(b)$ are moments: $$ m_k(b) := E[b(Xb)^k] = E[bXb \\ldots Xb]. $$</p> <p>Once $G(z, b(z))$ is calculated, we can calculate the Cauchy transform of the polynomial $p$ by using formula $$ \\phi[(z - p)^{-1}] = \\lim_{\\epsilon \\to 0} \\Big[ G(z, b_\\epsilon(z)) \\Big]_{1,1}. $$</p> <p>Finally, we can extract the density by the Stieljes inversion formula.</p> <p>For the first step we will modify a little bit the hsfb_map, and the modified version is hsfc_map.</p> <p>(3c) get_density_C: This is the function that uses iterations of hsfc_map to calculate the Cauchy transform and the density of the polynomial in semicircular variables. It requires coefficients in the linearization of the polynomial, it will not construct the linearization for you.</p> <p>(4) random_semicircle: We will check that the free probability methods give a good approximation to Gaussian block matrices. So we also define a basic block for these matrices.</p> <p>(5) Lambda: In calculation of the distribution of polynomials we will also need function $$ \\Lambda_\\epsilon(z) := \\begin{bmatrix}  z &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; i\\epsilon &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\, &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; i\\epsilon \\end{bmatrix} $$</p> <p>(6) G_semicircle: In some calculations, we need the Cauchy transform of the standard semicircle variable.</p> <p>(7) G-matrix semicircle: This is the function that computes the matrix Cauchy transform of $b \\otimes S$ where $S$ is a semicircle random variable. This is a relatively sophisticated method. For more simple methods based on integration and regularization approaches, see the example about anticommutator via subordination below.</p> <p>Take unitary matrices $U_1$ and $U_2$ so that $$  U_1^\\ast B U_2 = \\begin{bmatrix}  D &amp; 0 \\\\ 0 &amp; 0,  \\end{bmatrix}  $$ where $D = diag \\{\\lambda_1, \\ldots, \\lambda_r\\}$ is an invertible diagonal $r\\times r$ matrix, and $0$'s  represent matrices of appropriate dimensions. For self-adjoint $B$ we can take $U_1 = U_2$.</p> <p>Let $$    U_1^\\ast A U_2 = \\begin{bmatrix}a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22}    \\end{bmatrix}, $$ where $a_{11}$ is an $r\\times r$ matrix and let $$ S := a_{11} - a_{12} a_{22}^{-1} a_{21}. $$ Then \\begin{equation} %\\label{equ_matrix_G1} G_{B\\otimes x}(A) = E (A - B\\otimes x)^{-1}  = U_2 \\begin{bmatrix} I &amp; 0 \\\\ -  a_{22}^{-1} a_{21} &amp; I \\end{bmatrix}  M \\begin{bmatrix} I &amp;  -  a_{12} a_{22}^{-1} \\\\ 0 &amp; I \\end{bmatrix} U_1^\\ast, \\end{equation} where $$  M = \\begin{bmatrix}E (S - D \\otimes x)^{-1} &amp; 0 \\\\ 0 &amp; a_{22}^{-1}\\end{bmatrix}.    $$</p> <p>It remains to calculate $E (S - D \\otimes x)^{-1}$, and since $D$ is an invertible diagonal matrix, we can apply our original approach. Let us assume that $D^{-1} S$ is diagonalizable (this will hold for a generic $A$), then we can write $$  D^{-1} S = V \\begin{bmatrix}\\mu_1 &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; \\mu_r  \\end{bmatrix} V^{-1},  $$ and \\begin{equation} %\\label{equ_matrix_G2}  E \\big[(S - D\\otimes X)^{-1}\\big] =  E \\big[(D^{-1} S - I \\otimes X)^{-1}\\big] D^{-1} =  V \\begin{bmatrix}G_x(\\mu_1) &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; G_x(\\mu_r)  \\end{bmatrix} V^{-1} D^{-1}.  \\end{equation}</p> <p>(8) H_matrix_Semicircle: $H_{B \\otimes S}(w) = G_{B \\otimes S}^{-1}(w) - w$.</p> <p>(9) omega: Matrix subordination function. Recall that $\\omega_1(b)$ is the fixed point of the map $$ w \\to h_y(h_x(w) + b) + b. $$ where $h_x(w) = F_x(w) - w$ and $h_y(w) = F_y(w) - w$, and $F_x, F_y$ are inverses of the corresponding Cauchy transforms.</p> <p>Given that $\\omega_1(b)$ is calculated, we can find the Cauchy transform for the sum $x + y$ as $$ G_{x + y}(b) = G_x(\\omega_1(b)). $$</p> <p>We realize $\\omega_1(b)$ as a function $\\omega\\big(b, (a_1, a_2)\\big)$. This version of omega is only for semicircle variables $x = a_1 \\otimes S_1$ and $y = a_2 \\otimes S_2$.</p> <p>(10) G_free_Poisson: Cauchy transform of the free Poisson distribution with parameter $\\lambda$.</p> <p>(11) G_matrix_fpoisson: Matrix version of the Cauchy transform for the free Poisson r.v. with parameter $\\lambda$.</p> <p>(12) H_matrix_fpoisson: $G^{-1}(w) - w$.</p> <p>(13) omega_sum: computes the subordination function $\\omega_1$ for the sum of two matrix random variables, not necessarily semicircle. Requires H-functions for these random variables.</p> <p>(14) random_fpoisson: generates a random matrix with free Poisson distribution.</p> <p>(15) random_orthogonal: generates a random orthogonal matrix</p> <p>(16) G_matrix_custom: This is a function that calculates the matrix Cauchy transform, provided that the scalar Cauchy transform is known.</p> <p>(17) H_matrix_custom: The H-function that corresponds to G_matrix_custom</p> <p>(18) cauchy_transform_discrete: The scalar Cauchy transform of an arbitrary discrete distribution.</p> In\u00a0[\u00a0]: Copied! <pre>#(1) covariance map\ndef eta(B, AA):\n  '''\n  AA is a list or a tuple of Hermitian matrices A_1, \\ldots, A_k\n  B is sent to A_1 B A_1 + \\ldots + A_k B A_k\n  '''\n  n = B.shape[0]\n  s = len(AA)\n  result = np.zeros((n, n), dtype = np.float32)\n  for i in range(s):\n    result = result + AA[i] @ B @ AA[i]\n  return result\n\n#(2a) Iteration step needed to calculate the cauchy transform for\n# a matrix semicircle r.v.\ndef hfs_map(G, z, AA):\n  ''' G is a matrix, z is a complex number with positive imaginary part,\n  G is mapped to a smoothed version of $(z - \\eta(G))^{-1}$. AA is a list of matrices\n  needed to define the function $\\eta$.\n  '''\n  n = G.shape[0]\n  #return la.inv(z * np.eye(n) - eta(G, AA))\n  return (G + la.inv(z * np.eye(n) - eta(G, AA)))/2\n\n#(3a) A function that calculates the Cauchy transform and the distribution\n#density for a matrix semicircle r.v.\ndef get_density(x, AA, eps=0.01, max_iter=10000):\n  ''' Calculate the density at the real point x, given the data\n  in the tuple of matrices $AA = (A1, \\ldots, As)$\n  Uses eps as the distance of the point $x + i eps$ from the\n  real axis.\n  '''\n  z = x + 1j * eps\n  n = AA[0].shape[0]\n  G = 1/z * np.eye(n) #initialization\n  diffs = np.zeros((max_iter, 1))\n  for i in range(max_iter):\n    G1 = hfs_map(G, z, AA)\n    diffs[i] = la.norm(G1 - G)\n    if la.norm(G1 - G) &lt; 1e-10:\n      break\n    G = G1\n    if i == max_iter - 1:\n      print(\"Warning: no convegence after \", max_iter, \"iterations\")\n  f = (-1/np.pi) * np.imag(np.trace(G)/n)\n  #plt.plot(diffs) #this is for diagnostic purposes\n  #plt.yscale(\"log\")\n  return f\n\n\n#(2b) the main iteration steps in the calculation of the density for a\n#biased matrix semicircle\ndef hfsb_map(G, z, a, AA):\n  ''' G is a matrix, z is a complex number with positive imaginary part,\n  a is a bias matrix, AA is a list of matrices\n  needed to define the function $\\eta$.\n  '''\n  n = G.shape[0]\n  b = z * la.inv(z * np.eye(n) - a)\n  W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b\n  return (G + W)/2\n  #return W\n\n#(3b) calculates the Cauchy transform and the density of a biased matrix semicircle\ndef get_density_B(x, a, AA, eps=0.01, max_iter=10000):\n  ''' Calculate the density at the real point x, given the data\n  in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.\n  Uses eps as the distance of the point $x + i eps$ from the\n  real axis.\n  '''\n  z = x + 1j * eps\n  n = AA[0].shape[0]\n  G = 1/z * np.eye(n) #initialization\n  diffs = np.zeros((max_iter, 1))\n  for i in range(max_iter):\n    G1 = hfsb_map(G, z, a, AA)\n    diffs[i] = la.norm(G1 - G)\n    if la.norm(G1 - G) &lt; 1e-14:\n      break\n    G = G1\n    if i == max_iter - 1:\n      print(\"Warning: no convegence after \", max_iter, \"iterations\")\n  f = (-1/np.pi) * np.imag(np.trace(G)/n)\n  #plt.plot(diffs) #this is for diagnostic purposes\n  #plt.yscale(\"log\")\n  return f\n\n\n#(2c) An iteration step in the calculation of the Cauchy transform\n# and the density for a polynomial in semicircle r.v.s.\ndef hfsc_map(G, z, a, AA):\n  ''' G is a matrix, z is a complex number with positive imaginary part,\n  a is a bias matrix, AA is a list of matrices\n  needed to define the function $\\eta$.\n  '''\n  n = G.shape[0]\n  b = z * la.inv(Lambda(z, n) - a)\n  W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b\n  return (G + W)/2\n  #return W\n\n#(3c) A function that computes the Cauchy transform and the density for\n#a polynomial in semicircle r.v.s.\ndef get_density_C(x, a, AA, eps=0.01, max_iter=10000):\n  ''' Calculate the density at the real point x, given the data\n  in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.\n  Uses eps as the distance of the point $x + i eps$ from the\n  real axis.\n  '''\n  z = x + 1j * eps\n  n = AA[0].shape[0]\n  G = 1/z * np.eye(n) #initialization\n  diffs = np.zeros((max_iter, 1))\n  for i in range(max_iter):\n    G1 = hfsc_map(G, z, a, AA)\n    diffs[i] = la.norm(G1 - G)\n    if la.norm(G1 - G) &lt; 1e-12:\n      break\n    G = G1\n    if i == max_iter - 1:\n      print(\"Warning: no convegence after \", max_iter, \"iterations\")\n  f = (-1/np.pi) * np.imag(G[0, 0])\n  #plt.plot(diffs) #this is for diagnostic purposes\n  #plt.yscale(\"log\")\n  return f\n\n#(4) creates a random Hermitian Gaussian matrix with approximately semicircle\n# distribution.\ndef random_semicircle(size):\n  '''generate a random Hermitian Gaussian matrix of size n-by-n normalized by 1/sqrt(n),\n  where n = size'''\n  random_matrix = np.random.randn(size, size)\n  return (random_matrix + random_matrix.T)/( np.sqrt(2 * size))\n\n#Example of usage:\nsize = 200\nA = random_semicircle(size)\nprint(la.norm(A - A.T))\ne = la.eigvalsh(A)\nplt.plot(e)\nnp.trace(A * A)\n\ndef Lambda(z, size, eps = 1E-6):\n  ''' Lambda_eps(z) needed to calculate the distribution of a polynomial\n  of free random variables.'''\n  A = eps * 1.j * np.eye(size)\n  A[0, 0] = z\n  return A\n\nprint(Lambda(1.j, 3))\n\ndef G_semicircle(z):\n    \"\"\"\n    Computes the Cauchy transform of the semicircle distribution for a given complex number z,\n    ensuring that if z has a positive imaginary part, the output has a negative imaginary part,\n    and vice versa.\n\n    Parameters:\n        z (complex or array-like): The point(s) at which to evaluate the Cauchy transform.\n\n    Returns:\n        complex or ndarray: The value(s) of the Cauchy transform at z.\n    \"\"\"\n    z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex\n\n    # Compute the discriminant\n    discriminant = np.sqrt(z**2 - 4)\n\n    # Ensure the output's imaginary part has the desired symmetry\n    discriminant = np.where(discriminant.imag * z.imag &lt; 0, -discriminant, discriminant)\n\n    # Compute the Cauchy transform\n    G = (z - discriminant) / 2\n\n    return G\n\n# Example usage\nz = 3 + 4j  # Example input\nresult = G_semicircle(z)\nprint(f\"Cauchy transform at {z} is {result}\")\n\n# Test with an array of values\nz_array = [3 + 4j, 1 + 1j, 3 - 4j, 1 - 1j]\nresult_array = G_semicircle(z_array)\nprint(f\"Cauchy transform for array {z_array} is {result_array}\")\n\nplt.figure()\nz = np.linspace(-4,4) - 0.01j\nplt.plot(np.imag(G_semicircle(z)))\nz = np.linspace(-4,4) + 0.01j\nplt.plot(np.imag(G_semicircle(z)))\n\n\ndef G_matrix_semicircle(w, B, rank):\n  ''' computes G(w) for the semicirle B \\otimes x,\n  rank is the rank of matrix B '''\n  w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n  n = B.shape[0]\n  U1, d, U2t = la.svd(B)\n  U2 = np.conj(U2t.T)\n  #print(\"U1 =\", U1)\n  #print(d)\n  #print(\"U2 =\", U2)\n  #print(\"should be D: \", np.conj(U1.T) @ B @ U2) #\n  A_transf = np.conj(U1.T) @ w @ U2\n  #print(\"A_transf = \", A_transf)\n\n  A11 = A_transf[0:rank, 0:rank]\n  A12 = A_transf[0: rank, rank:n]\n  A21 = A_transf[rank:n, 0: rank]\n  A22 = A_transf[rank:n, rank:n]\n  D = np.diag(d[0:rank])\n  #print(\"D = \", D)\n  S = A11 - A12 @ la.inv(A22) @ A21\n  #print('S = ', S)\n  mu, V = la.eig(la.inv(D) @ S)\n  #print('mu =', mu)\n  #print(V)\n  #print('S = ', V @ np.diag(mu) @ la.inv(V))\n  #print(\"G(mu) = \", G_semicircle(mu))\n  M11 = V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(D)\n  #print('M11 = ', M11)\n  M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])\n  #print(\"M = \", M)\n  G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])\n       @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)\n  return(G)\n\ndef H_matrix_semicircle(w, B, rank):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(G_matrix_semicircle(w, B, rank)) - w)\n\n\nn = 3\nrank = 2\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = G_matrix_semicircle(w, A1, rank)\nprint(\"G_matrix_semicircle = \", G)\n\nH = H_matrix_semicircle(w, A1, rank)\nprint(\"H_matrix_semicircle = \", H)\n\ndef omega(b, AA, rank, max_iter = 10000):\n  ''' This computes subordination function for the sum of two semicircle variables.\n  AA = (A1, A2), rank is a (rank1, rank2), where rank1 is the rank of matrix A1,\n  and rank2 is the rank of matrix A2.\n  '''\n  W0 = 1.j * np.eye(n) #(initialization)\n  A1 = AA[0]\n  A2 = AA[1]\n  for i in range(max_iter):\n    W1 = H_matrix_semicircle(W0, A1, rank = rank[0]) + b\n    W2 = H_matrix_semicircle(W1, A2, rank = rank[1]) + b\n    if la.norm(W2 - W0) &lt; 1e-12:\n      break\n    W0 = W2\n    if i == max_iter - 1:\n      print(\"Warning: no convergence after \", max_iter, \"iterations\")\n  return W0\n\n#an example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\nprint('result = ', omega(B, AA, rank = (2, 2)))\n\n#(10) Cauchy transform of free Poisson\ndef G_free_poisson(z, lambda_param):\n    \"\"\"\n    Explicit formula for the Cauchy transform of the free Poisson distribution\n    with parameter \u03bb.\n\n    Args:\n        z (complex): The point at which to evaluate the Cauchy transform.\n        lambda_param (float): The parameter \u03bb of the free Poisson law.\n\n    Returns:\n        G (complex): The value of the Cauchy transform G(z).\n    \"\"\"\n\n    z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex\n    # Compute the interval [a, b] of the support\n    a = (1 - np.sqrt(lambda_param))**2\n    #print(a)\n    b = (1 + np.sqrt(lambda_param))**2\n    #print(b)\n\n    # Compute the square root term with correct branch\n    sqrt_term = np.sqrt((z - a) * (z - b))\n    #sqrt_term = np.sqrt((1 + z - lambda_param)**2 - 4 * z) #alternative expression\n\n    sqrt_term = np.where(sqrt_term.imag * z.imag &lt; 0, -sqrt_term, sqrt_term)\n\n    # Explicit formula for the Cauchy transform\n    G = (1 + z - lambda_param - sqrt_term) / (2 * z)\n    if lambda_param &lt; 1: #in this case G also has an atom at 0 with weight (1 - lambda)\n      G = G + (1 - lambda_param)/z\n\n    return G\n\n\n\nlambda_param = 4  # Set \u03bb (parameter of the free Poisson law)\nz = 3 + 1j          # Complex number at which to evaluate G(z)\n\n# Compute the Cauchy transform\nG_z = G_free_poisson(z, lambda_param)\nprint(f\"Cauchy transform G({z}) for \u03bb={lambda_param}: {G_z}\")\n\n#visualization\n\nplt.figure()\n#z = np.linspace(-4,4) - 0.01j\n#plt.plot(np.imag(G_free_poisson(z, 2)))\nm = 100\nal = (1 - np.sqrt(lambda_param))**2\nau = (1 + np.sqrt(lambda_param))**2\nx =  np.linspace(al,au, m)\nz = x - 0.01j\nplt.plot(x, np.imag(G_free_poisson(z, lambda_param)))\nz = x + 0.01j\nplt.plot(x, np.imag(G_free_poisson(z, lambda_param)))\nplt.grid()\n\n#let's check that this corresponds to a valid density function.\nf = - np.imag(G_free_poisson(z, lambda_param))/np.pi\nprint(sum(f)* (au - al)/m)\n\n\n# (11) Matrix version of the Cauchy transform for the free Poisson random variable.\ndef G_matrix_fpoisson(w, B, rank, lambda_param):\n  ''' computes G(w) for the free Poisson r.v. B \\otimes x,\n  rank is the rank of matrix B '''\n  w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n  n = B.shape[0]\n  U1, d, U2t = la.svd(B)\n  U2 = np.conj(U2t.T)\n  A_transf = np.conj(U1.T) @ w @ U2\n\n  A11 = A_transf[0:rank, 0:rank]\n  A12 = A_transf[0: rank, rank:n]\n  A21 = A_transf[rank:n, 0: rank]\n  A22 = A_transf[rank:n, rank:n]\n  D = np.diag(d[0:rank])\n  S = A11 - A12 @ la.inv(A22) @ A21\n  mu, V = la.eig(la.inv(D) @ S)\n  M11 = V @ np.diag(G_free_poisson(mu, lambda_param)) @ la.inv(V) @ la.inv(D)\n  M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])\n  G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])\n       @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)\n  return(G)\n\ndef H_matrix_fpoisson(w, B, rank, lambda_param):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(G_matrix_fpoisson(w, B, rank, lambda_param)) - w)\n\n\n#(13) subordination function for the sum of two matrix random variables.\ndef omega_sub(b, AA, rank, H1_name=\"H_matrix_semicircle\", H2_name=\"H_matrix_semicircle\",\n              H1_kwargs=None, H2_kwargs=None, max_iter=10000):\n    '''\n    Computes subordination function omega_1(b) for the sum of two free random variables variables.\n\n    AA = (A1, A2), where A1 and A2 are matrices.\n    rank = (rank1, rank2), where rank1 is the rank of matrix A1, and rank2 is the rank of matrix A2.\n    H1_name, H2_name are string names of the functions to be applied.\n    H1_kwargs, H2_kwargs are dictionaries containing additional arguments for H1 and H2.\n    '''\n    n = AA[0].shape[0]  # Assuming A1 and A2 are square matrices of the same size\n    W0 = 1.j * np.eye(n)  # Initialization\n    A1, A2 = AA\n\n    # Get function references from globals()\n    H1 = globals()[H1_name]\n    H2 = globals()[H2_name]\n\n    # Initialize kwargs dictionaries if None\n    if H1_kwargs is None:\n        H1_kwargs = {}\n    if H2_kwargs is None:\n        H2_kwargs = {}\n\n    for i in range(max_iter):\n        W1 = H1(W0, A1, rank=rank[0], **H1_kwargs) + b\n        W2 = H2(W1, A2, rank=rank[1], **H2_kwargs) + b\n\n        if la.norm(W2 - W0) &lt; 1e-12:\n            break\n        W0 = W2\n\n        if i == max_iter - 1:\n            print(\"Warning: no convergence after\", max_iter, \"iterations\")\n\n    return W0\n\n#(14) generator of a free Poisson matrix\ndef random_fpoisson(size, lam):\n  '''generate a random Hermitian matrix of size n-by-n, where n = size, that have the free Poisson\n  distribution with parameter lambda.\n  '''\n  random_matrix = np.random.randn(size, int(np.floor(size * lam)))\n  return (random_matrix @ random_matrix.T) /size\n\n#Example of usage:\nsize = 200\nlam = 4\nA = random_fpoisson(size, lam)\nprint(la.norm(A - A.T))\ne = la.eigvalsh(A)\nplt.figure()\nplt.plot(e)\nnp.trace(A)/size\n\n#(15) random orthogonal matrix\ndef random_orthogonal(n):\n    # Step 1: Generate a random n x n matrix A\n    A = np.random.randn(n, n)\n\n    # Step 2: Perform QR decomposition on A\n    Q, R = np.linalg.qr(A)\n\n    # Q is the orthogonal matrix we want\n    return Q\n\n# Example usage\nn = 3  # Dimension of the matrix\nQ = random_orthogonal(n)\nprint(\"Random Orthogonal Matrix Q:\\n\", Q)\n\n# (16) This is a function that calculates the matrix Cauchy transform, provided that\n#the scalar Cauchy transform is known.\ndef G_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):\n  ''' computes G(w) for the r.v. B \\otimes x, where x has a custom measure mu_x above,\n  with the scalar Cauchy transform function $G_name$, and\n  rank is the rank of matrix B '''\n\n  # Get function references from globals()\n  G = globals()[G_name]\n\n  # Initialize kwargs dictionaries if None\n  if G_kwargs is None:\n    G_kwargs = {}\n\n\n  w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n  n = B.shape[0]\n  U1, d, U2t = la.svd(B)\n  U2 = np.conj(U2t.T)\n  A_transf = np.conj(U1.T) @ w @ U2\n\n  A11 = A_transf[0:rank, 0:rank]\n  A12 = A_transf[0: rank, rank:n]\n  A21 = A_transf[rank:n, 0: rank]\n  A22 = A_transf[rank:n, rank:n]\n  D = np.diag(d[0:rank])\n  S = A11 - A12 @ la.inv(A22) @ A21\n  mu, V = la.eig(la.inv(D) @ S)\n  M11 = V @ np.diag(G(mu, **G_kwargs)) @ la.inv(V) @ la.inv(D)\n  M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])\n  G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])\n       @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)\n  return(G)\n\n# (17) The H-function that corresponds to G_matrix_custom\ndef H_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(G_matrix_custom(w, B, rank, G_name, G_kwargs)) - w)\n\nn = 3\nrank = 2\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = G_matrix_custom(w, A1, rank, G_name=\"G_semicircle\")\nprint(\"G_matrix_custom = \", G)\n\nH = H_matrix_custom(w, A1, rank,  G_name=\"G_semicircle\")\nprint(\"H_matrix_custom = \", H)\n\n\n#(18) The scalar Cauchy transform of an arbitrary discrete distribution\ndef cauchy_transform_discrete(z, points, weights):\n    \"\"\"\n    Computes the Cauchy transform G_mu(z) for a measure defined by\n    discrete points and their corresponding weights.\n\n    Parameters:\n    z : complex or array-like\n        Evaluation point(s) in the complex plane.\n    points : list or array-like\n        Locations of the discrete measure.\n    weights : list or array-like\n        Corresponding weights of the measure.\n    \"\"\"\n    z = np.asarray(z)[:, np.newaxis]  # Ensure z is a column vector\n    points = np.asarray(points)\n    weights = np.asarray(weights)\n    return np.sum(weights / (z - points), axis=1)\n\n# Example usage\npoints = np.array([-2, -1, 1])  # Support points\nweights = np.array([2/4, 1/4, 1/4])  # Corresponding weights\nz_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane\nG_values = cauchy_transform_discrete(z_values, points, weights)\n\n# Plot real and imaginary parts\nplt.figure(figsize=(8, 5))\nplt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed')\nplt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\")\nplt.xlabel(\"Re(z)\")\nplt.ylabel(\"G_mu(z)\")\nplt.legend()\nplt.title(\"Cauchy Transform of Given Distribution\")\nplt.grid()\nplt.show()\n</pre> #(1) covariance map def eta(B, AA):   '''   AA is a list or a tuple of Hermitian matrices A_1, \\ldots, A_k   B is sent to A_1 B A_1 + \\ldots + A_k B A_k   '''   n = B.shape[0]   s = len(AA)   result = np.zeros((n, n), dtype = np.float32)   for i in range(s):     result = result + AA[i] @ B @ AA[i]   return result  #(2a) Iteration step needed to calculate the cauchy transform for # a matrix semicircle r.v. def hfs_map(G, z, AA):   ''' G is a matrix, z is a complex number with positive imaginary part,   G is mapped to a smoothed version of $(z - \\eta(G))^{-1}$. AA is a list of matrices   needed to define the function $\\eta$.   '''   n = G.shape[0]   #return la.inv(z * np.eye(n) - eta(G, AA))   return (G + la.inv(z * np.eye(n) - eta(G, AA)))/2  #(3a) A function that calculates the Cauchy transform and the distribution #density for a matrix semicircle r.v. def get_density(x, AA, eps=0.01, max_iter=10000):   ''' Calculate the density at the real point x, given the data   in the tuple of matrices $AA = (A1, \\ldots, As)$   Uses eps as the distance of the point $x + i eps$ from the   real axis.   '''   z = x + 1j * eps   n = AA[0].shape[0]   G = 1/z * np.eye(n) #initialization   diffs = np.zeros((max_iter, 1))   for i in range(max_iter):     G1 = hfs_map(G, z, AA)     diffs[i] = la.norm(G1 - G)     if la.norm(G1 - G) &lt; 1e-10:       break     G = G1     if i == max_iter - 1:       print(\"Warning: no convegence after \", max_iter, \"iterations\")   f = (-1/np.pi) * np.imag(np.trace(G)/n)   #plt.plot(diffs) #this is for diagnostic purposes   #plt.yscale(\"log\")   return f   #(2b) the main iteration steps in the calculation of the density for a #biased matrix semicircle def hfsb_map(G, z, a, AA):   ''' G is a matrix, z is a complex number with positive imaginary part,   a is a bias matrix, AA is a list of matrices   needed to define the function $\\eta$.   '''   n = G.shape[0]   b = z * la.inv(z * np.eye(n) - a)   W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b   return (G + W)/2   #return W  #(3b) calculates the Cauchy transform and the density of a biased matrix semicircle def get_density_B(x, a, AA, eps=0.01, max_iter=10000):   ''' Calculate the density at the real point x, given the data   in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.   Uses eps as the distance of the point $x + i eps$ from the   real axis.   '''   z = x + 1j * eps   n = AA[0].shape[0]   G = 1/z * np.eye(n) #initialization   diffs = np.zeros((max_iter, 1))   for i in range(max_iter):     G1 = hfsb_map(G, z, a, AA)     diffs[i] = la.norm(G1 - G)     if la.norm(G1 - G) &lt; 1e-14:       break     G = G1     if i == max_iter - 1:       print(\"Warning: no convegence after \", max_iter, \"iterations\")   f = (-1/np.pi) * np.imag(np.trace(G)/n)   #plt.plot(diffs) #this is for diagnostic purposes   #plt.yscale(\"log\")   return f   #(2c) An iteration step in the calculation of the Cauchy transform # and the density for a polynomial in semicircle r.v.s. def hfsc_map(G, z, a, AA):   ''' G is a matrix, z is a complex number with positive imaginary part,   a is a bias matrix, AA is a list of matrices   needed to define the function $\\eta$.   '''   n = G.shape[0]   b = z * la.inv(Lambda(z, n) - a)   W = la.inv(z * np.eye(n) - b @ eta(G, AA)) @ b   return (G + W)/2   #return W  #(3c) A function that computes the Cauchy transform and the density for #a polynomial in semicircle r.v.s. def get_density_C(x, a, AA, eps=0.01, max_iter=10000):   ''' Calculate the density at the real point x, given the data   in the tuple of matrices $AA = (A1, \\ldots, As)$, and the bias matrix a.   Uses eps as the distance of the point $x + i eps$ from the   real axis.   '''   z = x + 1j * eps   n = AA[0].shape[0]   G = 1/z * np.eye(n) #initialization   diffs = np.zeros((max_iter, 1))   for i in range(max_iter):     G1 = hfsc_map(G, z, a, AA)     diffs[i] = la.norm(G1 - G)     if la.norm(G1 - G) &lt; 1e-12:       break     G = G1     if i == max_iter - 1:       print(\"Warning: no convegence after \", max_iter, \"iterations\")   f = (-1/np.pi) * np.imag(G[0, 0])   #plt.plot(diffs) #this is for diagnostic purposes   #plt.yscale(\"log\")   return f  #(4) creates a random Hermitian Gaussian matrix with approximately semicircle # distribution. def random_semicircle(size):   '''generate a random Hermitian Gaussian matrix of size n-by-n normalized by 1/sqrt(n),   where n = size'''   random_matrix = np.random.randn(size, size)   return (random_matrix + random_matrix.T)/( np.sqrt(2 * size))  #Example of usage: size = 200 A = random_semicircle(size) print(la.norm(A - A.T)) e = la.eigvalsh(A) plt.plot(e) np.trace(A * A)  def Lambda(z, size, eps = 1E-6):   ''' Lambda_eps(z) needed to calculate the distribution of a polynomial   of free random variables.'''   A = eps * 1.j * np.eye(size)   A[0, 0] = z   return A  print(Lambda(1.j, 3))  def G_semicircle(z):     \"\"\"     Computes the Cauchy transform of the semicircle distribution for a given complex number z,     ensuring that if z has a positive imaginary part, the output has a negative imaginary part,     and vice versa.      Parameters:         z (complex or array-like): The point(s) at which to evaluate the Cauchy transform.      Returns:         complex or ndarray: The value(s) of the Cauchy transform at z.     \"\"\"     z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex      # Compute the discriminant     discriminant = np.sqrt(z**2 - 4)      # Ensure the output's imaginary part has the desired symmetry     discriminant = np.where(discriminant.imag * z.imag &lt; 0, -discriminant, discriminant)      # Compute the Cauchy transform     G = (z - discriminant) / 2      return G  # Example usage z = 3 + 4j  # Example input result = G_semicircle(z) print(f\"Cauchy transform at {z} is {result}\")  # Test with an array of values z_array = [3 + 4j, 1 + 1j, 3 - 4j, 1 - 1j] result_array = G_semicircle(z_array) print(f\"Cauchy transform for array {z_array} is {result_array}\")  plt.figure() z = np.linspace(-4,4) - 0.01j plt.plot(np.imag(G_semicircle(z))) z = np.linspace(-4,4) + 0.01j plt.plot(np.imag(G_semicircle(z)))   def G_matrix_semicircle(w, B, rank):   ''' computes G(w) for the semicirle B \\otimes x,   rank is the rank of matrix B '''   w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex   n = B.shape[0]   U1, d, U2t = la.svd(B)   U2 = np.conj(U2t.T)   #print(\"U1 =\", U1)   #print(d)   #print(\"U2 =\", U2)   #print(\"should be D: \", np.conj(U1.T) @ B @ U2) #   A_transf = np.conj(U1.T) @ w @ U2   #print(\"A_transf = \", A_transf)    A11 = A_transf[0:rank, 0:rank]   A12 = A_transf[0: rank, rank:n]   A21 = A_transf[rank:n, 0: rank]   A22 = A_transf[rank:n, rank:n]   D = np.diag(d[0:rank])   #print(\"D = \", D)   S = A11 - A12 @ la.inv(A22) @ A21   #print('S = ', S)   mu, V = la.eig(la.inv(D) @ S)   #print('mu =', mu)   #print(V)   #print('S = ', V @ np.diag(mu) @ la.inv(V))   #print(\"G(mu) = \", G_semicircle(mu))   M11 = V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(D)   #print('M11 = ', M11)   M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])   #print(\"M = \", M)   G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])        @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)   return(G)  def H_matrix_semicircle(w, B, rank):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(G_matrix_semicircle(w, B, rank)) - w)   n = 3 rank = 2 A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) z = (0.0 + 1j) w = z * np.eye(n)  G = G_matrix_semicircle(w, A1, rank) print(\"G_matrix_semicircle = \", G)  H = H_matrix_semicircle(w, A1, rank) print(\"H_matrix_semicircle = \", H)  def omega(b, AA, rank, max_iter = 10000):   ''' This computes subordination function for the sum of two semicircle variables.   AA = (A1, A2), rank is a (rank1, rank2), where rank1 is the rank of matrix A1,   and rank2 is the rank of matrix A2.   '''   W0 = 1.j * np.eye(n) #(initialization)   A1 = AA[0]   A2 = AA[1]   for i in range(max_iter):     W1 = H_matrix_semicircle(W0, A1, rank = rank[0]) + b     W2 = H_matrix_semicircle(W1, A2, rank = rank[1]) + b     if la.norm(W2 - W0) &lt; 1e-12:       break     W0 = W2     if i == max_iter - 1:       print(\"Warning: no convergence after \", max_iter, \"iterations\")   return W0  #an example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)  print('result = ', omega(B, AA, rank = (2, 2)))  #(10) Cauchy transform of free Poisson def G_free_poisson(z, lambda_param):     \"\"\"     Explicit formula for the Cauchy transform of the free Poisson distribution     with parameter \u03bb.      Args:         z (complex): The point at which to evaluate the Cauchy transform.         lambda_param (float): The parameter \u03bb of the free Poisson law.      Returns:         G (complex): The value of the Cauchy transform G(z).     \"\"\"      z = np.asarray(z, dtype=np.complex128)  # Ensure input is treated as complex     # Compute the interval [a, b] of the support     a = (1 - np.sqrt(lambda_param))**2     #print(a)     b = (1 + np.sqrt(lambda_param))**2     #print(b)      # Compute the square root term with correct branch     sqrt_term = np.sqrt((z - a) * (z - b))     #sqrt_term = np.sqrt((1 + z - lambda_param)**2 - 4 * z) #alternative expression      sqrt_term = np.where(sqrt_term.imag * z.imag &lt; 0, -sqrt_term, sqrt_term)      # Explicit formula for the Cauchy transform     G = (1 + z - lambda_param - sqrt_term) / (2 * z)     if lambda_param &lt; 1: #in this case G also has an atom at 0 with weight (1 - lambda)       G = G + (1 - lambda_param)/z      return G    lambda_param = 4  # Set \u03bb (parameter of the free Poisson law) z = 3 + 1j          # Complex number at which to evaluate G(z)  # Compute the Cauchy transform G_z = G_free_poisson(z, lambda_param) print(f\"Cauchy transform G({z}) for \u03bb={lambda_param}: {G_z}\")  #visualization  plt.figure() #z = np.linspace(-4,4) - 0.01j #plt.plot(np.imag(G_free_poisson(z, 2))) m = 100 al = (1 - np.sqrt(lambda_param))**2 au = (1 + np.sqrt(lambda_param))**2 x =  np.linspace(al,au, m) z = x - 0.01j plt.plot(x, np.imag(G_free_poisson(z, lambda_param))) z = x + 0.01j plt.plot(x, np.imag(G_free_poisson(z, lambda_param))) plt.grid()  #let's check that this corresponds to a valid density function. f = - np.imag(G_free_poisson(z, lambda_param))/np.pi print(sum(f)* (au - al)/m)   # (11) Matrix version of the Cauchy transform for the free Poisson random variable. def G_matrix_fpoisson(w, B, rank, lambda_param):   ''' computes G(w) for the free Poisson r.v. B \\otimes x,   rank is the rank of matrix B '''   w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex   n = B.shape[0]   U1, d, U2t = la.svd(B)   U2 = np.conj(U2t.T)   A_transf = np.conj(U1.T) @ w @ U2    A11 = A_transf[0:rank, 0:rank]   A12 = A_transf[0: rank, rank:n]   A21 = A_transf[rank:n, 0: rank]   A22 = A_transf[rank:n, rank:n]   D = np.diag(d[0:rank])   S = A11 - A12 @ la.inv(A22) @ A21   mu, V = la.eig(la.inv(D) @ S)   M11 = V @ np.diag(G_free_poisson(mu, lambda_param)) @ la.inv(V) @ la.inv(D)   M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])   G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])        @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)   return(G)  def H_matrix_fpoisson(w, B, rank, lambda_param):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(G_matrix_fpoisson(w, B, rank, lambda_param)) - w)   #(13) subordination function for the sum of two matrix random variables. def omega_sub(b, AA, rank, H1_name=\"H_matrix_semicircle\", H2_name=\"H_matrix_semicircle\",               H1_kwargs=None, H2_kwargs=None, max_iter=10000):     '''     Computes subordination function omega_1(b) for the sum of two free random variables variables.      AA = (A1, A2), where A1 and A2 are matrices.     rank = (rank1, rank2), where rank1 is the rank of matrix A1, and rank2 is the rank of matrix A2.     H1_name, H2_name are string names of the functions to be applied.     H1_kwargs, H2_kwargs are dictionaries containing additional arguments for H1 and H2.     '''     n = AA[0].shape[0]  # Assuming A1 and A2 are square matrices of the same size     W0 = 1.j * np.eye(n)  # Initialization     A1, A2 = AA      # Get function references from globals()     H1 = globals()[H1_name]     H2 = globals()[H2_name]      # Initialize kwargs dictionaries if None     if H1_kwargs is None:         H1_kwargs = {}     if H2_kwargs is None:         H2_kwargs = {}      for i in range(max_iter):         W1 = H1(W0, A1, rank=rank[0], **H1_kwargs) + b         W2 = H2(W1, A2, rank=rank[1], **H2_kwargs) + b          if la.norm(W2 - W0) &lt; 1e-12:             break         W0 = W2          if i == max_iter - 1:             print(\"Warning: no convergence after\", max_iter, \"iterations\")      return W0  #(14) generator of a free Poisson matrix def random_fpoisson(size, lam):   '''generate a random Hermitian matrix of size n-by-n, where n = size, that have the free Poisson   distribution with parameter lambda.   '''   random_matrix = np.random.randn(size, int(np.floor(size * lam)))   return (random_matrix @ random_matrix.T) /size  #Example of usage: size = 200 lam = 4 A = random_fpoisson(size, lam) print(la.norm(A - A.T)) e = la.eigvalsh(A) plt.figure() plt.plot(e) np.trace(A)/size  #(15) random orthogonal matrix def random_orthogonal(n):     # Step 1: Generate a random n x n matrix A     A = np.random.randn(n, n)      # Step 2: Perform QR decomposition on A     Q, R = np.linalg.qr(A)      # Q is the orthogonal matrix we want     return Q  # Example usage n = 3  # Dimension of the matrix Q = random_orthogonal(n) print(\"Random Orthogonal Matrix Q:\\n\", Q)  # (16) This is a function that calculates the matrix Cauchy transform, provided that #the scalar Cauchy transform is known. def G_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):   ''' computes G(w) for the r.v. B \\otimes x, where x has a custom measure mu_x above,   with the scalar Cauchy transform function $G_name$, and   rank is the rank of matrix B '''    # Get function references from globals()   G = globals()[G_name]    # Initialize kwargs dictionaries if None   if G_kwargs is None:     G_kwargs = {}     w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex   n = B.shape[0]   U1, d, U2t = la.svd(B)   U2 = np.conj(U2t.T)   A_transf = np.conj(U1.T) @ w @ U2    A11 = A_transf[0:rank, 0:rank]   A12 = A_transf[0: rank, rank:n]   A21 = A_transf[rank:n, 0: rank]   A22 = A_transf[rank:n, rank:n]   D = np.diag(d[0:rank])   S = A11 - A12 @ la.inv(A22) @ A21   mu, V = la.eig(la.inv(D) @ S)   M11 = V @ np.diag(G(mu, **G_kwargs)) @ la.inv(V) @ la.inv(D)   M = np.block([[M11, np.zeros((rank, n - rank))], [np.zeros((n - rank, rank)), la.inv(A22)]])   G = U2 @ (np.block([[np.eye(rank), np.zeros((rank, n - rank))], [-  la.inv(A22) @ A21 , np.eye(n - rank)]])        @ M  @ np.block([[np.eye(rank), -A12 @ la.inv(A22)], [np.zeros((n - rank, rank)), np.eye(n - rank)]]))  @ np.conj(U1.T)   return(G)  # (17) The H-function that corresponds to G_matrix_custom def H_matrix_custom(w, B, rank, G_name=\"G_semicircle\", G_kwargs=None):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(G_matrix_custom(w, B, rank, G_name, G_kwargs)) - w)  n = 3 rank = 2 A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) z = (0.0 + 1j) w = z * np.eye(n)  G = G_matrix_custom(w, A1, rank, G_name=\"G_semicircle\") print(\"G_matrix_custom = \", G)  H = H_matrix_custom(w, A1, rank,  G_name=\"G_semicircle\") print(\"H_matrix_custom = \", H)   #(18) The scalar Cauchy transform of an arbitrary discrete distribution def cauchy_transform_discrete(z, points, weights):     \"\"\"     Computes the Cauchy transform G_mu(z) for a measure defined by     discrete points and their corresponding weights.      Parameters:     z : complex or array-like         Evaluation point(s) in the complex plane.     points : list or array-like         Locations of the discrete measure.     weights : list or array-like         Corresponding weights of the measure.     \"\"\"     z = np.asarray(z)[:, np.newaxis]  # Ensure z is a column vector     points = np.asarray(points)     weights = np.asarray(weights)     return np.sum(weights / (z - points), axis=1)  # Example usage points = np.array([-2, -1, 1])  # Support points weights = np.array([2/4, 1/4, 1/4])  # Corresponding weights z_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane G_values = cauchy_transform_discrete(z_values, points, weights)  # Plot real and imaginary parts plt.figure(figsize=(8, 5)) plt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed') plt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\") plt.xlabel(\"Re(z)\") plt.ylabel(\"G_mu(z)\") plt.legend() plt.title(\"Cauchy Transform of Given Distribution\") plt.grid() plt.show() <p>Let us try an example (borrowed from Mai-Speicher) $$ b = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} $$ $$ a_1 = \\begin{bmatrix} 0 &amp; 2 i &amp; 0 \\\\ -2i &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; i \\\\ 0 &amp; 0 &amp; 0 \\\\ -i &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_3 = \\begin{bmatrix} 0 &amp; i &amp; 0 \\\\ -i &amp; 0 &amp; -i \\\\ 0 &amp; i &amp; 0 \\end{bmatrix} $$ The matrix $S$ is then $$ S = i \\begin{bmatrix} 0 &amp; 2s_1 + s_3 &amp; s_2 \\\\ -2s_1 - s_3 &amp; 0 &amp; -s_3 \\\\ -s_2 &amp; s_3 &amp; 0 \\end{bmatrix} $$</p> In\u00a0[\u00a0]: Copied! <pre>B = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]], dtype=np.float32)\nprint(B)\nA1 = 2.j * np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 0]])\nprint(A1)\nA2 = 1.j * np.array([[0, 0, 1], [0, 0, 0], [-1, 0, 0]])\nA3 = 1.j * np.array([[0, 1, 0], [-1, 0, -1], [0, 1, 0]])\nAA = (A1, A2, A3)\netaB = eta(B, AA)\netaB\n</pre> B = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]], dtype=np.float32) print(B) A1 = 2.j * np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 0]]) print(A1) A2 = 1.j * np.array([[0, 0, 1], [0, 0, 0], [-1, 0, 0]]) A3 = 1.j * np.array([[0, 1, 0], [-1, 0, -1], [0, 1, 0]]) AA = (A1, A2, A3) etaB = eta(B, AA) etaB <p>Let us try the itetative method for solving the basic equation and iterate it to convergence.</p> In\u00a0[\u00a0]: Copied! <pre>z = .01j\nn = 3\nG0 = .1/z * np.eye(n)\nG = hfs_map(G0, z, AA)\nprint(G)\n\nmax_iter = 150\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  G1 = hfs_map(G, z, AA)\n  diffs[i] = la.norm(G1 - G)\n  G = G1\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(G)\n</pre> z = .01j n = 3 G0 = .1/z * np.eye(n) G = hfs_map(G0, z, AA) print(G)  max_iter = 150 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   G1 = hfs_map(G, z, AA)   diffs[i] = la.norm(G1 - G)   G = G1 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(G) In\u00a0[\u00a0]: Copied! <pre>f = get_density(0., AA)\nprint(f)\n</pre> f = get_density(0., AA) print(f) <p>Let us get the plot of the density. Two interesting features: (1) the density is symmetric.</p> <p>(2) the density might have a singularity at zero. -- I am not actually sure about this.</p> In\u00a0[\u00a0]: Copied! <pre>a = 8\nm = 100\nXX = np.linspace(-a, a, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n  f[i] = get_density(x, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\n</pre> a = 8 m = 100 XX = np.linspace(-a, a, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):   f[i] = get_density(x, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f)  <p>Let us now see if this aggrees with numerical data from simulations. Recall that matrix $S$ is $$ S = i \\begin{bmatrix} 0 &amp; 2s_1 + s_3 &amp; s_2 \\\\ -2s_1 - s_3 &amp; 0 &amp; -s_3 \\\\ -s_2 &amp; s_3 &amp; 0 \\end{bmatrix} $$ We will build it using our basic blocks and calculate eigenvalues. We will repeat it T times and plot the histogram of the results.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nT = 10\nEE = np.zeros((3 * size,T))\nfor count in range(T):\n  S1 = random_semicircle(size)\n  S2 = random_semicircle(size)\n  S3 = random_semicircle(size)\n  S = 1.j * np.block([[zero_m, 2 * S1 + S3, S2], [-2 * S1 - S3, zero_m, -S3], [-S2, S3, zero_m]])\n  #la.norm(S - np.conj(S.T))\n  e = la.eigvalsh(S)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\n#plt.plot(EE)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=20, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) T = 10 EE = np.zeros((3 * size,T)) for count in range(T):   S1 = random_semicircle(size)   S2 = random_semicircle(size)   S3 = random_semicircle(size)   S = 1.j * np.block([[zero_m, 2 * S1 + S3, S2], [-2 * S1 - S3, zero_m, -S3], [-S2, S3, zero_m]])   #la.norm(S - np.conj(S.T))   e = la.eigvalsh(S)   EE[:,count] = e  EE = EE.reshape(-1)  #plt.plot(EE)  plt.figure() # Plot histogram with density plt.hist(EE, bins=20, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend()   <p>Here we will be interested in the matrix $$ S =  \\begin{bmatrix} A &amp; B &amp; C \\\\ B &amp; A &amp; B \\\\ C &amp; B &amp; A \\end{bmatrix}, $$ where $A$, $B$, and $C$ are free semicircular. In this case we have $$ a_1 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}, \\, a_3 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$</p> In\u00a0[\u00a0]: Copied! <pre>A1 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nA2 = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\nA3 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A1)\nprint(A2)\nprint(A3)\nAA = (A1, A2, A3)\na = 5\nXX = np.linspace(-a, a, 100)\nm = XX.size\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n  f[i] = get_density(x, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\n</pre> A1 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) A2 = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]) A3 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A1) print(A2) print(A3) AA = (A1, A2, A3) a = 5 XX = np.linspace(-a, a, 100) m = XX.size f = np.zeros(XX.shape) for i, x in enumerate(XX):   f[i] = get_density(x, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) <p>Now we will check this result using numerical simulations</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nT = 30\nEE = np.zeros((3 * size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  C = random_semicircle(size)\n  S = np.block([[A, B, C], [B, A, B], [C, B, A]])\n  #la.norm(S - np.conj(S.T))\n  e = la.eigvalsh(S)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\n#plt.plot(EE)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) T = 30 EE = np.zeros((3 * size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   C = random_semicircle(size)   S = np.block([[A, B, C], [B, A, B], [C, B, A]])   #la.norm(S - np.conj(S.T))   e = la.eigvalsh(S)   EE[:,count] = e  EE = EE.reshape(-1)  #plt.plot(EE)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend()  <p>Here we will be interested in the matrix $$ S =  \\begin{bmatrix} 0 &amp; X_1 &amp; X_2 \\\\ X_1 &amp; 0 &amp; -1 \\\\ X_2 &amp; -1 &amp; 0 \\end{bmatrix}, $$ where $X_1$ and $X_2$ are two standard semicircular variables. In this case we have $$ a_0 = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}, \\, a_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$</p> <p>This example is significantly different from the previous example because the matrix semicirle variable has non-trivial expectation.</p> <p>First, we need to define an analogue of the hfs map, in order to be able to calculate the Cauchy transform $G_S(z) = G_{a_0 + X}$, where $X = a_1 \\otimes X_1 + a_2 \\otimes X_2$. For this we define $b = z(z I - a_0)^{-1}$ and define the map: $$ G \\mapsto \\frac{1}{2}\\Big[G + [z I - b \\eta(G)]^{-1} b\\Big]. $$ This should be iterated to convergence.</p> <p>Next we define the function that calculates the density. This function is similar defined for the matrix semicircle without the bias term but it uses hsfb map instead of hsf map.</p> <p>Let us plot density. While the result looks somewhat doubtful, however it is supported by numerical simulations.</p> In\u00a0[\u00a0]: Copied! <pre>#usage example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\n\n'''\n#checking convergence\nz = 1 + .01j\nn = 3\nG0 = 1/z * np.eye(n)\nG = hfsb_map(G0, z, A0, AA)\nprint(G)\nmax_iter = 1500\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  G1 = hfsb_map(G, z, A0, AA)\n  diffs[i] = la.norm(G1 - G)\n  G = G1\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(G)\n'''\nf = get_density_B(1., A0, AA)\nprint(f)\na = 4\nm = 100\nXX = np.linspace(-a, a, 200)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_B(x, A0, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> #usage example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2)  ''' #checking convergence z = 1 + .01j n = 3 G0 = 1/z * np.eye(n) G = hfsb_map(G0, z, A0, AA) print(G) max_iter = 1500 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   G1 = hfsb_map(G, z, A0, AA)   diffs[i] = la.norm(G1 - G)   G = G1 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(G) ''' f = get_density_B(1., A0, AA) print(f) a = 4 m = 100 XX = np.linspace(-a, a, 200) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_B(x, A0, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check.</p> <p>It appears that the results of numeric simulations are in good agreement with my theoretical results.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((3 * size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  C = random_semicircle(size)\n  S = np.block([[zero_m, B, C], [B, zero_m, -ones_m], [C, -ones_m, zero_m]])\n  #print(la.norm(S - np.conj(S.T)))\n  e = la.eigvalsh(S)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\n#plt.plot(EE)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((3 * size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   C = random_semicircle(size)   S = np.block([[zero_m, B, C], [B, zero_m, -ones_m], [C, -ones_m, zero_m]])   #print(la.norm(S - np.conj(S.T)))   e = la.eigvalsh(S)   EE[:,count] = e  EE = EE.reshape(-1)  #plt.plot(EE)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>In order to find this distribution, note that we need to calculate $G(z, b(z))$, where $$ b_\\epsilon(z) = z(\\Lambda_\\epsilon(z) - a_0)^{-1}, $$ $\\epsilon &gt; 0$ is a small regularization paremater, and $$ \\Lambda_\\epsilon(z) := \\begin{bmatrix}  z &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; i\\epsilon &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\, &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; i\\epsilon \\end{bmatrix}. $$ By definition, $$ G(z, b) = m_0(b)z^{-1} + m_1(b) z^{-2} + m_2(b) z^{-3} + \\ldots, $$ and $m_k(b)$ are moments: $$ m_k(b) := E[b(Xb)^k] = E[bXb \\ldots Xb]. $$</p> <p>Once $G(z, b(z))$ is calculated, we can calculate the Cauchy transform of the polynomial $p$ by using formula $$ \\phi[(z - p)^{-1}] = \\lim_{\\epsilon \\to 0} \\Big[ G(z, b_\\epsilon(z)) \\Big]_{1,1}. $$</p> <p>Finally, we can extract the density by the Stieljes inversion formula.</p> <p>See the realization of the functions hsfc_map and get_density_C above.</p> In\u00a0[\u00a0]: Copied! <pre>#usage example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nz = .5 + .01j\nn = 3\nG0 = 1/z * np.eye(n)\nG = hfsc_map(G0, z, A0, AA)\nprint(G)\n\nmax_iter = 200\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  G1 = hfsc_map(G, z, A0, AA)\n  diffs[i] = la.norm(G1 - G)\n  G = G1\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(G)\n</pre> #usage example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) z = .5 + .01j n = 3 G0 = 1/z * np.eye(n) G = hfsc_map(G0, z, A0, AA) print(G)  max_iter = 200 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   G1 = hfsc_map(G, z, A0, AA)   diffs[i] = la.norm(G1 - G)   G = G1 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(G) In\u00a0[\u00a0]: Copied! <pre>f = get_density_C(1., A0, AA)\nprint(f)\na = 4\nm = 200\nXX = np.linspace(-a, a, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_C(x, A0, AA)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n\nget_density_C(.5, A0, AA)\n</pre> f = get_density_C(1., A0, AA) print(f) a = 4 m = 200 XX = np.linspace(-a, a, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_C(x, A0, AA)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True)  get_density_C(.5, A0, AA) <p>And here is the numerical check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we replicate the results of the previous example by using the method of subordination functions from Belinschi-Mai-Speicher paper.</p> <p>The setup is the same. $$ S =  \\begin{bmatrix} 0 &amp; X_1 &amp; X_2 \\\\ X_1 &amp; 0 &amp; -1 \\\\ X_2 &amp; -1 &amp; 0 \\end{bmatrix}, $$ where $X_1$ and $X_2$ are two standard semicircular variables. In this case we have $$ a_0 = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}, \\, a_1 = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\, a_2 = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}. $$ However, we aim to calculate the matrix Cauchy transform differently.</p> <p>The subordination method is based on finding the fixed point of the map $$ w \\to h_y(h_x(w) + b) + b. $$ where $$h_x(w) = F_x(w) - w = \\Big[G_x(w)\\Big]^{-1} - w $$ and $h_y(w) = F_y(w) - w$.  ($x = a_1 \\otimes X_1$ and $y = a_2 \\otimes X_2$). This fixed point is the subordination function $\\omega_1(b)$ and the Cauchy transform of $x + y$ is given by $G_{x + y}(b) = G_x(\\omega_1(b))$. We want to a apply it to $b = \\Lambda_\\epsilon(z) - a_0$.</p> <p>The first thing to do is to calculate $G_x(w)$.</p> <p>In the basic approach we use the ingegral formula $$ E\\Big[ (w - b \\otimes x)^{-1}\\Big] = \\lim_{\\epsilon \\to 0}\\frac{-1}{\\pi} \\int_R (w - t b)^{-1} \\Im (G_x(t + i\\epsilon))\\, dt. $$</p> In\u00a0[\u00a0]: Copied! <pre>A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nA_test = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nprint(A_test)\nAA = (A1, A2)\nn = A0.shape[0]\nw = (0.5 + 0.1j) * np.eye(n)\nz = 0.01j\nprint(la.inv(w - z * A_test))\n\n# Define the matrix-valued function\ndef matrix_function(z, w, b):\n    \"\"\"Matrix-valued function of a complex variable z.\"\"\"\n    return -(1/np.pi) * la.inv(w - z * b) * np.imag(G_semicircle(z))\n\nprint(\"test:\", matrix_function(z, w, A_test))\n\n# Define the real path and shift it slightly above the real line\nepsilon = 1e-3  # Small imaginary shift\ndef shifted_path(x):\n    return x + 1j * epsilon\n\n# Integration limits\nal, au = -4, 4  # Example real integration limits\n\n# Perform the integration for each matrix entry\ndef cauchy_matrix_semicircle_0(w, b):\n    w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex\n    matrix_size = matrix_function(0, w, b).shape  # Get the shape of the matrix\n    result = np.zeros(matrix_size, dtype=complex)  # Initialize result matrix\n    for i in range(matrix_size[0]):\n        for j in range(matrix_size[1]):\n            # Define the scalar function for the (i, j)-th entry\n            def scalar_function(x, w, b):\n                z = shifted_path(x)\n                return matrix_function(z, w, b)[i, j]\n\n            scalar_func_with_params = partial(scalar_function, w = w, b = b)\n            # Perform the numerical integration\n            integral_real, _ = quad(lambda x: scalar_func_with_params(x).real, al, au)\n            integral_imag, _ = quad(lambda x: scalar_func_with_params(x).imag, al, au)\n            result[i, j] = integral_real + 1j * integral_imag  # Save the result\n    return result\n\ndef H_matrix_semicircle_0(w, A1, eps = 1E-8):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(cauchy_matrix_semicircle_0(w, A1)) - w)\n\n\n# Compute the cauchy transform via integral\nresult_matrix = cauchy_matrix_semicircle_0(w, A1)\nprint(\"Resultant matrix G after integration:\")\nprint(result_matrix)\n\n# Compute the h-function\nresult_matrix = H_matrix_semicircle_0(w, A1)\nprint(\"Resultant matrix H after integration:\")\nprint(result_matrix)\n\n\n\n#Let us visualize the result:\n\nm = 10\nx = np.linspace(-2, 2, m)\nGG = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  GG[i] = result[0, 0]\nplt.plot(np.imag(GG))\nprint(GG)\n</pre> A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) A_test = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]]) print(A0) print(A1) print(A2) print(A_test) AA = (A1, A2) n = A0.shape[0] w = (0.5 + 0.1j) * np.eye(n) z = 0.01j print(la.inv(w - z * A_test))  # Define the matrix-valued function def matrix_function(z, w, b):     \"\"\"Matrix-valued function of a complex variable z.\"\"\"     return -(1/np.pi) * la.inv(w - z * b) * np.imag(G_semicircle(z))  print(\"test:\", matrix_function(z, w, A_test))  # Define the real path and shift it slightly above the real line epsilon = 1e-3  # Small imaginary shift def shifted_path(x):     return x + 1j * epsilon  # Integration limits al, au = -4, 4  # Example real integration limits  # Perform the integration for each matrix entry def cauchy_matrix_semicircle_0(w, b):     w = np.asarray(w, dtype=np.complex128)  # Ensure input is treated as complex     matrix_size = matrix_function(0, w, b).shape  # Get the shape of the matrix     result = np.zeros(matrix_size, dtype=complex)  # Initialize result matrix     for i in range(matrix_size[0]):         for j in range(matrix_size[1]):             # Define the scalar function for the (i, j)-th entry             def scalar_function(x, w, b):                 z = shifted_path(x)                 return matrix_function(z, w, b)[i, j]              scalar_func_with_params = partial(scalar_function, w = w, b = b)             # Perform the numerical integration             integral_real, _ = quad(lambda x: scalar_func_with_params(x).real, al, au)             integral_imag, _ = quad(lambda x: scalar_func_with_params(x).imag, al, au)             result[i, j] = integral_real + 1j * integral_imag  # Save the result     return result  def H_matrix_semicircle_0(w, A1, eps = 1E-8):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(cauchy_matrix_semicircle_0(w, A1)) - w)   # Compute the cauchy transform via integral result_matrix = cauchy_matrix_semicircle_0(w, A1) print(\"Resultant matrix G after integration:\") print(result_matrix)  # Compute the h-function result_matrix = H_matrix_semicircle_0(w, A1) print(\"Resultant matrix H after integration:\") print(result_matrix)    #Let us visualize the result:  m = 10 x = np.linspace(-2, 2, m) GG = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   GG[i] = result[0, 0] plt.plot(np.imag(GG)) print(GG)  <p>Now we want to run the map $$ w \\to h_y(h_x(w) + b) + b. $$ to convergence. Again, we are interested in $b = \\Lambda_\\epsilon(z) - a_0$. This is supposed to give a subordination function $\\omega_1(b)$.</p> <p>Here we do have convergence alghough it is not particularly fast.</p> <p>Run the subordination functional mapping $$ w \\to h_y(h_x(w) + b) + b. $$ iteratively until convergence. Around 30 iterations is needed.</p> In\u00a0[\u00a0]: Copied! <pre>z = .5 + .01j\nn = 3\nB = Lambda(z, n) - A0\nprint(B)\n\nW0 = 1.j * np.eye(n) #(initialization)\nprint(W0)\n\nW1 = H_matrix_semicircle_0(W0, A1) + B\nW2 = H_matrix_semicircle_0(W1, A2) + B\nprint(W2)\n\nmax_iter = 30\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  W1 = H_matrix_semicircle_0(W0, A1) + B\n  #print(\"W1 = \", W1)\n  W2 = H_matrix_semicircle_0(W1, A2) + B\n  #print(\"W2 = \", W2)\n  diffs[i] = la.norm(W2 - W0)\n  W0 = W2\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(W0)\n</pre> z = .5 + .01j n = 3 B = Lambda(z, n) - A0 print(B)  W0 = 1.j * np.eye(n) #(initialization) print(W0)  W1 = H_matrix_semicircle_0(W0, A1) + B W2 = H_matrix_semicircle_0(W1, A2) + B print(W2)  max_iter = 30 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   W1 = H_matrix_semicircle_0(W0, A1) + B   #print(\"W1 = \", W1)   W2 = H_matrix_semicircle_0(W1, A2) + B   #print(\"W2 = \", W2)   diffs[i] = la.norm(W2 - W0)   W0 = W2 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(W0) <p>Then we can compute $G_{x + y}(b)$ as $G_x(\\omega_1(b))$. Then we take $G_{11}$ and apply Stieltjes inversion formula. Theh number is more or less in agreement with what we seen in the method that avoided using subordination.</p> In\u00a0[\u00a0]: Copied! <pre>-cauchy_matrix_semicircle_0(W0, A1)[0,0].imag/np.pi\n</pre> -cauchy_matrix_semicircle_0(W0, A1)[0,0].imag/np.pi <p>We look for $E(w - a_1 \\otimes x_1)^{-1}$. We will regularize $a_1$ so that it is invertible. Then $$  a_1^{-1} w = V \\begin{bmatrix}  \\mu_1 &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; \\mu_r  \\end{bmatrix} V^{-1}, $$ and then \\begin{equation} %\\label{equ_matrix_G2}  E \\big[(w - a_1\\otimes x_1)^{-1}\\big] =  E \\big[(a_1^{-1} w - I \\otimes x_1)^{-1}\\big] a_1^{-1} = V \\begin{bmatrix}G_x(\\mu_1) &amp; &amp;  \\\\  &amp;\\ddots &amp; \\\\ &amp; &amp; G_x(\\mu_r)\\end{bmatrix} V^{-1} a_1^{-1}.  \\end{equation}</p> In\u00a0[\u00a0]: Copied! <pre>def cauchy_matrix_semicircle_1(w, A1, eps = 1E-8):\n  ''' This is function that computes the Cauchy transform of $A1 \\otimes X$, where\n  X is the standard semicirlce and A1 is an $n\\times n$ matrix. The argument is w,\n  so we calculate E(w - A1 \\otimes X)^{-1}. The parameter eps is for regularization to handle\n  the case when A1 is not inverible.'''\n  n = A1.shape[0]\n  mu, V = la.eig(la.inv(A1 + 1.j * eps * np.eye(n)) @ w)\n  return V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(A1 + 1.j * eps * np.eye(n))\n\ndef H_matrix_semicircle_1(w, A1, eps = 1E-8):\n  ''' This is the h function: h = G(w)^{-1} - w$ '''\n  return(la.inv(Cauchy_matrix_semicircle_1(w, A1, eps)) - w)\n\n\nn = 2\nA1 = np.eye(n)\nA1 = np.array([[0, 1], [1, 0]])\n\nz = (0.0 + 0.01j)\nw = z * np.eye(n)\n\n#np.set_printoptions(precision=2)\narr = Cauchy_matrix_semicircle_1(w, A1)\nprint(arr)\nH_matrix_semicircle_1(w, A1)\n</pre> def cauchy_matrix_semicircle_1(w, A1, eps = 1E-8):   ''' This is function that computes the Cauchy transform of $A1 \\otimes X$, where   X is the standard semicirlce and A1 is an $n\\times n$ matrix. The argument is w,   so we calculate E(w - A1 \\otimes X)^{-1}. The parameter eps is for regularization to handle   the case when A1 is not inverible.'''   n = A1.shape[0]   mu, V = la.eig(la.inv(A1 + 1.j * eps * np.eye(n)) @ w)   return V @ np.diag(G_semicircle(mu)) @ la.inv(V) @ la.inv(A1 + 1.j * eps * np.eye(n))  def H_matrix_semicircle_1(w, A1, eps = 1E-8):   ''' This is the h function: h = G(w)^{-1} - w$ '''   return(la.inv(Cauchy_matrix_semicircle_1(w, A1, eps)) - w)   n = 2 A1 = np.eye(n) A1 = np.array([[0, 1], [1, 0]])  z = (0.0 + 0.01j) w = z * np.eye(n)  #np.set_printoptions(precision=2) arr = Cauchy_matrix_semicircle_1(w, A1) print(arr) H_matrix_semicircle_1(w, A1)   <p>Let us do some checking.</p> In\u00a0[\u00a0]: Copied! <pre>n = 2\nrank = 2\nA1 = np.eye(n)\nA1 = np.array([[0, 1], [1, 0]])\n\n#A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nn = 3\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n\n\n\n\n\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = cauchy_matrix_semicircle_0(w, A1)\nprint(G)\nprint('old appoach = \\n', cauchy_matrix_semicircle_0(w, A1))\nprint(G_semicircle(z))\n\n#H = H_matrix_semicircle_0(w, A1, rank)\n#print(H)\n\n#Let us visualize the result:\nm = 10\nx = np.linspace(-2, 2, m)\nGG = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  GG[i] = result[0, 0]\nplt.plot(np.imag(GG))\nprint(GG)\n\nm = 10\nx = np.linspace(-2, 2, m)\nGG_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_1((x[i]+ 0.1j) * np.eye(n), A1)\n  GG_new[i] = result[0, 0]\nplt.plot(np.imag(GG_new) + 0.01)\nprint(GG_new)\n</pre> n = 2 rank = 2 A1 = np.eye(n) A1 = np.array([[0, 1], [1, 0]])  #A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) n = 3 A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])      z = (0.0 + 1j) w = z * np.eye(n)  G = cauchy_matrix_semicircle_0(w, A1) print(G) print('old appoach = \\n', cauchy_matrix_semicircle_0(w, A1)) print(G_semicircle(z))  #H = H_matrix_semicircle_0(w, A1, rank) #print(H)  #Let us visualize the result: m = 10 x = np.linspace(-2, 2, m) GG = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   GG[i] = result[0, 0] plt.plot(np.imag(GG)) print(GG)  m = 10 x = np.linspace(-2, 2, m) GG_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_1((x[i]+ 0.1j) * np.eye(n), A1)   GG_new[i] = result[0, 0] plt.plot(np.imag(GG_new) + 0.01) print(GG_new) <p>This was explained above, in the beginning of the notebook, where we listed the most basic functions.</p> <p>The code below seems to indicate that $G(w)$ is the same as calculated by the brute-force method.</p> In\u00a0[\u00a0]: Copied! <pre>A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n\n\n\n#Let us visualize the result:\nm = 10\nx = np.linspace(-2, 2, m)\nGG = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  GG[i] = result[0, 0]\nplt.plot(np.imag(GG))\nprint(GG)\n\nm = 10\nx = np.linspace(-2, 2, m)\nGG_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = G_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)\n  GG_new[i] = result[0, 0]\nplt.plot(np.imag(GG_new) + 0.01)\nprint(GG_new)\n\nplt.figure()\n\nm = 10\nx = np.linspace(-2, 2, m)\nHH = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = H_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)\n  HH[i] = result[0, 0]\nplt.plot(np.imag(HH))\nprint(HH)\n\nm = 10\nx = np.linspace(-2, 2, m)\nHH_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = H_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)\n  HH_new[i] = result[0, 0]\nplt.plot(np.imag(HH_new) + 0.01)\nprint(HH_new)\n</pre>  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])    #Let us visualize the result: m = 10 x = np.linspace(-2, 2, m) GG = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = cauchy_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   GG[i] = result[0, 0] plt.plot(np.imag(GG)) print(GG)  m = 10 x = np.linspace(-2, 2, m) GG_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = G_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)   GG_new[i] = result[0, 0] plt.plot(np.imag(GG_new) + 0.01) print(GG_new)  plt.figure()  m = 10 x = np.linspace(-2, 2, m) HH = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = H_matrix_semicircle_0((x[i]+ 0.1j) * np.eye(n), A1)   HH[i] = result[0, 0] plt.plot(np.imag(HH)) print(HH)  m = 10 x = np.linspace(-2, 2, m) HH_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = H_matrix_semicircle((x[i]+ 0.1j) * np.eye(n), A1, rank = 2)   HH_new[i] = result[0, 0] plt.plot(np.imag(HH_new) + 0.01) print(HH_new)   <p>Let us get the subordination function. Recall that $\\omega_1(b)$ is the fixed point of the map $$ w \\to h_y(h_x(w) + b) + b. $$ where $h_x(w) = F_x(w) - w$ and $h_y(w) = F_y(w) - w$, and $F_x, F_y$ are inverses of the corresponding Cauchy transforms.</p> <p>Given that $\\omega_1(b)$ is calculated, we can find the Cauchy transform for the sum $x + y$ as $$ G_{x + y}(b) = G_x(\\omega_1(b)). $$</p> <p>We realize $\\omega_1(b)$ as a function $\\omega\\big(b, (x, y)\\big)$.</p> <p>First, let us check convergence of the method here.</p> In\u00a0[\u00a0]: Copied! <pre>def Lambda(z, size, eps = 1E-6):\n  A = eps * 1.j * np.eye(size)\n  A[0, 0] = z\n  return A\n\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\n\nW0 = 1.j * np.eye(n) #(initialization)\n  #print(W0)\nA1 = AA[0]\nA2 = AA[1]\n  #W1 = H_matrix_semicircle(W0, A1, rank = 2) + B\n  #W2 = H_matrix_semicircle(W1, A2, rank = 2) + B\n  #print(W2)\nmax_iter = 40\ndiffs = np.zeros((max_iter, 1))\nfor i in trange(max_iter):\n  W1 = H_matrix_semicircle(W0, A1, rank = 2) + B\n  #print(\"W1 = \", W1)\n  W2 = H_matrix_semicircle(W1, A2, rank = 2) + B\n  #print(\"W2 = \", W2)\n  diffs[i] = la.norm(W2 - W0)\n  W0 = W2\nplt.plot(diffs)\nplt.yscale(\"log\")\nplt.title(\"Convergence of the method\")\nprint(W0)\n</pre> def Lambda(z, size, eps = 1E-6):   A = eps * 1.j * np.eye(size)   A[0, 0] = z   return A  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)   W0 = 1.j * np.eye(n) #(initialization)   #print(W0) A1 = AA[0] A2 = AA[1]   #W1 = H_matrix_semicircle(W0, A1, rank = 2) + B   #W2 = H_matrix_semicircle(W1, A2, rank = 2) + B   #print(W2) max_iter = 40 diffs = np.zeros((max_iter, 1)) for i in trange(max_iter):   W1 = H_matrix_semicircle(W0, A1, rank = 2) + B   #print(\"W1 = \", W1)   W2 = H_matrix_semicircle(W1, A2, rank = 2) + B   #print(\"W2 = \", W2)   diffs[i] = la.norm(W2 - W0)   W0 = W2 plt.plot(diffs) plt.yscale(\"log\") plt.title(\"Convergence of the method\") print(W0) <p>Now we define the subordination function $\\omega_1(b) = \\omega\\big(b, (a_1, a_2)\\big)$.</p> In\u00a0[\u00a0]: Copied! <pre>#See the definition of the function in the beginning of the notebook\n\n#let us do some visualization\nm = 50\ntt = np.linspace(-2, 2, m)\nom = np.zeros((m, 1), dtype = np.complex128)\nfor i in range(m):\n  om[i] = omega(Lambda(tt[i] + 0.01j, n) - A0, (AA), rank = (2,2))[0,0]\n#print(om)\nplt.plot(np.imag(om))\nplt.plot(np.real(om))\n</pre> #See the definition of the function in the beginning of the notebook  #let us do some visualization m = 50 tt = np.linspace(-2, 2, m) om = np.zeros((m, 1), dtype = np.complex128) for i in range(m):   om[i] = omega(Lambda(tt[i] + 0.01j, n) - A0, (AA), rank = (2,2))[0,0] #print(om) plt.plot(np.imag(om)) plt.plot(np.real(om)) <p>Now our goal is to write the function that would compute density of the anticommutator at a given point.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator(x)\nprint(f)\n\n#visualization\nm = 50\na = 4\nXX = np.linspace(-a, a, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator(x)\n\nprint(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator(x) print(f)  #visualization m = 50 a = 4 XX = np.linspace(-a, a, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator(x)  print(sum(f)*2*a/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>And the numerical check.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>In this example, $X$ and $Y$ are free Poisson with parameters $\\lambda_1$ and $\\lambda_2$. We want to calculate their anticommutator. Here we are forced to use the subordination method.</p> <p>We havd defined the matrix version of the Cauchy transform for the free Poisson random variable $b \\otimes x$ (see above). Here we do some visualization.</p> In\u00a0[\u00a0]: Copied! <pre>n = 3\nlambda_param = 4\nA1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nrank = 2\nz = (0.0 + 1j)\nw = z * np.eye(n)\n\nG = G_matrix_fpoisson(w, A1, rank, lambda_param)\nprint(\"G_matrix_fpoisson = \", G)\n\n\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n#A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nm = 100\nx = np.linspace(-2, 2, m)\nGG_new = np.zeros(m, dtype=np.complex128)\nfor i in trange(m):\n  result = H_matrix_fpoisson((x[i]+ 0.1j) * np.eye(n), A1, rank = 2, lambda_param = 0.5)\n  GG_new[i] = result[0, 0]\nplt.plot(np.imag(GG_new) + 0.01)\nprint(GG_new)\n</pre> n = 3 lambda_param = 4 A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) rank = 2 z = (0.0 + 1j) w = z * np.eye(n)  G = G_matrix_fpoisson(w, A1, rank, lambda_param) print(\"G_matrix_fpoisson = \", G)   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) #A1 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) m = 100 x = np.linspace(-2, 2, m) GG_new = np.zeros(m, dtype=np.complex128) for i in trange(m):   result = H_matrix_fpoisson((x[i]+ 0.1j) * np.eye(n), A1, rank = 2, lambda_param = 0.5)   GG_new[i] = result[0, 0] plt.plot(np.imag(GG_new) + 0.01) print(GG_new) <p>Now we want to use the the general purpose subordination function omega_sub.</p> In\u00a0[\u00a0]: Copied! <pre>#an example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\nresult = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={\"lambda_param\":4},\n                   H2_kwargs={\"lambda_param\":4})\n</pre> #an example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)  result = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={\"lambda_param\":4},                    H2_kwargs={\"lambda_param\":4}) In\u00a0[\u00a0]: Copied! <pre>#let us do some visualization\nm = 50\ntt = np.linspace(1, 9, m)\nom = np.zeros((m, 1), dtype = np.complex128)\nfor i in range(m):\n  B = Lambda(tt[i] + 0.01j, n) - A0\n  om[i] = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={\"lambda_param\":4},\n                   H2_kwargs={\"lambda_param\":4})[0,0]\n#print(om)\nplt.plot(tt, np.imag(om))\nplt.plot(tt, np.real(om))\n</pre> #let us do some visualization m = 50 tt = np.linspace(1, 9, m) om = np.zeros((m, 1), dtype = np.complex128) for i in range(m):   B = Lambda(tt[i] + 0.01j, n) - A0   om[i] = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={\"lambda_param\":4},                    H2_kwargs={\"lambda_param\":4})[0,0] #print(om) plt.plot(tt, np.imag(om)) plt.plot(tt, np.real(om)) <p>Now let us try to calculate the density of the anticommutator.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_fpoisson(x, lambda_param, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={\"lambda_param\":lambda_param},\n                   H2_kwargs={\"lambda_param\":lambda_param})\n  Gxy = G_matrix_fpoisson(om, A1, rank = 2, lambda_param = lambda_param)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = 4\nf = get_density_anticommutator_fpoisson(x, lambda_param = 4)\nprint(f)\n\n#visualization\n\nm = 50\nal = 2\nau = 105\nlambda_param = 4\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_fpoisson(x,lambda_param)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n\n\nexpect = sum(XX * f)*(au - al)/m\nprint(\"expectation = \", expect)\n\n#g = - G_free_poisson(XX + .01j, 100).imag/(np.pi)\n#plt.plot(XX, g)\n</pre> def get_density_anticommutator_fpoisson(x, lambda_param, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_fpoisson\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={\"lambda_param\":lambda_param},                    H2_kwargs={\"lambda_param\":lambda_param})   Gxy = G_matrix_fpoisson(om, A1, rank = 2, lambda_param = lambda_param)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = 4 f = get_density_anticommutator_fpoisson(x, lambda_param = 4) print(f)  #visualization  m = 50 al = 2 au = 105 lambda_param = 4 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_fpoisson(x,lambda_param)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True)   expect = sum(XX * f)*(au - al)/m print(\"expectation = \", expect)  #g = - G_free_poisson(XX + .01j, 100).imag/(np.pi) #plt.plot(XX, g)  <p>Numerical check:</p> <p>Finding the distribution of the anti-commutator of the free Poisson r.v.s by using large random matrices.</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nlam = 4\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_fpoisson(size, lam)\n  B = random_fpoisson(size, lam)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 lam = 4 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_fpoisson(size, lam)   B = random_fpoisson(size, lam)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Let us use the subordination function for this pair.</p> In\u00a0[\u00a0]: Copied! <pre>#an example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nn = A0.shape[0]\n\nz = .5 + .01j\nB = Lambda(z, n) - A0\nprint(B)\n\nresult = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={},\n                   H2_kwargs={\"lambda_param\":4})\n</pre> #an example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) n = A0.shape[0]  z = .5 + .01j B = Lambda(z, n) - A0 print(B)  result = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={},                    H2_kwargs={\"lambda_param\":4}) In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_S_FP(x, lambda_param, eps = 0.01):\n  '''calculates the density of the anticommutator of a semicircle and\n  a free poisson r.v. with parameter lambda_param'''\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={},\n                   H2_kwargs={\"lambda_param\":lambda_param})\n  Gxy = G_matrix_semicircle(om, A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = 4\nf = get_density_anticommutator_fpoisson(x, lambda_param = 4)\nprint(f)\n\n#visualization\n\nm = 50\nal = -20\nau = 20\nlambda_param = 4\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_S_FP(x,lambda_param)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_S_FP(x, lambda_param, eps = 0.01):   '''calculates the density of the anticommutator of a semicircle and   a free poisson r.v. with parameter lambda_param'''   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={},                    H2_kwargs={\"lambda_param\":lambda_param})   Gxy = G_matrix_semicircle(om, A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = 4 f = get_density_anticommutator_fpoisson(x, lambda_param = 4) print(f)  #visualization  m = 50 al = -20 au = 20 lambda_param = 4 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_S_FP(x,lambda_param)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nlam = 4\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_fpoisson(size, lam)\n  e = la.eigvalsh(A @ B + B @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 lam = 4 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_fpoisson(size, lam)   e = la.eigvalsh(A @ B + B @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we consider the deformed anticommutator $$ p(X, Y) = X Y + Y X + X^2 $$ (This is an example 5.2 from Belinschi-Mai-Speicher 2013).</p> <p>This polynomial has a nice linearization: $$ L = \\begin{bmatrix} 0 &amp; X &amp; \\frac{1}{2}X + Y \\\\ X &amp; 0 &amp; -1 \\\\ \\frac{1}{2}X + Y &amp; - 1 &amp; 0 \\end{bmatrix} =  \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; - 1 &amp; 0 \\end{bmatrix} + \\begin{bmatrix} 0 &amp; 1 &amp; \\frac{1}{2} \\\\ 1 &amp; 0 &amp; 0 \\\\ \\frac{1}{2} &amp; 0 &amp; 0 \\end{bmatrix} X + \\begin{bmatrix} 0 &amp; 0 &amp;  1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix} Y. $$</p> In\u00a0[\u00a0]: Copied! <pre>#usage example\nA0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\nA1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\nA2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\nprint(A0)\nprint(A1)\nprint(A2)\nAA = (A1, A2)\nf = get_density_C(1., A0, AA)\nprint(f)\nal = -3\nau = 8\nm = 200\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_C(x, A0, AA)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> #usage example A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]]) A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]]) A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]]) print(A0) print(A1) print(A2) AA = (A1, A2) f = get_density_C(1., A0, AA) print(f) al = -3 au = 8 m = 200 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_C(x, A0, AA)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Nymerical check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Now we repeat the calculation using the subordination method.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator_deform(x)\nprint(f)\n\n#visualization\nm = 50\nal = -3\nau = 7\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform(x)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   Gxy = G_matrix_semicircle(omega(B, AA, rank = (2, 2)), A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator_deform(x) print(f)  #visualization m = 50 al = -3 au = 7 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform(x)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we will use $X$ -- semicircle and $Y$ -- free Poisson.</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform_SFP(x, lambda_param, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",\n                   H2_name = \"H_matrix_fpoisson\",\n                   H1_kwargs={},\n                   H2_kwargs={\"lambda_param\":lambda_param})\n  Gxy = G_matrix_semicircle(om, A1, rank = 2)\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nlam = 4\nf = get_density_anticommutator_deform_SFP(x, lambda_param = lam)\nprint(f)\n\n#visualization\nm = 100\nal = -18\nau = 25\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform_SFP(x, lam)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform_SFP(x, lambda_param, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_semicircle\",                    H2_name = \"H_matrix_fpoisson\",                    H1_kwargs={},                    H2_kwargs={\"lambda_param\":lambda_param})   Gxy = G_matrix_semicircle(om, A1, rank = 2)   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 lam = 4 f = get_density_anticommutator_deform_SFP(x, lambda_param = lam) print(f)  #visualization m = 100 al = -18 au = 25 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform_SFP(x, lam)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nlam = 4\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  A = random_semicircle(size)\n  B = random_fpoisson(size, lam)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 lam = 4 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   A = random_semicircle(size)   B = random_fpoisson(size, lam)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here we considere the deformed anticommutator $$ p(X, Y) = X Y + Y X + X^2, $$ when $X$ has the distribution $\\mu_X = \\frac{1}{4}(2 \\delta_{-2} + \\delta_{-1} + \\delta_{+1})$ and $Y$ is the standard semicircle.</p> <p>This is Example 10.4. from Speicher LN on Non-commutative distributions.</p> In\u00a0[\u00a0]: Copied! <pre>def cauchy_transform_custom(z):\n    \"\"\"\n    Computes the Cauchy transform G_mu(z) of the measure\n    mu_X = (1/4)(2\u03b4_{-2} + \u03b4_{-1} + \u03b4_{+1})\n    \"\"\"\n    return (1/4) * (2 / (z + 2) + 1 / (z + 1) + 1 / (z - 1))\n\n# Example usage\nz_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane\nG_values = np.array([cauchy_transform_custom(z) for z in z_values])\n\n# Plot real and imaginary parts\nplt.figure(figsize=(8, 5))\nplt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed')\nplt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\")\nplt.xlabel(\"Re(z)\")\nplt.ylabel(\"G_mu(z)\")\nplt.legend()\nplt.title(\"Cauchy Transform of Given Distribution\")\nplt.grid()\nplt.show()\n</pre> def cauchy_transform_custom(z):     \"\"\"     Computes the Cauchy transform G_mu(z) of the measure     mu_X = (1/4)(2\u03b4_{-2} + \u03b4_{-1} + \u03b4_{+1})     \"\"\"     return (1/4) * (2 / (z + 2) + 1 / (z + 1) + 1 / (z - 1))  # Example usage z_values = np.linspace(-3, 3, 500) + 0.1j  # Evaluate on the upper half-plane G_values = np.array([cauchy_transform_custom(z) for z in z_values])  # Plot real and imaginary parts plt.figure(figsize=(8, 5)) plt.plot(z_values.real, G_values.real, label=\"Re(G_mu(z))\", linestyle='dashed') plt.plot(z_values.real, G_values.imag, label=\"Im(G_mu(z))\") plt.xlabel(\"Re(z)\") plt.ylabel(\"G_mu(z)\") plt.legend() plt.title(\"Cauchy Transform of Given Distribution\") plt.grid() plt.show()   In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform_custom(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",\n                   H2_name = \"H_matrix_semicircle\",\n                   H1_kwargs={\"G_name\":\"cauchy_transform_custom\"},\n                   H2_kwargs={})\n  Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_custom\")\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator_deform_custom(x)\nprint(f)\n\n#visualization\nm = 100\nal = -4\nau = 11\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform_custom(x)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform_custom(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",                    H2_name = \"H_matrix_semicircle\",                    H1_kwargs={\"G_name\":\"cauchy_transform_custom\"},                    H2_kwargs={})   Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_custom\")   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator_deform_custom(x) print(f)  #visualization m = 100 al = -4 au = 11 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform_custom(x)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical Check:</p> In\u00a0[\u00a0]: Copied! <pre>size = 200\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  # Define the values for the diagonal\n  diagonal_values = np.concatenate([np.full(100, -2), np.full(50, -1), np.full(50, 1)])\n  A = np.diag(diagonal_values)\n  B = random_semicircle(size)\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 200 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   # Define the values for the diagonal   diagonal_values = np.concatenate([np.full(100, -2), np.full(50, -1), np.full(50, 1)])   A = np.diag(diagonal_values)   B = random_semicircle(size)   e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend() <p>Here as in previous example, $X$ has measure $\\mu_X = \\frac{1}{4}(2 \\delta_{-2} + \\delta_{-1} + \\delta_{+1})$ and $Y$ has measure $\\mu_Y = \\frac{1}{2}(\\delta_1 + \\delta_3)$</p> <p>Calculation using subordination function. (This is somewhat different from the result in Speicher's Lecture notes. Maybe because X and Y switch places?)</p> In\u00a0[\u00a0]: Copied! <pre>def get_density_anticommutator_deform_custom2(x, eps = 0.01):\n  A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])\n  A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])\n  A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])\n  AA = (A1, A2)\n  n = A0.shape[0]\n  z = x + eps * 1j\n  B = Lambda(z, n) - A0\n  om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",\n                   H2_name = \"H_matrix_custom\",\n                   H1_kwargs={\"G_name\":\"cauchy_transform_discrete\",\n                              \"G_kwargs\":{\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])}},\n                   H2_kwargs={\"G_name\":\"cauchy_transform_discrete\",\n                              \"G_kwargs\":{\"points\":np.array([1, 3]), \"weights\": np.array([1/2, 1/2])}})\n  Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_discrete\",\n                        G_kwargs={\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])})\n  f = (-1/np.pi) * Gxy[0,0].imag\n  return f\n\nx = .5\nf = get_density_anticommutator_deform_custom2(x)\nprint(f)\n\n#visualization\nm = 100\nal = -10\nau = 10\nXX = np.linspace(al, au, m)\nf = np.zeros(XX.shape)\nfor i, x in enumerate(XX):\n   f[i] = get_density_anticommutator_deform_custom2(x)\n\nprint(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately)\nplt.plot(XX, f)\nplt.grid(True)\n</pre> def get_density_anticommutator_deform_custom2(x, eps = 0.01):   A0 = np.array([[0, 0, 0], [0, 0, -1], [0, -1, 0]])   A1 = np.array([[0, 1, 1/2], [1, 0, 0], [1/2, 0, 0]])   A2 = np.array([[0, 0, 1], [0, 0, 0], [1, 0, 0]])   AA = (A1, A2)   n = A0.shape[0]   z = x + eps * 1j   B = Lambda(z, n) - A0   om = omega_sub(B, (A1, A2), rank = (2,2), H1_name = \"H_matrix_custom\",                    H2_name = \"H_matrix_custom\",                    H1_kwargs={\"G_name\":\"cauchy_transform_discrete\",                               \"G_kwargs\":{\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])}},                    H2_kwargs={\"G_name\":\"cauchy_transform_discrete\",                               \"G_kwargs\":{\"points\":np.array([1, 3]), \"weights\": np.array([1/2, 1/2])}})   Gxy = G_matrix_custom(om, A1, rank = 2, G_name=\"cauchy_transform_discrete\",                         G_kwargs={\"points\":np.array([-2, -1, 1]), \"weights\": np.array([2/4, 1/4, 1/4])})   f = (-1/np.pi) * Gxy[0,0].imag   return f  x = .5 f = get_density_anticommutator_deform_custom2(x) print(f)  #visualization m = 100 al = -10 au = 10 XX = np.linspace(al, au, m) f = np.zeros(XX.shape) for i, x in enumerate(XX):    f[i] = get_density_anticommutator_deform_custom2(x)  print(sum(f)*(au - al)/m) #just to check that the integral of the density is 1 (approximately) plt.plot(XX, f) plt.grid(True) <p>Numerical check</p> In\u00a0[\u00a0]: Copied! <pre>size = 400\nzero_m = np.zeros((size, size))\nones_m = np.eye(size)\nT = 30\nEE = np.zeros((size,T))\nfor count in range(T):\n  # Define the values for the diagonal\n  diagonal_values = np.concatenate([np.full(200, -2), np.full(100, -1), np.full(100, 1)])\n  A = np.diag(diagonal_values)\n  diagonal_values = np.concatenate([np.full(200, 1), np.full(200, 3)])\n  B = np.diag(diagonal_values)\n  Q = random_orthogonal(size)\n  B = Q @ B @ Q.T\n\n  e = la.eigvalsh(A @ B + B @ A + A @ A)\n  EE[:,count] = e\n\nEE = EE.reshape(-1)\n\nplt.figure()\n# Plot histogram with density\nplt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\")\n# Plot theoretical density curve\nplt.plot(XX, f, color='red', label=\"Theoretical Density\")\n\n# Add labels and legend\nplt.title(\"Histogram with Theoretical Density Curve\")\nplt.xlabel(\"Values\")\nplt.ylabel(\"Density\")\nplt.legend()\n</pre> size = 400 zero_m = np.zeros((size, size)) ones_m = np.eye(size) T = 30 EE = np.zeros((size,T)) for count in range(T):   # Define the values for the diagonal   diagonal_values = np.concatenate([np.full(200, -2), np.full(100, -1), np.full(100, 1)])   A = np.diag(diagonal_values)   diagonal_values = np.concatenate([np.full(200, 1), np.full(200, 3)])   B = np.diag(diagonal_values)   Q = random_orthogonal(size)   B = Q @ B @ Q.T    e = la.eigvalsh(A @ B + B @ A + A @ A)   EE[:,count] = e  EE = EE.reshape(-1)  plt.figure() # Plot histogram with density plt.hist(EE, bins=30, density=True, edgecolor='black', alpha=0.6, label=\"Histogram\") # Plot theoretical density curve plt.plot(XX, f, color='red', label=\"Theoretical Density\")  # Add labels and legend plt.title(\"Histogram with Theoretical Density Curve\") plt.xlabel(\"Values\") plt.ylabel(\"Density\") plt.legend()"},{"location":"notebooks/10_semicircle_methods/#methods-for-calculating-the-eigevalue-density-of-matrix-semicircle-and-of-polynomials-in-free-random-variables","title":"Methods for calculating the eigevalue density of matrix semicircle and of polynomials in free random variables\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#theoretical-background-and-functions","title":"Theoretical background and functions\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-1-eigenvalue-distribution-of-a-matrix-semicircle","title":"Example 1 (Eigenvalue distribution of a matrix semicircle).\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-2-toeplitz-semicircle-matrix","title":"Example 2: Toeplitz Semicircle Matrix\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-3a-this-is-the-matrix-semicircle-that-arises-in-the-study-of-the-anti-commutator","title":"Example 3a: This is the matrix semicircle that arises in the study of the anti-commutator\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-3b-the-eigenvalue-distribution-of-the-anticommutator-of-two-semicircle-rvs-via-the-equation-for-the-biased-matrix-semicircle","title":"Example 3b: The eigenvalue distribution  of the anticommutator of two semicircle r.v.s. via the equation for the biased matrix semicircle.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-4-anticommutator-using-subordination","title":"Example 4: Anticommutator using subordination.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#the-most-basic-approach-to-calculation-of-the-cauchy-transform-gw-of-b-otimes-x","title":"The most basic approach to calculation of the Cauchy transform $G(w)$ of $b \\otimes X$\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#here-is-the-cauchy-transform-of-a-otimes-x-using-regularization","title":"Here is the Cauchy transform of $a \\otimes X$ using regularization.\u00b6","text":"<p>It works OK. May be preferable since does not require specifying or estimating the rank of a.</p>"},{"location":"notebooks/10_semicircle_methods/#more-sophisticated-approach-to-the-calculation-of-cauchy-transform-gw-of-b-otimes-x","title":"More sophisticated approach to the calculation of Cauchy transform $G(w)$ of $b \\otimes X$\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#matrix-subordination-function","title":"Matrix Subordination Function\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#calculating-the-density-of-the-anticommutator","title":"Calculating the density of the anticommutator.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-4b-anticommutator-of-poisson-rvs","title":"Example 4b. Anticommutator of Poisson r.v.s.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-4c-anticommutator-of-a-semicircle-and-a-free-poisson-rvs","title":"Example 4c: Anticommutator of a semicircle and a free Poisson r.v.s\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-5-deformed-anticommutator","title":"Example 5: Deformed anticommutator\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#5a-first-we-consider-the-situation-when-x-and-y-are-free-semicircles","title":"5a. First we consider the situation when $X$ and $Y$ are free semicircles.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-5b-deformed-anticommutator-of-the-semicircle-and-the-free-poisson","title":"Example 5b: Deformed anticommutator of the semicircle and the free Poisson.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-5c-another-deformed-anticommutator","title":"Example 5c. Another deformed anticommutator.\u00b6","text":""},{"location":"notebooks/10_semicircle_methods/#example-59-yet-another-deformed-anticommutator","title":"Example 5.9 Yet another deformed anticommutator\u00b6","text":""}]}